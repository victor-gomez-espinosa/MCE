{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Victor Gómez\n",
    "### victor.gomez@cimat.mx\n",
    "\n",
    "\n",
    "# Maestría en Cómputo Estadístico\n",
    "# CIMAT Monterrey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tópicos selectos de Cómputo<center>\n",
    "# <center>Tarea 1: Valores faltantes<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrucciones: \n",
    "- Usa python 3.x , tensorflow 2.x y xgboost 1.x\n",
    "- ejecuta todas las celdas para replicar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías necesarias\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase que realiza la imputación de datos faltantes en un DataFrame por distintos métodos.\n",
    "\n",
    "## Métodos de imputación de valores faltantes:\n",
    "\n",
    "- 1 Métodos no estadísticos: LOCF (Last observation carried forward)\n",
    "- 2 Métodos estadísticos:    mean_mode (media para atributos numéricos, moda para atributos categoricos)\n",
    "\n",
    "Métodos de Machine Learning: \n",
    "- 3 Vecinos mas cercanos: knn (KNeighbors: k: 10)\n",
    "- 4 Arboles de decisión:  trees (XGBoost: learning rate: 0.3, n_estimators: 150, subsample: 0.9, regularización: gamma: 10, lambda: 10)\n",
    "- 5 Redes neuronales:     MLP (epocas: 100, optimizador: adam, función de costo: binary_crossentropy(o categorical_crossentropy), metrica: accuracy, hiddenlayers:  2 con 50 neuronas, función de activación: relu. outputlayer: función de activación: sigmoid (o softmax) con 1 (o número de clases) neuronas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _missing_value import MissingValueImputation #Clase que realiza la imputacion de datos faltantes en un DataFrame por distintos metodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos de datos con valores faltantes (%MVs):\n",
    "datasets de: https://sci2s.ugr.es/keel/missing.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cf9cd8bba8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "names=['adult','automobile','bands','cleveland','dermatology','hepatitis','housevotes','mammographic','marketing','mushroom']\n",
    "Ejemplos=[48842,205,539,303,366,155,435,961,8993,8124]\n",
    "clases=[2,6,2,5,6,2,2,2,9,2]\n",
    "mvs=[7.41,26.83,32.28,1.98,2.19,48.39,46.67,13.63,23.54,30.53]\n",
    "\n",
    "df_mvs_dict={\n",
    "            'Ejemplos': Ejemplos,\n",
    "             'Clases': clases,\n",
    "             'MVs': mvs}\n",
    "\n",
    "df_mvs=pd.DataFrame(df_mvs_dict)\n",
    "df_mvs.index=names\n",
    "\n",
    "axes = df_mvs.plot.bar(rot=45, subplots=True,figsize=(7,7))\n",
    "axes[1].legend(loc=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los 10 conjuntos de datos presentan diferentes características, 3 contienen mucha mayor cantidad de ejemplos que  los demás, además que algunos tienen mayor cantidad de porcentaje de datos faltantes que otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de ML para clasificación con validación cruzada 5-Fold y metrica accuracy:\n",
    "\n",
    "- 1 KNeighbors: k: 10\n",
    "- 2 XGBoost: learning rate: 0.3, n_estimators: 150, subsample: 0.9, regularización: gamma: 10,lambda: 10\n",
    "- 3 MLP, epocas: 100, optimizador: adam, función de costo: binary_crossentropy(o categorical_crossentropy), hiddenlayers:  2 con 50 neuronas, función de activación: relu. outputlayer: función de activación: sigmoid (o softmax) con 1 (o número de clases) neuronas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_clean(df): #libera memoria\n",
    "    df=pd.DataFrame()\n",
    "    del df\n",
    "    \n",
    "#Models:\n",
    "#K Neighbors\n",
    "def KNN_model():\n",
    "    return KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "#XGBoost\n",
    "def XGBoost_model(num_classes=2):\n",
    "    \n",
    "    \n",
    "    if num_classes ==2:\n",
    "        gbm=xgb.XGBClassifier(learning_rate=0.3,n_estimators=150,subsample=0.9,gamma=10,reg_lambda=10,n_jobs=-1)\n",
    "                \n",
    "    else:\n",
    "        gbm=xgb.XGBClassifier(objective='multi:softmax',learning_rate=0.3,n_estimators=150,subsample=0.9,gamma=10,reg_lambda=10,n_jobs=-1)\n",
    "    \n",
    "    return gbm \n",
    "\n",
    "\n",
    "\n",
    "#MLP\n",
    "def create_model(input_features,neurons_output,activation,loss):\n",
    "    neur_Hid_layer=[50,50]\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(neur_Hid_layer[0],activation='relu',input_shape=(input_features,))) #first layer\n",
    "    model.add(tf.keras.layers.BatchNormalization()) #batch normalization\n",
    "        \n",
    "    layers=len(neur_Hid_layer)\n",
    "        \n",
    "    if layers>1: #next hidden layers\n",
    "        for layer in range(1,layers):\n",
    "            model.add(tf.keras.layers.Dense(neur_Hid_layer[layer],activation='relu'))\n",
    "            model.add(tf.keras.layers.BatchNormalization()) #batch normalization\n",
    "        \n",
    "    if activation!='linear':\n",
    "        model.add(tf.keras.layers.Dense(neurons_output,activation=activation)) #output layer\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(neurons_output)) #output layer\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy']) #compìla el modelo\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "#Cross validation \n",
    "def model_train_CV_(model,X_train,y_train,cv=5): \n",
    "    kfold=cross_val_score(model,X_train,y_train,cv=cv,n_jobs=-1)\n",
    "    score=kfold.mean()\n",
    "    return score\n",
    "\n",
    "#prepara los datos, entrena, y obtiene scores para cada modelo\n",
    "def Data_Prer_Mod_train(df,model_name,cv): \n",
    "    _,column=df.shape\n",
    "    column=column-1\n",
    "    flat= lambda x,num_features: x.reshape([-1, num_features]) # Flatten function\n",
    "    if model_name=='KNN':\n",
    "        y=df.iloc[:,column].values\n",
    "        X=pd.get_dummies(df.drop([column],axis=1),drop_first=True).values\n",
    "        model=KNN_model()\n",
    "        score=model_train_CV_(model,X,y,cv)\n",
    "        \n",
    "    elif model_name=='XGB':\n",
    "        y=df.iloc[:,column].values\n",
    "        X=pd.get_dummies(df.drop([column],axis=1),drop_first=True).values\n",
    "        num_classes=np.shape(df.iloc[:,column].unique())[0]\n",
    "        model=XGBoost_model(num_classes)\n",
    "        score=model_train_CV_(model,X,y,cv)\n",
    "    elif model_name=='MLP':\n",
    "        y=df.iloc[:,column].values\n",
    "        X=pd.get_dummies(df.drop([column],axis=1),drop_first=True).values\n",
    "        num_classes=np.shape(df.iloc[:,column].unique())[0]\n",
    "        num_features=X.shape[1]\n",
    "        scaler = StandardScaler() #Standardize\n",
    "        scaler.fit(X)\n",
    "        X=scaler.transform(X)\n",
    "        X=flat(X,num_features) #flatten\n",
    "        \n",
    "        if num_classes==2:\n",
    "            y=np.reshape(y,(-1, 1))\n",
    "            enc=OneHotEncoder(drop='first').fit(y)\n",
    "            y = enc.transform(y).toarray()\n",
    "            lossf='binary_crossentropy'\n",
    "            activ_output='sigmoid'\n",
    "            neurons_output=1\n",
    "            \n",
    "        else:\n",
    "            y=np.reshape(y,(-1, 1))\n",
    "            enc=OneHotEncoder().fit(y)\n",
    "            y = enc.transform(y).toarray()\n",
    "            lossf='categorical_crossentropy'\n",
    "            activ_output='softmax'\n",
    "            neurons_output=num_classes\n",
    "        \n",
    "        model=KerasClassifier(build_fn=create_model, epochs=100,  input_features=num_features,neurons_output=neurons_output,activation=activ_output,loss=lossf)\n",
    "\n",
    "        kfold=cross_val_score(model,X,y,cv=cv,n_jobs=-1)\n",
    "        score=kfold.mean()\n",
    "        \n",
    "    return np.round(score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Importación de los conjuntos de datos, Imputación de los datos faltantes, Ajuste de modelos y Validacion cruzada 5-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :LOCF model :KNN score:  0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :LOCF model :XGB score:  0.5\n",
      "DataFrame: automobile imput_method :LOCF model :MLP score:  0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :mean_mode model :KNN score:  0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :mean_mode model :XGB score:  0.48\n",
      "DataFrame: automobile imput_method :mean_mode model :MLP score:  0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :knn model :KNN score:  0.39\n",
      "DataFrame: automobile imput_method :knn model :XGB score:  0.48\n",
      "DataFrame: automobile imput_method :knn model :MLP score:  0.78\n",
      "[0]\tvalidation_0-mae:87.38394\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:63.14961\n",
      "[2]\tvalidation_0-mae:46.67097\n",
      "[3]\tvalidation_0-mae:35.23204\n",
      "[4]\tvalidation_0-mae:27.69285\n",
      "[5]\tvalidation_0-mae:22.26601\n",
      "[6]\tvalidation_0-mae:18.56758\n",
      "[7]\tvalidation_0-mae:15.52041\n",
      "[8]\tvalidation_0-mae:13.31204\n",
      "[9]\tvalidation_0-mae:11.25003\n",
      "[10]\tvalidation_0-mae:9.90334\n",
      "[11]\tvalidation_0-mae:8.75421\n",
      "[12]\tvalidation_0-mae:7.89097\n",
      "[13]\tvalidation_0-mae:7.20449\n",
      "[14]\tvalidation_0-mae:6.55076\n",
      "[15]\tvalidation_0-mae:5.99387\n",
      "[16]\tvalidation_0-mae:5.42914\n",
      "[17]\tvalidation_0-mae:4.98692\n",
      "[18]\tvalidation_0-mae:4.65623\n",
      "[19]\tvalidation_0-mae:4.26929\n",
      "[20]\tvalidation_0-mae:4.01351\n",
      "[21]\tvalidation_0-mae:3.75540\n",
      "[22]\tvalidation_0-mae:3.50051\n",
      "[23]\tvalidation_0-mae:3.31703\n",
      "[24]\tvalidation_0-mae:3.19633\n",
      "[25]\tvalidation_0-mae:3.01867\n",
      "[26]\tvalidation_0-mae:2.92012\n",
      "[27]\tvalidation_0-mae:2.84021\n",
      "[28]\tvalidation_0-mae:2.71481\n",
      "[29]\tvalidation_0-mae:2.64475\n",
      "[30]\tvalidation_0-mae:2.57441\n",
      "[31]\tvalidation_0-mae:2.52212\n",
      "[32]\tvalidation_0-mae:2.48220\n",
      "[33]\tvalidation_0-mae:2.41866\n",
      "[34]\tvalidation_0-mae:2.37268\n",
      "[35]\tvalidation_0-mae:2.33569\n",
      "[36]\tvalidation_0-mae:2.29233\n",
      "[37]\tvalidation_0-mae:2.28409\n",
      "[38]\tvalidation_0-mae:2.24717\n",
      "[39]\tvalidation_0-mae:2.22921\n",
      "[40]\tvalidation_0-mae:2.19508\n",
      "[41]\tvalidation_0-mae:2.15799\n",
      "[42]\tvalidation_0-mae:2.12641\n",
      "[43]\tvalidation_0-mae:2.06699\n",
      "[44]\tvalidation_0-mae:2.05270\n",
      "[45]\tvalidation_0-mae:2.00304\n",
      "[46]\tvalidation_0-mae:1.99679\n",
      "[47]\tvalidation_0-mae:1.96770\n",
      "[48]\tvalidation_0-mae:1.92415\n",
      "[49]\tvalidation_0-mae:1.91967\n",
      "[50]\tvalidation_0-mae:1.86862\n",
      "[51]\tvalidation_0-mae:1.86465\n",
      "[52]\tvalidation_0-mae:1.83869\n",
      "[53]\tvalidation_0-mae:1.79440\n",
      "[54]\tvalidation_0-mae:1.78198\n",
      "[55]\tvalidation_0-mae:1.75951\n",
      "[56]\tvalidation_0-mae:1.72520\n",
      "[57]\tvalidation_0-mae:1.72320\n",
      "[58]\tvalidation_0-mae:1.71808\n",
      "[59]\tvalidation_0-mae:1.70674\n",
      "[60]\tvalidation_0-mae:1.68578\n",
      "[61]\tvalidation_0-mae:1.67179\n",
      "[62]\tvalidation_0-mae:1.65252\n",
      "[63]\tvalidation_0-mae:1.64854\n",
      "[64]\tvalidation_0-mae:1.64470\n",
      "[65]\tvalidation_0-mae:1.64162\n",
      "[66]\tvalidation_0-mae:1.63816\n",
      "[67]\tvalidation_0-mae:1.63619\n",
      "[68]\tvalidation_0-mae:1.61784\n",
      "[69]\tvalidation_0-mae:1.61831\n",
      "[70]\tvalidation_0-mae:1.61478\n",
      "[71]\tvalidation_0-mae:1.60948\n",
      "[72]\tvalidation_0-mae:1.59958\n",
      "[73]\tvalidation_0-mae:1.59222\n",
      "[74]\tvalidation_0-mae:1.59149\n",
      "[75]\tvalidation_0-mae:1.58857\n",
      "[76]\tvalidation_0-mae:1.58732\n",
      "[77]\tvalidation_0-mae:1.58402\n",
      "[78]\tvalidation_0-mae:1.57507\n",
      "[79]\tvalidation_0-mae:1.56558\n",
      "[80]\tvalidation_0-mae:1.56208\n",
      "[81]\tvalidation_0-mae:1.56224\n",
      "[82]\tvalidation_0-mae:1.55890\n",
      "[83]\tvalidation_0-mae:1.54872\n",
      "[84]\tvalidation_0-mae:1.55065\n",
      "[85]\tvalidation_0-mae:1.54460\n",
      "[86]\tvalidation_0-mae:1.54309\n",
      "[87]\tvalidation_0-mae:1.54180\n",
      "[88]\tvalidation_0-mae:1.52849\n",
      "[89]\tvalidation_0-mae:1.52344\n",
      "[90]\tvalidation_0-mae:1.50572\n",
      "[91]\tvalidation_0-mae:1.49065\n",
      "[92]\tvalidation_0-mae:1.49521\n",
      "[93]\tvalidation_0-mae:1.49618\n",
      "Stopping. Best iteration:\n",
      "[91]\tvalidation_0-mae:1.49065\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.58758\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.53657\n",
      "[2]\tvalidation_0-logloss:0.49388\n",
      "[3]\tvalidation_0-logloss:0.46349\n",
      "[4]\tvalidation_0-logloss:0.43786\n",
      "[5]\tvalidation_0-logloss:0.41972\n",
      "[6]\tvalidation_0-logloss:0.40173\n",
      "[7]\tvalidation_0-logloss:0.38775\n",
      "[8]\tvalidation_0-logloss:0.38776\n",
      "[9]\tvalidation_0-logloss:0.38752\n",
      "[10]\tvalidation_0-logloss:0.37304\n",
      "[11]\tvalidation_0-logloss:0.37307\n",
      "[12]\tvalidation_0-logloss:0.37306\n",
      "Stopping. Best iteration:\n",
      "[10]\tvalidation_0-logloss:0.37304\n",
      "\n",
      "[0]\tvalidation_0-mae:2.02748\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.44779\n",
      "[2]\tvalidation_0-mae:1.03904\n",
      "[3]\tvalidation_0-mae:0.74360\n",
      "[4]\tvalidation_0-mae:0.53412\n",
      "[5]\tvalidation_0-mae:0.38972\n",
      "[6]\tvalidation_0-mae:0.30997\n",
      "[7]\tvalidation_0-mae:0.26842\n",
      "[8]\tvalidation_0-mae:0.24692\n",
      "[9]\tvalidation_0-mae:0.24148\n",
      "[10]\tvalidation_0-mae:0.23826\n",
      "[11]\tvalidation_0-mae:0.23647\n",
      "[12]\tvalidation_0-mae:0.23565\n",
      "[13]\tvalidation_0-mae:0.23511\n",
      "[14]\tvalidation_0-mae:0.23535\n",
      "[15]\tvalidation_0-mae:0.23547\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-mae:0.23511\n",
      "\n",
      "[0]\tvalidation_0-mae:1.97008\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.40749\n",
      "[2]\tvalidation_0-mae:1.01618\n",
      "[3]\tvalidation_0-mae:0.74213\n",
      "[4]\tvalidation_0-mae:0.55637\n",
      "[5]\tvalidation_0-mae:0.43459\n",
      "[6]\tvalidation_0-mae:0.35364\n",
      "[7]\tvalidation_0-mae:0.30003\n",
      "[8]\tvalidation_0-mae:0.27090\n",
      "[9]\tvalidation_0-mae:0.25358\n",
      "[10]\tvalidation_0-mae:0.24518\n",
      "[11]\tvalidation_0-mae:0.23971\n",
      "[12]\tvalidation_0-mae:0.23619\n",
      "[13]\tvalidation_0-mae:0.23390\n",
      "[14]\tvalidation_0-mae:0.23348\n",
      "[15]\tvalidation_0-mae:0.23331\n",
      "[16]\tvalidation_0-mae:0.23317\n",
      "[17]\tvalidation_0-mae:0.23274\n",
      "[18]\tvalidation_0-mae:0.23273\n",
      "[19]\tvalidation_0-mae:0.23251\n",
      "[20]\tvalidation_0-mae:0.23242\n",
      "[21]\tvalidation_0-mae:0.23260\n",
      "[22]\tvalidation_0-mae:0.23242\n",
      "[23]\tvalidation_0-mae:0.23224\n",
      "[24]\tvalidation_0-mae:0.23178\n",
      "[25]\tvalidation_0-mae:0.23200\n",
      "[26]\tvalidation_0-mae:0.23185\n",
      "Stopping. Best iteration:\n",
      "[24]\tvalidation_0-mae:0.23178\n",
      "\n",
      "[0]\tvalidation_0-mae:75.97684\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:55.53259\n",
      "[2]\tvalidation_0-mae:40.52405\n",
      "[3]\tvalidation_0-mae:30.21710\n",
      "[4]\tvalidation_0-mae:23.04353\n",
      "[5]\tvalidation_0-mae:17.74033\n",
      "[6]\tvalidation_0-mae:13.87398\n",
      "[7]\tvalidation_0-mae:11.10754\n",
      "[8]\tvalidation_0-mae:8.91788\n",
      "[9]\tvalidation_0-mae:7.33278\n",
      "[10]\tvalidation_0-mae:6.24183\n",
      "[11]\tvalidation_0-mae:5.48048\n",
      "[12]\tvalidation_0-mae:4.82688\n",
      "[13]\tvalidation_0-mae:4.31071\n",
      "[14]\tvalidation_0-mae:3.87629\n",
      "[15]\tvalidation_0-mae:3.50516\n",
      "[16]\tvalidation_0-mae:3.21591\n",
      "[17]\tvalidation_0-mae:2.99922\n",
      "[18]\tvalidation_0-mae:2.81269\n",
      "[19]\tvalidation_0-mae:2.65585\n",
      "[20]\tvalidation_0-mae:2.50031\n",
      "[21]\tvalidation_0-mae:2.38053\n",
      "[22]\tvalidation_0-mae:2.27176\n",
      "[23]\tvalidation_0-mae:2.17993\n",
      "[24]\tvalidation_0-mae:2.09068\n",
      "[25]\tvalidation_0-mae:2.04349\n",
      "[26]\tvalidation_0-mae:1.97050\n",
      "[27]\tvalidation_0-mae:1.92777\n",
      "[28]\tvalidation_0-mae:1.89485\n",
      "[29]\tvalidation_0-mae:1.84357\n",
      "[30]\tvalidation_0-mae:1.79582\n",
      "[31]\tvalidation_0-mae:1.75403\n",
      "[32]\tvalidation_0-mae:1.72808\n",
      "[33]\tvalidation_0-mae:1.70474\n",
      "[34]\tvalidation_0-mae:1.66857\n",
      "[35]\tvalidation_0-mae:1.63360\n",
      "[36]\tvalidation_0-mae:1.60180\n",
      "[37]\tvalidation_0-mae:1.57437\n",
      "[38]\tvalidation_0-mae:1.53796\n",
      "[39]\tvalidation_0-mae:1.52121\n",
      "[40]\tvalidation_0-mae:1.50014\n",
      "[41]\tvalidation_0-mae:1.48352\n",
      "[42]\tvalidation_0-mae:1.46592\n",
      "[43]\tvalidation_0-mae:1.45296\n",
      "[44]\tvalidation_0-mae:1.43672\n",
      "[45]\tvalidation_0-mae:1.41246\n",
      "[46]\tvalidation_0-mae:1.40204\n",
      "[47]\tvalidation_0-mae:1.38777\n",
      "[48]\tvalidation_0-mae:1.37375\n",
      "[49]\tvalidation_0-mae:1.33941\n",
      "[50]\tvalidation_0-mae:1.33833\n",
      "[51]\tvalidation_0-mae:1.32662\n",
      "[52]\tvalidation_0-mae:1.31925\n",
      "[53]\tvalidation_0-mae:1.31681\n",
      "[54]\tvalidation_0-mae:1.29253\n",
      "[55]\tvalidation_0-mae:1.29455\n",
      "[56]\tvalidation_0-mae:1.29260\n",
      "Stopping. Best iteration:\n",
      "[54]\tvalidation_0-mae:1.29253\n",
      "\n",
      "[0]\tvalidation_0-mae:3664.70483\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:2625.99927\n",
      "[2]\tvalidation_0-mae:1875.09192\n",
      "[3]\tvalidation_0-mae:1343.10205\n",
      "[4]\tvalidation_0-mae:970.99939\n",
      "[5]\tvalidation_0-mae:714.12500\n",
      "[6]\tvalidation_0-mae:533.48346\n",
      "[7]\tvalidation_0-mae:402.69388\n",
      "[8]\tvalidation_0-mae:316.55518\n",
      "[9]\tvalidation_0-mae:251.18950\n",
      "[10]\tvalidation_0-mae:202.83777\n",
      "[11]\tvalidation_0-mae:164.14616\n",
      "[12]\tvalidation_0-mae:136.40939\n",
      "[13]\tvalidation_0-mae:117.77930\n",
      "[14]\tvalidation_0-mae:103.51335\n",
      "[15]\tvalidation_0-mae:89.51884\n",
      "[16]\tvalidation_0-mae:80.14171\n",
      "[17]\tvalidation_0-mae:69.61666\n",
      "[18]\tvalidation_0-mae:61.95618\n",
      "[19]\tvalidation_0-mae:53.66784\n",
      "[20]\tvalidation_0-mae:48.42193\n",
      "[21]\tvalidation_0-mae:43.83224\n",
      "[22]\tvalidation_0-mae:40.29000\n",
      "[23]\tvalidation_0-mae:36.96811\n",
      "[24]\tvalidation_0-mae:34.41351\n",
      "[25]\tvalidation_0-mae:32.93859\n",
      "[26]\tvalidation_0-mae:31.01601\n",
      "[27]\tvalidation_0-mae:29.19622\n",
      "[28]\tvalidation_0-mae:26.85236\n",
      "[29]\tvalidation_0-mae:24.95187\n",
      "[30]\tvalidation_0-mae:23.89408\n",
      "[31]\tvalidation_0-mae:22.41917\n",
      "[32]\tvalidation_0-mae:21.04117\n",
      "[33]\tvalidation_0-mae:20.53801\n",
      "[34]\tvalidation_0-mae:19.43426\n",
      "[35]\tvalidation_0-mae:18.32382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36]\tvalidation_0-mae:17.12335\n",
      "[37]\tvalidation_0-mae:16.45349\n",
      "[38]\tvalidation_0-mae:15.26633\n",
      "[39]\tvalidation_0-mae:14.72631\n",
      "[40]\tvalidation_0-mae:14.00850\n",
      "[41]\tvalidation_0-mae:13.39026\n",
      "[42]\tvalidation_0-mae:12.66663\n",
      "[43]\tvalidation_0-mae:12.19048\n",
      "[44]\tvalidation_0-mae:11.75091\n",
      "[45]\tvalidation_0-mae:11.22744\n",
      "[46]\tvalidation_0-mae:10.82458\n",
      "[47]\tvalidation_0-mae:10.42368\n",
      "[48]\tvalidation_0-mae:9.91040\n",
      "[49]\tvalidation_0-mae:9.62536\n",
      "[50]\tvalidation_0-mae:9.35010\n",
      "[51]\tvalidation_0-mae:8.97877\n",
      "[52]\tvalidation_0-mae:8.50780\n",
      "[53]\tvalidation_0-mae:8.19062\n",
      "[54]\tvalidation_0-mae:7.94886\n",
      "[55]\tvalidation_0-mae:7.68018\n",
      "[56]\tvalidation_0-mae:7.37758\n",
      "[57]\tvalidation_0-mae:7.11895\n",
      "[58]\tvalidation_0-mae:6.90014\n",
      "[59]\tvalidation_0-mae:6.75735\n",
      "[60]\tvalidation_0-mae:6.49760\n",
      "[61]\tvalidation_0-mae:6.29545\n",
      "[62]\tvalidation_0-mae:6.12745\n",
      "[63]\tvalidation_0-mae:5.99685\n",
      "[64]\tvalidation_0-mae:5.77244\n",
      "[65]\tvalidation_0-mae:5.64287\n",
      "[66]\tvalidation_0-mae:5.44337\n",
      "[67]\tvalidation_0-mae:5.13114\n",
      "[68]\tvalidation_0-mae:5.02887\n",
      "[69]\tvalidation_0-mae:4.82406\n",
      "[70]\tvalidation_0-mae:4.62813\n",
      "[71]\tvalidation_0-mae:4.48964\n",
      "[72]\tvalidation_0-mae:4.41910\n",
      "[73]\tvalidation_0-mae:4.23768\n",
      "[74]\tvalidation_0-mae:4.09330\n",
      "[75]\tvalidation_0-mae:3.92571\n",
      "[76]\tvalidation_0-mae:3.78228\n",
      "[77]\tvalidation_0-mae:3.67507\n",
      "[78]\tvalidation_0-mae:3.58214\n",
      "[79]\tvalidation_0-mae:3.50370\n",
      "[80]\tvalidation_0-mae:3.45687\n",
      "[81]\tvalidation_0-mae:3.32356\n",
      "[82]\tvalidation_0-mae:3.27965\n",
      "[83]\tvalidation_0-mae:3.20793\n",
      "[84]\tvalidation_0-mae:3.14091\n",
      "[85]\tvalidation_0-mae:3.02163\n",
      "[86]\tvalidation_0-mae:2.95249\n",
      "[87]\tvalidation_0-mae:2.88803\n",
      "[88]\tvalidation_0-mae:2.78305\n",
      "[89]\tvalidation_0-mae:2.66501\n",
      "[90]\tvalidation_0-mae:2.56785\n",
      "[91]\tvalidation_0-mae:2.51267\n",
      "[92]\tvalidation_0-mae:2.46503\n",
      "[93]\tvalidation_0-mae:2.39867\n",
      "[94]\tvalidation_0-mae:2.35746\n",
      "[95]\tvalidation_0-mae:2.30521\n",
      "[96]\tvalidation_0-mae:2.24624\n",
      "[97]\tvalidation_0-mae:2.16594\n",
      "[98]\tvalidation_0-mae:2.10617\n",
      "[99]\tvalidation_0-mae:2.06755\n",
      "[100]\tvalidation_0-mae:2.03323\n",
      "[101]\tvalidation_0-mae:1.98886\n",
      "[102]\tvalidation_0-mae:1.93281\n",
      "[103]\tvalidation_0-mae:1.90761\n",
      "[104]\tvalidation_0-mae:1.90025\n",
      "[105]\tvalidation_0-mae:1.87128\n",
      "[106]\tvalidation_0-mae:1.84399\n",
      "[107]\tvalidation_0-mae:1.83388\n",
      "[108]\tvalidation_0-mae:1.79652\n",
      "[109]\tvalidation_0-mae:1.77600\n",
      "[110]\tvalidation_0-mae:1.75689\n",
      "[111]\tvalidation_0-mae:1.73424\n",
      "[112]\tvalidation_0-mae:1.70032\n",
      "[113]\tvalidation_0-mae:1.69579\n",
      "[114]\tvalidation_0-mae:1.65993\n",
      "[115]\tvalidation_0-mae:1.64481\n",
      "[116]\tvalidation_0-mae:1.62924\n",
      "[117]\tvalidation_0-mae:1.62480\n",
      "[118]\tvalidation_0-mae:1.61753\n",
      "[119]\tvalidation_0-mae:1.61454\n",
      "[120]\tvalidation_0-mae:1.59052\n",
      "[121]\tvalidation_0-mae:1.58119\n",
      "[122]\tvalidation_0-mae:1.56177\n",
      "[123]\tvalidation_0-mae:1.55445\n",
      "[124]\tvalidation_0-mae:1.54655\n",
      "[125]\tvalidation_0-mae:1.53067\n",
      "[126]\tvalidation_0-mae:1.53074\n",
      "[127]\tvalidation_0-mae:1.52545\n",
      "[128]\tvalidation_0-mae:1.50834\n",
      "[129]\tvalidation_0-mae:1.50709\n",
      "[130]\tvalidation_0-mae:1.50569\n",
      "[131]\tvalidation_0-mae:1.50437\n",
      "[132]\tvalidation_0-mae:1.50454\n",
      "[133]\tvalidation_0-mae:1.49917\n",
      "[134]\tvalidation_0-mae:1.48764\n",
      "[135]\tvalidation_0-mae:1.48770\n",
      "[136]\tvalidation_0-mae:1.48769\n",
      "Stopping. Best iteration:\n",
      "[134]\tvalidation_0-mae:1.48764\n",
      "\n",
      "[0]\tvalidation_0-mae:9744.38672\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:7110.38037\n",
      "[2]\tvalidation_0-mae:5382.74756\n",
      "[3]\tvalidation_0-mae:4172.73145\n",
      "[4]\tvalidation_0-mae:3257.23706\n",
      "[5]\tvalidation_0-mae:2613.81177\n",
      "[6]\tvalidation_0-mae:2172.20312\n",
      "[7]\tvalidation_0-mae:1844.73584\n",
      "[8]\tvalidation_0-mae:1613.80957\n",
      "[9]\tvalidation_0-mae:1414.81921\n",
      "[10]\tvalidation_0-mae:1268.80640\n",
      "[11]\tvalidation_0-mae:1148.94617\n",
      "[12]\tvalidation_0-mae:1045.92017\n",
      "[13]\tvalidation_0-mae:976.52124\n",
      "[14]\tvalidation_0-mae:916.64923\n",
      "[15]\tvalidation_0-mae:848.28894\n",
      "[16]\tvalidation_0-mae:799.46857\n",
      "[17]\tvalidation_0-mae:749.34961\n",
      "[18]\tvalidation_0-mae:715.45062\n",
      "[19]\tvalidation_0-mae:692.27936\n",
      "[20]\tvalidation_0-mae:654.26196\n",
      "[21]\tvalidation_0-mae:629.05627\n",
      "[22]\tvalidation_0-mae:611.29291\n",
      "[23]\tvalidation_0-mae:585.74493\n",
      "[24]\tvalidation_0-mae:568.72540\n",
      "[25]\tvalidation_0-mae:550.23206\n",
      "[26]\tvalidation_0-mae:535.86145\n",
      "[27]\tvalidation_0-mae:517.87238\n",
      "[28]\tvalidation_0-mae:496.77051\n",
      "[29]\tvalidation_0-mae:482.18637\n",
      "[30]\tvalidation_0-mae:467.00385\n",
      "[31]\tvalidation_0-mae:444.96716\n",
      "[32]\tvalidation_0-mae:431.15463\n",
      "[33]\tvalidation_0-mae:419.71289\n",
      "[34]\tvalidation_0-mae:402.07388\n",
      "[35]\tvalidation_0-mae:393.48962\n",
      "[36]\tvalidation_0-mae:386.20029\n",
      "[37]\tvalidation_0-mae:374.53604\n",
      "[38]\tvalidation_0-mae:366.65427\n",
      "[39]\tvalidation_0-mae:359.23694\n",
      "[40]\tvalidation_0-mae:349.20914\n",
      "[41]\tvalidation_0-mae:341.94583\n",
      "[42]\tvalidation_0-mae:335.85019\n",
      "[43]\tvalidation_0-mae:330.75937\n",
      "[44]\tvalidation_0-mae:323.40884\n",
      "[45]\tvalidation_0-mae:314.61844\n",
      "[46]\tvalidation_0-mae:305.64713\n",
      "[47]\tvalidation_0-mae:301.18243\n",
      "[48]\tvalidation_0-mae:294.47690\n",
      "[49]\tvalidation_0-mae:288.35458\n",
      "[50]\tvalidation_0-mae:281.28342\n",
      "[51]\tvalidation_0-mae:275.87881\n",
      "[52]\tvalidation_0-mae:270.29004\n",
      "[53]\tvalidation_0-mae:263.77930\n",
      "[54]\tvalidation_0-mae:260.80243\n",
      "[55]\tvalidation_0-mae:254.27231\n",
      "[56]\tvalidation_0-mae:247.96668\n",
      "[57]\tvalidation_0-mae:242.91116\n",
      "[58]\tvalidation_0-mae:239.20360\n",
      "[59]\tvalidation_0-mae:235.91336\n",
      "[60]\tvalidation_0-mae:232.20232\n",
      "[61]\tvalidation_0-mae:229.71986\n",
      "[62]\tvalidation_0-mae:225.51181\n",
      "[63]\tvalidation_0-mae:222.26840\n",
      "[64]\tvalidation_0-mae:219.56722\n",
      "[65]\tvalidation_0-mae:216.01039\n",
      "[66]\tvalidation_0-mae:213.78628\n",
      "[67]\tvalidation_0-mae:211.37210\n",
      "[68]\tvalidation_0-mae:209.30005\n",
      "[69]\tvalidation_0-mae:206.68019\n",
      "[70]\tvalidation_0-mae:203.80833\n",
      "[71]\tvalidation_0-mae:202.25890\n",
      "[72]\tvalidation_0-mae:199.13763\n",
      "[73]\tvalidation_0-mae:196.89290\n",
      "[74]\tvalidation_0-mae:194.57342\n",
      "[75]\tvalidation_0-mae:192.28850\n",
      "[76]\tvalidation_0-mae:189.71544\n",
      "[77]\tvalidation_0-mae:188.62804\n",
      "[78]\tvalidation_0-mae:186.95531\n",
      "[79]\tvalidation_0-mae:184.41777\n",
      "[80]\tvalidation_0-mae:183.58617\n",
      "[81]\tvalidation_0-mae:180.78116\n",
      "[82]\tvalidation_0-mae:178.98358\n",
      "[83]\tvalidation_0-mae:176.02536\n",
      "[84]\tvalidation_0-mae:174.49162\n",
      "[85]\tvalidation_0-mae:172.12711\n",
      "[86]\tvalidation_0-mae:170.71747\n",
      "[87]\tvalidation_0-mae:168.69452\n",
      "[88]\tvalidation_0-mae:166.81284\n",
      "[89]\tvalidation_0-mae:165.47493\n",
      "[90]\tvalidation_0-mae:163.58609\n",
      "[91]\tvalidation_0-mae:162.15321\n",
      "[92]\tvalidation_0-mae:159.71266\n",
      "[93]\tvalidation_0-mae:157.83936\n",
      "[94]\tvalidation_0-mae:154.85455\n",
      "[95]\tvalidation_0-mae:154.29913\n",
      "[96]\tvalidation_0-mae:152.28567\n",
      "[97]\tvalidation_0-mae:150.06596\n",
      "[98]\tvalidation_0-mae:147.94428\n",
      "[99]\tvalidation_0-mae:145.75914\n",
      "[100]\tvalidation_0-mae:145.61159\n",
      "[101]\tvalidation_0-mae:144.53752\n",
      "[102]\tvalidation_0-mae:144.09703\n",
      "[103]\tvalidation_0-mae:142.25314\n",
      "[104]\tvalidation_0-mae:140.74455\n",
      "[105]\tvalidation_0-mae:138.25789\n",
      "[106]\tvalidation_0-mae:136.58934\n",
      "[107]\tvalidation_0-mae:134.78262\n",
      "[108]\tvalidation_0-mae:133.13115\n",
      "[109]\tvalidation_0-mae:132.37633\n",
      "[110]\tvalidation_0-mae:131.42137\n",
      "[111]\tvalidation_0-mae:130.45392\n",
      "[112]\tvalidation_0-mae:129.08182\n",
      "[113]\tvalidation_0-mae:128.46876\n",
      "[114]\tvalidation_0-mae:127.54751\n",
      "[115]\tvalidation_0-mae:127.14357\n",
      "[116]\tvalidation_0-mae:124.85918\n",
      "[117]\tvalidation_0-mae:123.59570\n",
      "[118]\tvalidation_0-mae:123.10150\n",
      "[119]\tvalidation_0-mae:121.87244\n",
      "[120]\tvalidation_0-mae:121.17345\n",
      "[121]\tvalidation_0-mae:120.03913\n",
      "[122]\tvalidation_0-mae:118.92044\n",
      "[123]\tvalidation_0-mae:117.94735\n",
      "[124]\tvalidation_0-mae:117.07252\n",
      "[125]\tvalidation_0-mae:116.33465\n",
      "[126]\tvalidation_0-mae:113.99048\n",
      "[127]\tvalidation_0-mae:113.02531\n",
      "[128]\tvalidation_0-mae:112.23037\n",
      "[129]\tvalidation_0-mae:111.04208\n",
      "[130]\tvalidation_0-mae:110.08866\n",
      "[131]\tvalidation_0-mae:108.80877\n",
      "[132]\tvalidation_0-mae:107.40063\n",
      "[133]\tvalidation_0-mae:106.52982\n",
      "[134]\tvalidation_0-mae:105.35426\n",
      "[135]\tvalidation_0-mae:104.66519\n",
      "[136]\tvalidation_0-mae:104.21510\n",
      "[137]\tvalidation_0-mae:102.94724\n",
      "[138]\tvalidation_0-mae:102.14039\n",
      "[139]\tvalidation_0-mae:101.03043\n",
      "[140]\tvalidation_0-mae:100.70738\n",
      "[141]\tvalidation_0-mae:101.11536\n",
      "[142]\tvalidation_0-mae:100.30522\n",
      "[143]\tvalidation_0-mae:99.10934\n",
      "[144]\tvalidation_0-mae:98.60603\n",
      "[145]\tvalidation_0-mae:98.37240\n",
      "[146]\tvalidation_0-mae:97.84497\n",
      "[147]\tvalidation_0-mae:97.66979\n",
      "[148]\tvalidation_0-mae:96.88327\n",
      "[149]\tvalidation_0-mae:96.14546\n",
      "DataFrame: automobile imput_method :trees model :KNN score:  0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :trees model :XGB score:  0.49\n",
      "DataFrame: automobile imput_method :trees model :MLP score:  0.77\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/sample - loss: 121.1160 - mae: 121.1160 - val_loss: 121.3785 - val_mae: 121.3784\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 121.0741 - mae: 121.0741 - val_loss: 121.3102 - val_mae: 121.3102\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 121.0295 - mae: 121.0295 - val_loss: 121.2399 - val_mae: 121.2399\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 120.9818 - mae: 120.9818 - val_loss: 121.1661 - val_mae: 121.1661\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 120.9306 - mae: 120.9306 - val_loss: 121.0905 - val_mae: 121.0905\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 120.8756 - mae: 120.8756 - val_loss: 121.0107 - val_mae: 121.0107\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 120.8164 - mae: 120.8164 - val_loss: 120.9253 - val_mae: 120.9253\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 120.7527 - mae: 120.7527 - val_loss: 120.8365 - val_mae: 120.8365\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 120.6842 - mae: 120.6842 - val_loss: 120.7430 - val_mae: 120.7430\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 120.6109 - mae: 120.6109 - val_loss: 120.6449 - val_mae: 120.6449\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 120.5323 - mae: 120.5323 - val_loss: 120.5421 - val_mae: 120.5421\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 120.4485 - mae: 120.4485 - val_loss: 120.4355 - val_mae: 120.4355\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 120.3591 - mae: 120.3591 - val_loss: 120.3232 - val_mae: 120.3232\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 120.2642 - mae: 120.2642 - val_loss: 120.2055 - val_mae: 120.2055\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 120.1635 - mae: 120.1635 - val_loss: 120.0842 - val_mae: 120.0842\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 120.0570 - mae: 120.0570 - val_loss: 119.9575 - val_mae: 119.9575\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 119.9445 - mae: 119.9445 - val_loss: 119.8258 - val_mae: 119.8258\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 119.8261 - mae: 119.8261 - val_loss: 119.6865 - val_mae: 119.6865\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 119.7016 - mae: 119.7016 - val_loss: 119.5434 - val_mae: 119.5434\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 119.5710 - mae: 119.5710 - val_loss: 119.3964 - val_mae: 119.3964\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 119.4341 - mae: 119.4341 - val_loss: 119.2439 - val_mae: 119.2439\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 119.2911 - mae: 119.2911 - val_loss: 119.0843 - val_mae: 119.0843\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 119.1417 - mae: 119.1417 - val_loss: 118.9206 - val_mae: 118.9206\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 118.9860 - mae: 118.9860 - val_loss: 118.7507 - val_mae: 118.7507\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 118.8239 - mae: 118.8239 - val_loss: 118.5766 - val_mae: 118.5766\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 118.6554 - mae: 118.6554 - val_loss: 118.3924 - val_mae: 118.3924\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 118.4805 - mae: 118.4805 - val_loss: 118.2064 - val_mae: 118.2064\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 118.2991 - mae: 118.2991 - val_loss: 118.0149 - val_mae: 118.0149\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 118.1112 - mae: 118.1112 - val_loss: 117.8138 - val_mae: 117.8138\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 117.9169 - mae: 117.9169 - val_loss: 117.6095 - val_mae: 117.6095\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 117.7160 - mae: 117.7160 - val_loss: 117.3993 - val_mae: 117.3993\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 117.5085 - mae: 117.5085 - val_loss: 117.1786 - val_mae: 117.1786\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 117.2945 - mae: 117.2945 - val_loss: 116.9531 - val_mae: 116.9531\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 117.0740 - mae: 117.0740 - val_loss: 116.7259 - val_mae: 116.7259\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 116.8468 - mae: 116.8468 - val_loss: 116.4964 - val_mae: 116.4964\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 116.6131 - mae: 116.6131 - val_loss: 116.2613 - val_mae: 116.2613\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 116.3727 - mae: 116.3727 - val_loss: 116.0156 - val_mae: 116.0156\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 116.1258 - mae: 116.1258 - val_loss: 115.7643 - val_mae: 115.7643\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 115.8723 - mae: 115.8723 - val_loss: 115.5089 - val_mae: 115.5089\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 115.6121 - mae: 115.6121 - val_loss: 115.2423 - val_mae: 115.2423\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 115.3453 - mae: 115.3453 - val_loss: 114.9705 - val_mae: 114.9705\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 115.0719 - mae: 115.0719 - val_loss: 114.6923 - val_mae: 114.6923\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 114.7919 - mae: 114.7919 - val_loss: 114.4100 - val_mae: 114.4100\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 114.5053 - mae: 114.5053 - val_loss: 114.1177 - val_mae: 114.1177\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 114.2120 - mae: 114.2120 - val_loss: 113.8195 - val_mae: 113.8195\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 113.9122 - mae: 113.9122 - val_loss: 113.5175 - val_mae: 113.5175\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 113.6057 - mae: 113.6057 - val_loss: 113.2129 - val_mae: 113.2129\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 113.2926 - mae: 113.2926 - val_loss: 112.8973 - val_mae: 112.8973\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 112.9728 - mae: 112.9729 - val_loss: 112.5734 - val_mae: 112.5734\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 112.6465 - mae: 112.6465 - val_loss: 112.2408 - val_mae: 112.2408\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 112.3136 - mae: 112.3136 - val_loss: 111.9051 - val_mae: 111.9051\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 111.9741 - mae: 111.9741 - val_loss: 111.5672 - val_mae: 111.5672\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 111.6279 - mae: 111.6279 - val_loss: 111.2146 - val_mae: 111.2146\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 111.2752 - mae: 111.2752 - val_loss: 110.8555 - val_mae: 110.8555\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 110.9159 - mae: 110.9159 - val_loss: 110.5012 - val_mae: 110.5013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 110.5501 - mae: 110.5501 - val_loss: 110.1342 - val_mae: 110.1342\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 110.1776 - mae: 110.1776 - val_loss: 109.7673 - val_mae: 109.7673\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 109.7986 - mae: 109.7986 - val_loss: 109.3844 - val_mae: 109.3844\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 109.4130 - mae: 109.4130 - val_loss: 108.9889 - val_mae: 108.9889\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 109.0209 - mae: 109.0209 - val_loss: 108.5937 - val_mae: 108.5937\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 108.6223 - mae: 108.6223 - val_loss: 108.1933 - val_mae: 108.1933\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 108.2171 - mae: 108.2171 - val_loss: 107.7817 - val_mae: 107.7817\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 107.8054 - mae: 107.8054 - val_loss: 107.3553 - val_mae: 107.3553\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 107.3871 - mae: 107.3871 - val_loss: 106.9307 - val_mae: 106.9307\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 106.9624 - mae: 106.9624 - val_loss: 106.4904 - val_mae: 106.4904\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 106.5311 - mae: 106.5311 - val_loss: 106.0555 - val_mae: 106.0555\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 106.0934 - mae: 106.0934 - val_loss: 105.6109 - val_mae: 105.6109\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 105.6492 - mae: 105.6491 - val_loss: 105.1611 - val_mae: 105.1611\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 105.1984 - mae: 105.1984 - val_loss: 104.7023 - val_mae: 104.7023\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 104.7413 - mae: 104.7413 - val_loss: 104.2488 - val_mae: 104.2488\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 164us/sample - loss: 104.2777 - mae: 104.2776 - val_loss: 103.8009 - val_mae: 103.8009\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 103.8076 - mae: 103.8076 - val_loss: 103.3378 - val_mae: 103.3378\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 103.3311 - mae: 103.3311 - val_loss: 102.8564 - val_mae: 102.8564\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 102.8481 - mae: 102.8481 - val_loss: 102.3632 - val_mae: 102.3632\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 102.3588 - mae: 102.3588 - val_loss: 101.8589 - val_mae: 101.8588\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 101.8630 - mae: 101.8630 - val_loss: 101.3522 - val_mae: 101.3522\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 101.3609 - mae: 101.3609 - val_loss: 100.8304 - val_mae: 100.8304\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 100.8523 - mae: 100.8523 - val_loss: 100.3056 - val_mae: 100.3056\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 100.3374 - mae: 100.3374 - val_loss: 99.7822 - val_mae: 99.7822\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 99.8161 - mae: 99.8161 - val_loss: 99.2553 - val_mae: 99.2553\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 99.2885 - mae: 99.2885 - val_loss: 98.7180 - val_mae: 98.7179\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 98.7545 - mae: 98.7545 - val_loss: 98.1920 - val_mae: 98.1920\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 98.2141 - mae: 98.2141 - val_loss: 97.6410 - val_mae: 97.6410\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 97.6675 - mae: 97.6675 - val_loss: 97.1007 - val_mae: 97.1007\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 97.1145 - mae: 97.1145 - val_loss: 96.5347 - val_mae: 96.5347\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 96.5552 - mae: 96.5552 - val_loss: 95.9762 - val_mae: 95.9762\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.9897 - mae: 95.9897 - val_loss: 95.4107 - val_mae: 95.4107\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.4178 - mae: 95.4178 - val_loss: 94.8388 - val_mae: 94.8388\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 94.8397 - mae: 94.8397 - val_loss: 94.2586 - val_mae: 94.2586\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 94.2553 - mae: 94.2553 - val_loss: 93.6473 - val_mae: 93.6473\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 93.6646 - mae: 93.6646 - val_loss: 92.9978 - val_mae: 92.9978\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 93.0678 - mae: 93.0678 - val_loss: 92.3797 - val_mae: 92.3797\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 92.4647 - mae: 92.4647 - val_loss: 91.7567 - val_mae: 91.7567\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.8553 - mae: 91.8553 - val_loss: 91.1362 - val_mae: 91.1362\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.2398 - mae: 91.2398 - val_loss: 90.5382 - val_mae: 90.5382\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 90.6180 - mae: 90.6180 - val_loss: 89.9320 - val_mae: 89.9319\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 89.9901 - mae: 89.9901 - val_loss: 89.2949 - val_mae: 89.2949\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 89.3560 - mae: 89.3560 - val_loss: 88.6373 - val_mae: 88.6373\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 88.7157 - mae: 88.7157 - val_loss: 87.9841 - val_mae: 87.9841\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 88.0693 - mae: 88.0693 - val_loss: 87.3452 - val_mae: 87.3452\n",
      "159/159 [==============================] - 0s 44us/sample - loss: 87.3452 - mae: 87.3452\n",
      "Val score is 87.34516906738281\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 7ms/sample - loss: 0.6851 - accuracy: 0.6478 - val_loss: 0.7374 - val_accuracy: 0.4528\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 0.5475 - accuracy: 0.6981 - val_loss: 0.6815 - val_accuracy: 0.4654\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 0.4448 - accuracy: 0.7862 - val_loss: 0.6320 - val_accuracy: 0.5660\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 0.3931 - accuracy: 0.8428 - val_loss: 0.5905 - val_accuracy: 0.6415\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 0.3447 - accuracy: 0.8428 - val_loss: 0.5532 - val_accuracy: 0.7107\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.3206 - accuracy: 0.9119 - val_loss: 0.5185 - val_accuracy: 0.7610\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 153us/sample - loss: 0.2857 - accuracy: 0.9119 - val_loss: 0.4892 - val_accuracy: 0.8113\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 0.2513 - accuracy: 0.9182 - val_loss: 0.4604 - val_accuracy: 0.8742\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 0.2303 - accuracy: 0.9057 - val_loss: 0.4352 - val_accuracy: 0.8868\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 0.2181 - accuracy: 0.9371 - val_loss: 0.4109 - val_accuracy: 0.8868\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 0.2377 - accuracy: 0.8994 - val_loss: 0.3891 - val_accuracy: 0.8868\n",
      "159/159 [==============================] - 0s 45us/sample - loss: 0.3891 - accuracy: 0.8868\n",
      "Val score is 0.8867924809455872\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/sample - loss: 3.2802 - mae: 3.2802 - val_loss: 3.8428 - val_mae: 3.8428\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 3.2287 - mae: 3.2287 - val_loss: 3.7565 - val_mae: 3.7565\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 3.1837 - mae: 3.1837 - val_loss: 3.6228 - val_mae: 3.6228\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 3.1222 - mae: 3.1222 - val_loss: 3.5002 - val_mae: 3.5002\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 3.0660 - mae: 3.0660 - val_loss: 3.3935 - val_mae: 3.3935\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 3.0064 - mae: 3.0064 - val_loss: 3.2924 - val_mae: 3.2924\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 156us/sample - loss: 2.9432 - mae: 2.9432 - val_loss: 3.1918 - val_mae: 3.1918\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 2.8809 - mae: 2.8809 - val_loss: 3.0697 - val_mae: 3.0697\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 2.8079 - mae: 2.8079 - val_loss: 2.9499 - val_mae: 2.9499\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.7339 - mae: 2.7339 - val_loss: 2.8364 - val_mae: 2.8364\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 2.6547 - mae: 2.6547 - val_loss: 2.7222 - val_mae: 2.7222\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 156us/sample - loss: 2.5735 - mae: 2.5735 - val_loss: 2.5801 - val_mae: 2.5801\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 2.4908 - mae: 2.4908 - val_loss: 2.4401 - val_mae: 2.4401\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 2.3891 - mae: 2.3891 - val_loss: 2.3079 - val_mae: 2.3079\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 154us/sample - loss: 2.3033 - mae: 2.3033 - val_loss: 2.1689 - val_mae: 2.1689\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.1896 - mae: 2.1896 - val_loss: 2.0324 - val_mae: 2.0324\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 2.0797 - mae: 2.0797 - val_loss: 1.9018 - val_mae: 1.9018\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 1.9677 - mae: 1.9677 - val_loss: 1.7597 - val_mae: 1.7597\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 1.8459 - mae: 1.8459 - val_loss: 1.6113 - val_mae: 1.6113\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 1.7382 - mae: 1.7382 - val_loss: 1.4384 - val_mae: 1.4384\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.6584 - mae: 1.6584 - val_loss: 1.2775 - val_mae: 1.2775\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 155us/sample - loss: 1.4803 - mae: 1.4803 - val_loss: 1.1256 - val_mae: 1.1256\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.3710 - mae: 1.3710 - val_loss: 1.0189 - val_mae: 1.0189\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 1.2434 - mae: 1.2434 - val_loss: 0.8863 - val_mae: 0.8863\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.1719 - mae: 1.1719 - val_loss: 0.7200 - val_mae: 0.7200\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 1.0132 - mae: 1.0132 - val_loss: 0.6219 - val_mae: 0.6219\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.9150 - mae: 0.9150 - val_loss: 0.5149 - val_mae: 0.5149\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.8264 - mae: 0.8264 - val_loss: 0.4409 - val_mae: 0.4409\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 0.7474 - mae: 0.7474 - val_loss: 0.3416 - val_mae: 0.3416\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 0.6715 - mae: 0.6715 - val_loss: 0.3492 - val_mae: 0.3492\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.6067 - mae: 0.6067 - val_loss: 0.3592 - val_mae: 0.3592\n",
      "159/159 [==============================] - 0s 51us/sample - loss: 0.3592 - mae: 0.3592\n",
      "Val score is 0.3591989576816559\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 6ms/sample - loss: 3.2483 - mae: 3.2483 - val_loss: 2.7612 - val_mae: 2.7612\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 3.1721 - mae: 3.1721 - val_loss: 2.6838 - val_mae: 2.6838\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 130us/sample - loss: 3.1109 - mae: 3.1109 - val_loss: 2.6204 - val_mae: 2.6204\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 153us/sample - loss: 3.0574 - mae: 3.0574 - val_loss: 2.5674 - val_mae: 2.5674\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 3.0041 - mae: 3.0041 - val_loss: 2.5280 - val_mae: 2.5280\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.9498 - mae: 2.9498 - val_loss: 2.4966 - val_mae: 2.4966\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 2.8941 - mae: 2.8941 - val_loss: 2.4562 - val_mae: 2.4562\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.8308 - mae: 2.8308 - val_loss: 2.4030 - val_mae: 2.4030\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 2.7657 - mae: 2.7657 - val_loss: 2.3489 - val_mae: 2.3489\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 258us/sample - loss: 2.7148 - mae: 2.7148 - val_loss: 2.3013 - val_mae: 2.3013\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 2.6307 - mae: 2.6307 - val_loss: 2.2491 - val_mae: 2.2491\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 2.5568 - mae: 2.5568 - val_loss: 2.1872 - val_mae: 2.1872\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 2.4809 - mae: 2.4809 - val_loss: 2.1105 - val_mae: 2.1105\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.3946 - mae: 2.3946 - val_loss: 2.0238 - val_mae: 2.0238\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.3058 - mae: 2.3058 - val_loss: 1.9348 - val_mae: 1.9348\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 2.2233 - mae: 2.2233 - val_loss: 1.8398 - val_mae: 1.8398\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 2.1169 - mae: 2.1169 - val_loss: 1.7282 - val_mae: 1.7282\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 2.0147 - mae: 2.0147 - val_loss: 1.5969 - val_mae: 1.5969\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 1.9171 - mae: 1.9171 - val_loss: 1.4991 - val_mae: 1.4991\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.8052 - mae: 1.8052 - val_loss: 1.3817 - val_mae: 1.3817\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.7022 - mae: 1.7022 - val_loss: 1.2827 - val_mae: 1.2827\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.6061 - mae: 1.6061 - val_loss: 1.1697 - val_mae: 1.1697\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.4603 - mae: 1.4603 - val_loss: 1.0521 - val_mae: 1.0521\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.3313 - mae: 1.3313 - val_loss: 0.9211 - val_mae: 0.9211\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 1.2268 - mae: 1.2268 - val_loss: 0.8292 - val_mae: 0.8292\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 145us/sample - loss: 1.1425 - mae: 1.1425 - val_loss: 0.6923 - val_mae: 0.6923\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 1.0252 - mae: 1.0252 - val_loss: 0.5969 - val_mae: 0.5969\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 0.9189 - mae: 0.9189 - val_loss: 0.5118 - val_mae: 0.5118\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 0.8187 - mae: 0.8187 - val_loss: 0.4255 - val_mae: 0.4255\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 0.7005 - mae: 0.7005 - val_loss: 0.3964 - val_mae: 0.3964\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.6148 - mae: 0.6148 - val_loss: 0.3514 - val_mae: 0.3514\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 0.5887 - mae: 0.5887 - val_loss: 0.3498 - val_mae: 0.3498\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.5165 - mae: 0.5165 - val_loss: 0.3376 - val_mae: 0.3376\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 0.5567 - mae: 0.5567 - val_loss: 0.3130 - val_mae: 0.3130\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.4867 - mae: 0.4867 - val_loss: 0.3478 - val_mae: 0.3478\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 0.4684 - mae: 0.4684 - val_loss: 0.3309 - val_mae: 0.3309\n",
      "159/159 [==============================] - 0s 55us/sample - loss: 0.3309 - mae: 0.3309\n",
      "Val score is 0.33085858821868896\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/sample - loss: 95.8202 - mae: 95.8202 - val_loss: 95.0126 - val_mae: 95.0126\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 95.7778 - mae: 95.7778 - val_loss: 94.9812 - val_mae: 94.9812\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 95.7326 - mae: 95.7326 - val_loss: 94.9460 - val_mae: 94.9460\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.6844 - mae: 95.6844 - val_loss: 94.9063 - val_mae: 94.9063\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.6327 - mae: 95.6327 - val_loss: 94.8637 - val_mae: 94.8637\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.5770 - mae: 95.5770 - val_loss: 94.8167 - val_mae: 94.8167\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 133us/sample - loss: 95.5172 - mae: 95.5172 - val_loss: 94.7652 - val_mae: 94.7652\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 95.4528 - mae: 95.4528 - val_loss: 94.7090 - val_mae: 94.7090\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 95.3837 - mae: 95.3837 - val_loss: 94.6484 - val_mae: 94.6484\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.3095 - mae: 95.3095 - val_loss: 94.5821 - val_mae: 94.5821\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 95.2301 - mae: 95.2301 - val_loss: 94.5117 - val_mae: 94.5117\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 95.1454 - mae: 95.1454 - val_loss: 94.4363 - val_mae: 94.4363\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 95.0551 - mae: 95.0551 - val_loss: 94.3549 - val_mae: 94.3549\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 94.9592 - mae: 94.9592 - val_loss: 94.2677 - val_mae: 94.2677\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 94.8574 - mae: 94.8574 - val_loss: 94.1731 - val_mae: 94.1731\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 94.7498 - mae: 94.7498 - val_loss: 94.0755 - val_mae: 94.0755\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 94.6363 - mae: 94.6363 - val_loss: 93.9740 - val_mae: 93.9740\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 94.5166 - mae: 94.5166 - val_loss: 93.8650 - val_mae: 93.8651\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 94.3909 - mae: 94.3909 - val_loss: 93.7496 - val_mae: 93.7496\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 94.2590 - mae: 94.2590 - val_loss: 93.6283 - val_mae: 93.6283\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 94.1209 - mae: 94.1209 - val_loss: 93.5026 - val_mae: 93.5026\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 156us/sample - loss: 93.9765 - mae: 93.9765 - val_loss: 93.3670 - val_mae: 93.3670\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 93.8258 - mae: 93.8258 - val_loss: 93.2263 - val_mae: 93.2263\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 93.6687 - mae: 93.6687 - val_loss: 93.0788 - val_mae: 93.0788\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 93.5052 - mae: 93.5052 - val_loss: 92.9260 - val_mae: 92.9260\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 93.3353 - mae: 93.3354 - val_loss: 92.7659 - val_mae: 92.7659\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 93.1590 - mae: 93.1590 - val_loss: 92.5993 - val_mae: 92.5993\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 92.9762 - mae: 92.9762 - val_loss: 92.4264 - val_mae: 92.4265\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 92.7869 - mae: 92.7869 - val_loss: 92.2442 - val_mae: 92.2442\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 92.5911 - mae: 92.5911 - val_loss: 92.0619 - val_mae: 92.0619\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 92.3887 - mae: 92.3887 - val_loss: 91.8736 - val_mae: 91.8736\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 92.1799 - mae: 92.1799 - val_loss: 91.6723 - val_mae: 91.6723\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.9644 - mae: 91.9644 - val_loss: 91.4647 - val_mae: 91.4647\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 91.7424 - mae: 91.7424 - val_loss: 91.2513 - val_mae: 91.2513\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.5138 - mae: 91.5138 - val_loss: 91.0292 - val_mae: 91.0292\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.2786 - mae: 91.2786 - val_loss: 90.8006 - val_mae: 90.8006\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.0369 - mae: 91.0369 - val_loss: 90.5656 - val_mae: 90.5656\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 90.7885 - mae: 90.7885 - val_loss: 90.3241 - val_mae: 90.3241\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 90.5336 - mae: 90.5335 - val_loss: 90.0755 - val_mae: 90.0755\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 90.2720 - mae: 90.2720 - val_loss: 89.8204 - val_mae: 89.8204\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 90.0038 - mae: 90.0038 - val_loss: 89.5521 - val_mae: 89.5521\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 89.7290 - mae: 89.7290 - val_loss: 89.2803 - val_mae: 89.2803\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 89.4476 - mae: 89.4476 - val_loss: 89.0078 - val_mae: 89.0078\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 89.1596 - mae: 89.1596 - val_loss: 88.7323 - val_mae: 88.7323\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 88.8650 - mae: 88.8650 - val_loss: 88.4494 - val_mae: 88.4494\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 88.5637 - mae: 88.5637 - val_loss: 88.1588 - val_mae: 88.1588\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 145us/sample - loss: 88.2559 - mae: 88.2559 - val_loss: 87.8564 - val_mae: 87.8564\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 87.9414 - mae: 87.9414 - val_loss: 87.5456 - val_mae: 87.5456\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 87.6204 - mae: 87.6204 - val_loss: 87.2397 - val_mae: 87.2397\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 87.2927 - mae: 87.2927 - val_loss: 86.9278 - val_mae: 86.9278\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 86.9585 - mae: 86.9585 - val_loss: 86.5997 - val_mae: 86.5997\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 86.6177 - mae: 86.6177 - val_loss: 86.2600 - val_mae: 86.2600\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 86.2703 - mae: 86.2703 - val_loss: 85.9129 - val_mae: 85.9129\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 85.9163 - mae: 85.9163 - val_loss: 85.5634 - val_mae: 85.5634\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 85.5557 - mae: 85.5557 - val_loss: 85.2049 - val_mae: 85.2049\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 85.1886 - mae: 85.1886 - val_loss: 84.8347 - val_mae: 84.8347\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 84.8149 - mae: 84.8149 - val_loss: 84.4598 - val_mae: 84.4598\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 84.4347 - mae: 84.4347 - val_loss: 84.0770 - val_mae: 84.0770\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 84.0479 - mae: 84.0479 - val_loss: 83.6794 - val_mae: 83.6794\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 83.6546 - mae: 83.6546 - val_loss: 83.2781 - val_mae: 83.2781\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 83.2548 - mae: 83.2548 - val_loss: 82.8773 - val_mae: 82.8773\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 82.8484 - mae: 82.8484 - val_loss: 82.4733 - val_mae: 82.4733\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 129us/sample - loss: 82.4355 - mae: 82.4355 - val_loss: 82.0685 - val_mae: 82.0685\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 82.0161 - mae: 82.0161 - val_loss: 81.6644 - val_mae: 81.6644\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 81.5902 - mae: 81.5902 - val_loss: 81.2448 - val_mae: 81.2448\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 81.1578 - mae: 81.1578 - val_loss: 80.8237 - val_mae: 80.8237\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 80.7190 - mae: 80.7190 - val_loss: 80.3961 - val_mae: 80.3961\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 80.2736 - mae: 80.2736 - val_loss: 79.9392 - val_mae: 79.9392\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 79.8218 - mae: 79.8218 - val_loss: 79.4750 - val_mae: 79.4750\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 79.3636 - mae: 79.3635 - val_loss: 78.9934 - val_mae: 78.9934\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 78.8989 - mae: 78.8988 - val_loss: 78.5154 - val_mae: 78.5154\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 78.4277 - mae: 78.4277 - val_loss: 78.0513 - val_mae: 78.0512\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 77.9501 - mae: 77.9501 - val_loss: 77.5836 - val_mae: 77.5836\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 77.4662 - mae: 77.4662 - val_loss: 77.1041 - val_mae: 77.1041\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 76.9758 - mae: 76.9758 - val_loss: 76.6178 - val_mae: 76.6178\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 76.4790 - mae: 76.4790 - val_loss: 76.1238 - val_mae: 76.1238\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 75.9758 - mae: 75.9758 - val_loss: 75.6183 - val_mae: 75.6183\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 75.4662 - mae: 75.4662 - val_loss: 75.1165 - val_mae: 75.1165\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 74.9503 - mae: 74.9503 - val_loss: 74.6078 - val_mae: 74.6078\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 74.4280 - mae: 74.4280 - val_loss: 74.0830 - val_mae: 74.0830\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 73.8994 - mae: 73.8994 - val_loss: 73.5407 - val_mae: 73.5407\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 73.3644 - mae: 73.3644 - val_loss: 72.9865 - val_mae: 72.9865\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 72.8232 - mae: 72.8232 - val_loss: 72.4372 - val_mae: 72.4372\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 72.2755 - mae: 72.2755 - val_loss: 71.9015 - val_mae: 71.9015\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 71.7216 - mae: 71.7216 - val_loss: 71.3550 - val_mae: 71.3550\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 71.1614 - mae: 71.1614 - val_loss: 70.8070 - val_mae: 70.8070\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 70.5949 - mae: 70.5949 - val_loss: 70.2494 - val_mae: 70.2494\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 70.0221 - mae: 70.0221 - val_loss: 69.7039 - val_mae: 69.7039\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 69.4431 - mae: 69.4431 - val_loss: 69.1445 - val_mae: 69.1445\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 68.8780 - mae: 68.8780 - val_loss: 68.1185 - val_mae: 68.1184\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 131us/sample - loss: 68.3034 - mae: 68.3034 - val_loss: 66.3785 - val_mae: 66.3785\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 67.6777 - mae: 67.6777 - val_loss: 64.8599 - val_mae: 64.8599\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 67.0777 - mae: 67.0777 - val_loss: 63.8166 - val_mae: 63.8166\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 66.4698 - mae: 66.4698 - val_loss: 63.0292 - val_mae: 63.0292\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 65.8546 - mae: 65.8546 - val_loss: 62.3871 - val_mae: 62.3871\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 65.2326 - mae: 65.2326 - val_loss: 61.8204 - val_mae: 61.8204\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 64.6041 - mae: 64.6041 - val_loss: 61.2830 - val_mae: 61.2830\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 63.9692 - mae: 63.9692 - val_loss: 60.7500 - val_mae: 60.7500\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 63.3281 - mae: 63.3281 - val_loss: 60.2273 - val_mae: 60.2273\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 62.6807 - mae: 62.6807 - val_loss: 59.7009 - val_mae: 59.7009\n",
      "159/159 [==============================] - 0s 53us/sample - loss: 59.7009 - mae: 59.7009\n",
      "Val score is 59.70094299316406\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 7ms/sample - loss: 5113.8157 - mae: 5113.8159 - val_loss: 5113.5227 - val_mae: 5113.5229\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5113.7627 - mae: 5113.7627 - val_loss: 5113.4674 - val_mae: 5113.4673\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5113.7067 - mae: 5113.7065 - val_loss: 5113.4090 - val_mae: 5113.4092\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 5113.6475 - mae: 5113.6475 - val_loss: 5113.3472 - val_mae: 5113.3472\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 5113.5851 - mae: 5113.5850 - val_loss: 5113.2812 - val_mae: 5113.2812\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5113.5187 - mae: 5113.5190 - val_loss: 5113.2113 - val_mae: 5113.2114\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 152us/sample - loss: 5113.4483 - mae: 5113.4487 - val_loss: 5113.1356 - val_mae: 5113.1357\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 5113.3733 - mae: 5113.3735 - val_loss: 5113.0576 - val_mae: 5113.0576\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5113.2935 - mae: 5113.2935 - val_loss: 5112.9765 - val_mae: 5112.9766\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 5113.2093 - mae: 5113.2090 - val_loss: 5112.8873 - val_mae: 5112.8872\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5113.1197 - mae: 5113.1196 - val_loss: 5112.7947 - val_mae: 5112.7949\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 5113.0251 - mae: 5113.0254 - val_loss: 5112.6972 - val_mae: 5112.6973\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 5112.9248 - mae: 5112.9248 - val_loss: 5112.5932 - val_mae: 5112.5938\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.8193 - mae: 5112.8193 - val_loss: 5112.4848 - val_mae: 5112.4849\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.7084 - mae: 5112.7085 - val_loss: 5112.3702 - val_mae: 5112.3701\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.5917 - mae: 5112.5918 - val_loss: 5112.2513 - val_mae: 5112.2515\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5112.4691 - mae: 5112.4692 - val_loss: 5112.1272 - val_mae: 5112.1270\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5112.3405 - mae: 5112.3403 - val_loss: 5111.9990 - val_mae: 5111.9990\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.2063 - mae: 5112.2061 - val_loss: 5111.8627 - val_mae: 5111.8628\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.0661 - mae: 5112.0659 - val_loss: 5111.7226 - val_mae: 5111.7227\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5111.9197 - mae: 5111.9199 - val_loss: 5111.5749 - val_mae: 5111.5752\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5111.7673 - mae: 5111.7671 - val_loss: 5111.4206 - val_mae: 5111.4204\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5111.6089 - mae: 5111.6089 - val_loss: 5111.2610 - val_mae: 5111.2612\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 5111.4441 - mae: 5111.4443 - val_loss: 5111.0967 - val_mae: 5111.0967\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 5111.2734 - mae: 5111.2734 - val_loss: 5110.9264 - val_mae: 5110.9263\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5111.0963 - mae: 5111.0962 - val_loss: 5110.7511 - val_mae: 5110.7510\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5110.9130 - mae: 5110.9126 - val_loss: 5110.5674 - val_mae: 5110.5674\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 5110.7233 - mae: 5110.7231 - val_loss: 5110.3787 - val_mae: 5110.3789\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5110.5275 - mae: 5110.5273 - val_loss: 5110.1842 - val_mae: 5110.1841\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5110.3249 - mae: 5110.3252 - val_loss: 5109.9821 - val_mae: 5109.9824\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 5110.1165 - mae: 5110.1162 - val_loss: 5109.7738 - val_mae: 5109.7734\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5109.9013 - mae: 5109.9009 - val_loss: 5109.5587 - val_mae: 5109.5591\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 5109.6797 - mae: 5109.6797 - val_loss: 5109.3361 - val_mae: 5109.3364\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5109.4520 - mae: 5109.4521 - val_loss: 5109.1091 - val_mae: 5109.1089\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 5109.2175 - mae: 5109.2178 - val_loss: 5108.8757 - val_mae: 5108.8760\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5108.9770 - mae: 5108.9771 - val_loss: 5108.6350 - val_mae: 5108.6353\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5108.7298 - mae: 5108.7300 - val_loss: 5108.3927 - val_mae: 5108.3921\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5108.4761 - mae: 5108.4761 - val_loss: 5108.1387 - val_mae: 5108.1387\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5108.2159 - mae: 5108.2163 - val_loss: 5107.8810 - val_mae: 5107.8809\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 5107.9493 - mae: 5107.9492 - val_loss: 5107.6157 - val_mae: 5107.6157\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5107.6761 - mae: 5107.6763 - val_loss: 5107.3462 - val_mae: 5107.3457\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5107.3964 - mae: 5107.3965 - val_loss: 5107.0663 - val_mae: 5107.0664\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5107.1107 - mae: 5107.1108 - val_loss: 5106.7774 - val_mae: 5106.7773\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5106.8178 - mae: 5106.8174 - val_loss: 5106.4888 - val_mae: 5106.4888\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 5106.5188 - mae: 5106.5190 - val_loss: 5106.1906 - val_mae: 5106.1904\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5106.2130 - mae: 5106.2129 - val_loss: 5105.8885 - val_mae: 5105.8882\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5105.9008 - mae: 5105.9009 - val_loss: 5105.5794 - val_mae: 5105.5796\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 5105.5824 - mae: 5105.5825 - val_loss: 5105.2622 - val_mae: 5105.2622\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5105.2573 - mae: 5105.2573 - val_loss: 5104.9416 - val_mae: 5104.9414\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5104.9256 - mae: 5104.9258 - val_loss: 5104.6076 - val_mae: 5104.6079\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 5104.5873 - mae: 5104.5874 - val_loss: 5104.2721 - val_mae: 5104.2720\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5104.2429 - mae: 5104.2432 - val_loss: 5103.9353 - val_mae: 5103.9351\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5103.8916 - mae: 5103.8916 - val_loss: 5103.5845 - val_mae: 5103.5840\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 5103.5341 - mae: 5103.5342 - val_loss: 5103.2340 - val_mae: 5103.2344\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5103.1700 - mae: 5103.1699 - val_loss: 5102.8825 - val_mae: 5102.8823\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5102.7993 - mae: 5102.7993 - val_loss: 5102.5189 - val_mae: 5102.5190\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5102.4221 - mae: 5102.4224 - val_loss: 5102.1504 - val_mae: 5102.1504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5102.0386 - mae: 5102.0391 - val_loss: 5101.7742 - val_mae: 5101.7739\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5101.6484 - mae: 5101.6484 - val_loss: 5101.3874 - val_mae: 5101.3872\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5101.2519 - mae: 5101.2515 - val_loss: 5100.9892 - val_mae: 5100.9893\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 5100.8489 - mae: 5100.8491 - val_loss: 5100.5870 - val_mae: 5100.5874\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5100.4393 - mae: 5100.4395 - val_loss: 5100.1624 - val_mae: 5100.1621\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 5100.0234 - mae: 5100.0229 - val_loss: 5099.7379 - val_mae: 5099.7378\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 5099.6011 - mae: 5099.6011 - val_loss: 5099.3014 - val_mae: 5099.3013\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 155us/sample - loss: 5099.1722 - mae: 5099.1724 - val_loss: 5098.8562 - val_mae: 5098.8564\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 5098.7370 - mae: 5098.7368 - val_loss: 5098.4106 - val_mae: 5098.4106\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 130us/sample - loss: 5098.2953 - mae: 5098.2954 - val_loss: 5097.9523 - val_mae: 5097.9521\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 126us/sample - loss: 5097.8469 - mae: 5097.8472 - val_loss: 5097.4993 - val_mae: 5097.4990\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5097.3924 - mae: 5097.3921 - val_loss: 5097.0555 - val_mae: 5097.0557\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 5096.9316 - mae: 5096.9316 - val_loss: 5096.5952 - val_mae: 5096.5957\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5096.4642 - mae: 5096.4644 - val_loss: 5096.1270 - val_mae: 5096.1270\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5095.9907 - mae: 5095.9907 - val_loss: 5095.6483 - val_mae: 5095.6484\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 126us/sample - loss: 5095.5103 - mae: 5095.5103 - val_loss: 5095.1612 - val_mae: 5095.1611\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5095.0241 - mae: 5095.0239 - val_loss: 5094.6702 - val_mae: 5094.6704\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 5094.5311 - mae: 5094.5312 - val_loss: 5094.1709 - val_mae: 5094.1709\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 5094.0318 - mae: 5094.0312 - val_loss: 5093.6681 - val_mae: 5093.6685\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5093.5263 - mae: 5093.5259 - val_loss: 5093.1602 - val_mae: 5093.1602\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 5093.0144 - mae: 5093.0142 - val_loss: 5092.6558 - val_mae: 5092.6562\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5092.4960 - mae: 5092.4961 - val_loss: 5092.1660 - val_mae: 5092.1660\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 129us/sample - loss: 5091.9715 - mae: 5091.9717 - val_loss: 5091.6497 - val_mae: 5091.6504\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 152us/sample - loss: 5091.4408 - mae: 5091.4404 - val_loss: 5091.1099 - val_mae: 5091.1099\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 5090.9035 - mae: 5090.9038 - val_loss: 5090.5592 - val_mae: 5090.5596\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5090.3600 - mae: 5090.3599 - val_loss: 5089.9871 - val_mae: 5089.9868\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 5089.8102 - mae: 5089.8105 - val_loss: 5089.4130 - val_mae: 5089.4126\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 5089.2542 - mae: 5089.2539 - val_loss: 5088.8526 - val_mae: 5088.8525\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5088.6919 - mae: 5088.6919 - val_loss: 5088.3009 - val_mae: 5088.3013\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5088.1232 - mae: 5088.1235 - val_loss: 5087.7262 - val_mae: 5087.7261\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5087.5485 - mae: 5087.5488 - val_loss: 5087.1445 - val_mae: 5087.1445\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5086.9675 - mae: 5086.9673 - val_loss: 5086.5638 - val_mae: 5086.5635\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5086.3802 - mae: 5086.3799 - val_loss: 5085.9866 - val_mae: 5085.9868\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5085.7868 - mae: 5085.7871 - val_loss: 5085.3978 - val_mae: 5085.3979\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5085.1870 - mae: 5085.1870 - val_loss: 5084.8001 - val_mae: 5084.7998\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 5084.5809 - mae: 5084.5811 - val_loss: 5084.1943 - val_mae: 5084.1943\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5083.9688 - mae: 5083.9688 - val_loss: 5083.5817 - val_mae: 5083.5815\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5083.3505 - mae: 5083.3506 - val_loss: 5082.9369 - val_mae: 5082.9370\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5082.7262 - mae: 5082.7266 - val_loss: 5082.2913 - val_mae: 5082.2915\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5082.0955 - mae: 5082.0957 - val_loss: 5081.6578 - val_mae: 5081.6582\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5081.4587 - mae: 5081.4585 - val_loss: 5081.0275 - val_mae: 5081.0273\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5080.8157 - mae: 5080.8154 - val_loss: 5080.3800 - val_mae: 5080.3799\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5080.1668 - mae: 5080.1665 - val_loss: 5079.7063 - val_mae: 5079.7065\n",
      "159/159 [==============================] - 0s 50us/sample - loss: 5079.7063 - mae: 5079.7065\n",
      "Val score is 5079.70654296875\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/sample - loss: 11445.7118 - mae: 11445.7119 - val_loss: 11445.6955 - val_mae: 11445.6953\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.6656 - mae: 11445.6650 - val_loss: 11445.6466 - val_mae: 11445.6475\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11445.6164 - mae: 11445.6162 - val_loss: 11445.5978 - val_mae: 11445.5977\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11445.5641 - mae: 11445.5645 - val_loss: 11445.5446 - val_mae: 11445.5449\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.5088 - mae: 11445.5098 - val_loss: 11445.4885 - val_mae: 11445.4883\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.4488 - mae: 11445.4492 - val_loss: 11445.4269 - val_mae: 11445.4268\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.3856 - mae: 11445.3848 - val_loss: 11445.3609 - val_mae: 11445.3604\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11445.3174 - mae: 11445.3174 - val_loss: 11445.2899 - val_mae: 11445.2910\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.2447 - mae: 11445.2441 - val_loss: 11445.2157 - val_mae: 11445.2158\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11445.1669 - mae: 11445.1670 - val_loss: 11445.1328 - val_mae: 11445.1328\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11445.0841 - mae: 11445.0840 - val_loss: 11445.0457 - val_mae: 11445.0459\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11444.9960 - mae: 11444.9961 - val_loss: 11444.9552 - val_mae: 11444.9541\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 11444.9020 - mae: 11444.9014 - val_loss: 11444.8567 - val_mae: 11444.8564\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 11444.8028 - mae: 11444.8037 - val_loss: 11444.7553 - val_mae: 11444.7549\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11444.6977 - mae: 11444.6973 - val_loss: 11444.6468 - val_mae: 11444.6475\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11444.5873 - mae: 11444.5869 - val_loss: 11444.5294 - val_mae: 11444.5293\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11444.4707 - mae: 11444.4717 - val_loss: 11444.4086 - val_mae: 11444.4082\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11444.3485 - mae: 11444.3486 - val_loss: 11444.2810 - val_mae: 11444.2803\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 11444.2198 - mae: 11444.2188 - val_loss: 11444.1474 - val_mae: 11444.1475\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11444.0853 - mae: 11444.0850 - val_loss: 11444.0061 - val_mae: 11444.0059\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11443.9442 - mae: 11443.9443 - val_loss: 11443.8599 - val_mae: 11443.8604\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11443.7974 - mae: 11443.7969 - val_loss: 11443.7083 - val_mae: 11443.7090\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 11443.6440 - mae: 11443.6436 - val_loss: 11443.5471 - val_mae: 11443.5469\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11443.4847 - mae: 11443.4844 - val_loss: 11443.3820 - val_mae: 11443.3828\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 11443.3187 - mae: 11443.3184 - val_loss: 11443.2091 - val_mae: 11443.2080\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 130us/sample - loss: 11443.1461 - mae: 11443.1465 - val_loss: 11443.0295 - val_mae: 11443.0293\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11442.9681 - mae: 11442.9678 - val_loss: 11442.8427 - val_mae: 11442.8428\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11442.7829 - mae: 11442.7832 - val_loss: 11442.6485 - val_mae: 11442.6475\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11442.5918 - mae: 11442.5908 - val_loss: 11442.4529 - val_mae: 11442.4531\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 11442.3936 - mae: 11442.3936 - val_loss: 11442.2511 - val_mae: 11442.2510\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11442.1891 - mae: 11442.1885 - val_loss: 11442.0398 - val_mae: 11442.0400\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11441.9786 - mae: 11441.9795 - val_loss: 11441.8241 - val_mae: 11441.8242\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11441.7610 - mae: 11441.7607 - val_loss: 11441.6004 - val_mae: 11441.6006\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11441.5367 - mae: 11441.5381 - val_loss: 11441.3728 - val_mae: 11441.3730\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 277us/sample - loss: 11441.3067 - mae: 11441.3066 - val_loss: 11441.1430 - val_mae: 11441.1436\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 11441.0697 - mae: 11441.0693 - val_loss: 11440.9047 - val_mae: 11440.9053\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11440.8261 - mae: 11440.8262 - val_loss: 11440.6608 - val_mae: 11440.6602\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11440.5764 - mae: 11440.5771 - val_loss: 11440.4068 - val_mae: 11440.4072\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11440.3195 - mae: 11440.3193 - val_loss: 11440.1495 - val_mae: 11440.1504\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11440.0563 - mae: 11440.0566 - val_loss: 11439.8836 - val_mae: 11439.8848\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11439.7865 - mae: 11439.7871 - val_loss: 11439.6169 - val_mae: 11439.6162\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 11439.5106 - mae: 11439.5107 - val_loss: 11439.3336 - val_mae: 11439.3340\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11439.2272 - mae: 11439.2275 - val_loss: 11439.0413 - val_mae: 11439.0410\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11438.9382 - mae: 11438.9375 - val_loss: 11438.7467 - val_mae: 11438.7471\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11438.6424 - mae: 11438.6426 - val_loss: 11438.4448 - val_mae: 11438.4453\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11438.3389 - mae: 11438.3379 - val_loss: 11438.1385 - val_mae: 11438.1387\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11438.0302 - mae: 11438.0303 - val_loss: 11437.8274 - val_mae: 11437.8271\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11437.7147 - mae: 11437.7139 - val_loss: 11437.5115 - val_mae: 11437.5107\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11437.3918 - mae: 11437.3926 - val_loss: 11437.1878 - val_mae: 11437.1875\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11437.0632 - mae: 11437.0635 - val_loss: 11436.8567 - val_mae: 11436.8564\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11436.7276 - mae: 11436.7275 - val_loss: 11436.5117 - val_mae: 11436.5117\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11436.3860 - mae: 11436.3857 - val_loss: 11436.1657 - val_mae: 11436.1650\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 11436.0367 - mae: 11436.0371 - val_loss: 11435.8134 - val_mae: 11435.8135\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11435.6816 - mae: 11435.6816 - val_loss: 11435.4633 - val_mae: 11435.4639\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 11435.3198 - mae: 11435.3193 - val_loss: 11435.1047 - val_mae: 11435.1045\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11434.9519 - mae: 11434.9521 - val_loss: 11434.7419 - val_mae: 11434.7412\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11434.5769 - mae: 11434.5771 - val_loss: 11434.3764 - val_mae: 11434.3760\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11434.1960 - mae: 11434.1953 - val_loss: 11433.9902 - val_mae: 11433.9902\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11433.8078 - mae: 11433.8086 - val_loss: 11433.5936 - val_mae: 11433.5928\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 131us/sample - loss: 11433.4134 - mae: 11433.4131 - val_loss: 11433.1908 - val_mae: 11433.1904\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11433.0125 - mae: 11433.0127 - val_loss: 11432.7927 - val_mae: 11432.7920\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11432.6054 - mae: 11432.6055 - val_loss: 11432.3885 - val_mae: 11432.3887\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11432.1918 - mae: 11432.1914 - val_loss: 11431.9773 - val_mae: 11431.9766\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 138us/sample - loss: 11431.7710 - mae: 11431.7705 - val_loss: 11431.5620 - val_mae: 11431.5615\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11431.3441 - mae: 11431.3447 - val_loss: 11431.1366 - val_mae: 11431.1367\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 11430.9110 - mae: 11430.9121 - val_loss: 11430.6994 - val_mae: 11430.6992\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11430.4711 - mae: 11430.4707 - val_loss: 11430.2650 - val_mae: 11430.2656\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11430.0251 - mae: 11430.0254 - val_loss: 11429.8280 - val_mae: 11429.8281\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11429.5718 - mae: 11429.5713 - val_loss: 11429.3783 - val_mae: 11429.3789\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 11429.1127 - mae: 11429.1133 - val_loss: 11428.9213 - val_mae: 11428.9209\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11428.6470 - mae: 11428.6475 - val_loss: 11428.4544 - val_mae: 11428.4541\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11428.1755 - mae: 11428.1758 - val_loss: 11427.9859 - val_mae: 11427.9863\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11427.6972 - mae: 11427.6963 - val_loss: 11427.5036 - val_mae: 11427.5039\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 11427.2125 - mae: 11427.2119 - val_loss: 11427.0189 - val_mae: 11427.0186\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 11426.7211 - mae: 11426.7197 - val_loss: 11426.5497 - val_mae: 11426.5508\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11426.2239 - mae: 11426.2236 - val_loss: 11426.0745 - val_mae: 11426.0742\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11425.7194 - mae: 11425.7197 - val_loss: 11425.5780 - val_mae: 11425.5771\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 11425.2092 - mae: 11425.2090 - val_loss: 11425.0718 - val_mae: 11425.0713\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11424.6924 - mae: 11424.6924 - val_loss: 11424.5522 - val_mae: 11424.5527\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11424.1693 - mae: 11424.1689 - val_loss: 11424.0334 - val_mae: 11424.0332\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 163us/sample - loss: 11423.6401 - mae: 11423.6396 - val_loss: 11423.5015 - val_mae: 11423.5020\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 11423.1040 - mae: 11423.1045 - val_loss: 11422.9482 - val_mae: 11422.9482\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11422.5621 - mae: 11422.5625 - val_loss: 11422.3883 - val_mae: 11422.3887\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 11422.0139 - mae: 11422.0137 - val_loss: 11421.8238 - val_mae: 11421.8242\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 11421.4594 - mae: 11421.4600 - val_loss: 11421.2537 - val_mae: 11421.2549\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 11420.8979 - mae: 11420.8975 - val_loss: 11420.6749 - val_mae: 11420.6748\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 11420.3311 - mae: 11420.3301 - val_loss: 11420.0858 - val_mae: 11420.0850\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 11419.7583 - mae: 11419.7598 - val_loss: 11419.4730 - val_mae: 11419.4736\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 11419.1783 - mae: 11419.1787 - val_loss: 11418.8922 - val_mae: 11418.8916\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11418.5921 - mae: 11418.5918 - val_loss: 11418.3139 - val_mae: 11418.3145\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 11418.0001 - mae: 11418.0000 - val_loss: 11417.7243 - val_mae: 11417.7236\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11417.4017 - mae: 11417.4014 - val_loss: 11417.1279 - val_mae: 11417.1270\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11416.7968 - mae: 11416.7969 - val_loss: 11416.4928 - val_mae: 11416.4932\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11416.1860 - mae: 11416.1865 - val_loss: 11415.8534 - val_mae: 11415.8535\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11415.5690 - mae: 11415.5684 - val_loss: 11415.2105 - val_mae: 11415.2109\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11414.9460 - mae: 11414.9453 - val_loss: 11414.5826 - val_mae: 11414.5830\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11414.3158 - mae: 11414.3154 - val_loss: 11413.9688 - val_mae: 11413.9688\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 11413.6808 - mae: 11413.6807 - val_loss: 11413.3366 - val_mae: 11413.3369\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11413.0387 - mae: 11413.0381 - val_loss: 11412.7213 - val_mae: 11412.7217\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 11412.3909 - mae: 11412.3916 - val_loss: 11412.0810 - val_mae: 11412.0811\n",
      "159/159 [==============================] - 0s 52us/sample - loss: 11412.0810 - mae: 11412.0811\n",
      "Val score is 11412.0810546875\n",
      "DataFrame: automobile imput_method :MLP model :KNN score:  0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :MLP model :XGB score:  0.49\n",
      "DataFrame: automobile imput_method :MLP model :MLP score:  0.73\n",
      "DataFrame: bands imput_method :LOCF model :KNN score:  0.58\n",
      "DataFrame: bands imput_method :LOCF model :XGB score:  0.53\n",
      "DataFrame: bands imput_method :LOCF model :MLP score:  0.51\n",
      "DataFrame: bands imput_method :mean_mode model :KNN score:  0.57\n",
      "DataFrame: bands imput_method :mean_mode model :XGB score:  0.55\n",
      "DataFrame: bands imput_method :mean_mode model :MLP score:  0.52\n",
      "DataFrame: bands imput_method :knn model :KNN score:  0.57\n",
      "DataFrame: bands imput_method :knn model :XGB score:  0.58\n",
      "DataFrame: bands imput_method :knn model :MLP score:  0.52\n",
      "[0]\tvalidation_0-mae:31.50691\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:22.23046\n",
      "[2]\tvalidation_0-mae:15.89912\n",
      "[3]\tvalidation_0-mae:11.80451\n",
      "[4]\tvalidation_0-mae:9.14674\n",
      "[5]\tvalidation_0-mae:7.45403\n",
      "[6]\tvalidation_0-mae:6.23692\n",
      "[7]\tvalidation_0-mae:5.41807\n",
      "[8]\tvalidation_0-mae:4.86756\n",
      "[9]\tvalidation_0-mae:4.43761\n",
      "[10]\tvalidation_0-mae:4.11810\n",
      "[11]\tvalidation_0-mae:3.85741\n",
      "[12]\tvalidation_0-mae:3.60898\n",
      "[13]\tvalidation_0-mae:3.41948\n",
      "[14]\tvalidation_0-mae:3.23728\n",
      "[15]\tvalidation_0-mae:3.15168\n",
      "[16]\tvalidation_0-mae:3.00099\n",
      "[17]\tvalidation_0-mae:2.88246\n",
      "[18]\tvalidation_0-mae:2.73230\n",
      "[19]\tvalidation_0-mae:2.64787\n",
      "[20]\tvalidation_0-mae:2.50644\n",
      "[21]\tvalidation_0-mae:2.41926\n",
      "[22]\tvalidation_0-mae:2.29377\n",
      "[23]\tvalidation_0-mae:2.17823\n",
      "[24]\tvalidation_0-mae:2.08834\n",
      "[25]\tvalidation_0-mae:2.00991\n",
      "[26]\tvalidation_0-mae:1.95194\n",
      "[27]\tvalidation_0-mae:1.88152\n",
      "[28]\tvalidation_0-mae:1.80535\n",
      "[29]\tvalidation_0-mae:1.73612\n",
      "[30]\tvalidation_0-mae:1.70586\n",
      "[31]\tvalidation_0-mae:1.65254\n",
      "[32]\tvalidation_0-mae:1.57827\n",
      "[33]\tvalidation_0-mae:1.55823\n",
      "[34]\tvalidation_0-mae:1.52229\n",
      "[35]\tvalidation_0-mae:1.49659\n",
      "[36]\tvalidation_0-mae:1.47374\n",
      "[37]\tvalidation_0-mae:1.43658\n",
      "[38]\tvalidation_0-mae:1.40688\n",
      "[39]\tvalidation_0-mae:1.38344\n",
      "[40]\tvalidation_0-mae:1.36359\n",
      "[41]\tvalidation_0-mae:1.36368\n",
      "[42]\tvalidation_0-mae:1.34842\n",
      "[43]\tvalidation_0-mae:1.31930\n",
      "[44]\tvalidation_0-mae:1.28751\n",
      "[45]\tvalidation_0-mae:1.27825\n",
      "[46]\tvalidation_0-mae:1.25195\n",
      "[47]\tvalidation_0-mae:1.23840\n",
      "[48]\tvalidation_0-mae:1.21309\n",
      "[49]\tvalidation_0-mae:1.19102\n",
      "[50]\tvalidation_0-mae:1.19110\n",
      "[51]\tvalidation_0-mae:1.18193\n",
      "[52]\tvalidation_0-mae:1.16485\n",
      "[53]\tvalidation_0-mae:1.14216\n",
      "[54]\tvalidation_0-mae:1.13623\n",
      "[55]\tvalidation_0-mae:1.12808\n",
      "[56]\tvalidation_0-mae:1.12805\n",
      "[57]\tvalidation_0-mae:1.12226\n",
      "[58]\tvalidation_0-mae:1.11554\n",
      "[59]\tvalidation_0-mae:1.11544\n",
      "[60]\tvalidation_0-mae:1.11597\n",
      "[61]\tvalidation_0-mae:1.11563\n",
      "Stopping. Best iteration:\n",
      "[59]\tvalidation_0-mae:1.11544\n",
      "\n",
      "[0]\tvalidation_0-mae:35.63655\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:25.15060\n",
      "[2]\tvalidation_0-mae:17.76931\n",
      "[3]\tvalidation_0-mae:12.61894\n",
      "[4]\tvalidation_0-mae:9.46654\n",
      "[5]\tvalidation_0-mae:7.81821\n",
      "[6]\tvalidation_0-mae:6.57718\n",
      "[7]\tvalidation_0-mae:5.84639\n",
      "[8]\tvalidation_0-mae:5.30130\n",
      "[9]\tvalidation_0-mae:4.86982\n",
      "[10]\tvalidation_0-mae:4.54762\n",
      "[11]\tvalidation_0-mae:4.21363\n",
      "[12]\tvalidation_0-mae:3.97043\n",
      "[13]\tvalidation_0-mae:3.69325\n",
      "[14]\tvalidation_0-mae:3.53896\n",
      "[15]\tvalidation_0-mae:3.43127\n",
      "[16]\tvalidation_0-mae:3.22429\n",
      "[17]\tvalidation_0-mae:3.14287\n",
      "[18]\tvalidation_0-mae:3.03186\n",
      "[19]\tvalidation_0-mae:2.89755\n",
      "[20]\tvalidation_0-mae:2.84673\n",
      "[21]\tvalidation_0-mae:2.71738\n",
      "[22]\tvalidation_0-mae:2.58186\n",
      "[23]\tvalidation_0-mae:2.51476\n",
      "[24]\tvalidation_0-mae:2.40321\n",
      "[25]\tvalidation_0-mae:2.27218\n",
      "[26]\tvalidation_0-mae:2.19345\n",
      "[27]\tvalidation_0-mae:2.10888\n",
      "[28]\tvalidation_0-mae:2.04927\n",
      "[29]\tvalidation_0-mae:1.94915\n",
      "[30]\tvalidation_0-mae:1.88581\n",
      "[31]\tvalidation_0-mae:1.81681\n",
      "[32]\tvalidation_0-mae:1.75619\n",
      "[33]\tvalidation_0-mae:1.71178\n",
      "[34]\tvalidation_0-mae:1.66909\n",
      "[35]\tvalidation_0-mae:1.62158\n",
      "[36]\tvalidation_0-mae:1.56605\n",
      "[37]\tvalidation_0-mae:1.51537\n",
      "[38]\tvalidation_0-mae:1.47721\n",
      "[39]\tvalidation_0-mae:1.43294\n",
      "[40]\tvalidation_0-mae:1.39776\n",
      "[41]\tvalidation_0-mae:1.37984\n",
      "[42]\tvalidation_0-mae:1.36604\n",
      "[43]\tvalidation_0-mae:1.33807\n",
      "[44]\tvalidation_0-mae:1.29908\n",
      "[45]\tvalidation_0-mae:1.27361\n",
      "[46]\tvalidation_0-mae:1.26163\n",
      "[47]\tvalidation_0-mae:1.26145\n",
      "[48]\tvalidation_0-mae:1.24344\n",
      "[49]\tvalidation_0-mae:1.22737\n",
      "[50]\tvalidation_0-mae:1.20767\n",
      "[51]\tvalidation_0-mae:1.20765\n",
      "[52]\tvalidation_0-mae:1.18248\n",
      "[53]\tvalidation_0-mae:1.17029\n",
      "[54]\tvalidation_0-mae:1.15313\n",
      "[55]\tvalidation_0-mae:1.14624\n",
      "[56]\tvalidation_0-mae:1.14263\n",
      "[57]\tvalidation_0-mae:1.12910\n",
      "[58]\tvalidation_0-mae:1.11818\n",
      "[59]\tvalidation_0-mae:1.09558\n",
      "[60]\tvalidation_0-mae:1.09089\n",
      "[61]\tvalidation_0-mae:1.08141\n",
      "[62]\tvalidation_0-mae:1.07677\n",
      "[63]\tvalidation_0-mae:1.07203\n",
      "[64]\tvalidation_0-mae:1.06624\n",
      "[65]\tvalidation_0-mae:1.06063\n",
      "[66]\tvalidation_0-mae:1.05371\n",
      "[67]\tvalidation_0-mae:1.04953\n",
      "[68]\tvalidation_0-mae:1.04747\n",
      "[69]\tvalidation_0-mae:1.04746\n",
      "[70]\tvalidation_0-mae:1.04573\n",
      "[71]\tvalidation_0-mae:1.04066\n",
      "[72]\tvalidation_0-mae:1.04061\n",
      "[73]\tvalidation_0-mae:1.03839\n",
      "[74]\tvalidation_0-mae:1.03836\n",
      "[75]\tvalidation_0-mae:1.03836\n",
      "[76]\tvalidation_0-mae:1.03428\n",
      "[77]\tvalidation_0-mae:1.03465\n",
      "[78]\tvalidation_0-mae:1.03446\n",
      "Stopping. Best iteration:\n",
      "[76]\tvalidation_0-mae:1.03428\n",
      "\n",
      "[0]\tvalidation_0-mae:0.16030\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.11855\n",
      "[2]\tvalidation_0-mae:0.09247\n",
      "[3]\tvalidation_0-mae:0.07564\n",
      "[4]\tvalidation_0-mae:0.06750\n",
      "[5]\tvalidation_0-mae:0.06149\n",
      "[6]\tvalidation_0-mae:0.05902\n",
      "[7]\tvalidation_0-mae:0.05838\n",
      "[8]\tvalidation_0-mae:0.05792\n",
      "[9]\tvalidation_0-mae:0.05767\n",
      "[10]\tvalidation_0-mae:0.05739\n",
      "[11]\tvalidation_0-mae:0.05722\n",
      "[12]\tvalidation_0-mae:0.05708\n",
      "[13]\tvalidation_0-mae:0.05703\n",
      "[14]\tvalidation_0-mae:0.05699\n",
      "[15]\tvalidation_0-mae:0.05694\n",
      "[16]\tvalidation_0-mae:0.05692\n",
      "[17]\tvalidation_0-mae:0.05690\n",
      "[18]\tvalidation_0-mae:0.05691\n",
      "[19]\tvalidation_0-mae:0.05697\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-mae:0.05690\n",
      "\n",
      "[0]\tvalidation_0-mae:10.49257\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:7.41146\n",
      "[2]\tvalidation_0-mae:5.24306\n",
      "[3]\tvalidation_0-mae:3.70759\n",
      "[4]\tvalidation_0-mae:2.63632\n",
      "[5]\tvalidation_0-mae:1.90864\n",
      "[6]\tvalidation_0-mae:1.43925\n",
      "[7]\tvalidation_0-mae:1.19450\n",
      "[8]\tvalidation_0-mae:1.04392\n",
      "[9]\tvalidation_0-mae:0.94626\n",
      "[10]\tvalidation_0-mae:0.91390\n",
      "[11]\tvalidation_0-mae:0.89862\n",
      "[12]\tvalidation_0-mae:0.85467\n",
      "[13]\tvalidation_0-mae:0.84421\n",
      "[14]\tvalidation_0-mae:0.82985\n",
      "[15]\tvalidation_0-mae:0.82934\n",
      "[16]\tvalidation_0-mae:0.82916\n",
      "[17]\tvalidation_0-mae:0.82915\n",
      "[18]\tvalidation_0-mae:0.82273\n",
      "[19]\tvalidation_0-mae:0.82287\n",
      "[20]\tvalidation_0-mae:0.82299\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-mae:0.82273\n",
      "\n",
      "[0]\tvalidation_0-mae:55.11354\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:38.88678\n",
      "[2]\tvalidation_0-mae:27.43528\n",
      "[3]\tvalidation_0-mae:19.44142\n",
      "[4]\tvalidation_0-mae:13.80732\n",
      "[5]\tvalidation_0-mae:10.17705\n",
      "[6]\tvalidation_0-mae:7.94920\n",
      "[7]\tvalidation_0-mae:6.62990\n",
      "[8]\tvalidation_0-mae:5.66486\n",
      "[9]\tvalidation_0-mae:5.01560\n",
      "[10]\tvalidation_0-mae:4.64515\n",
      "[11]\tvalidation_0-mae:4.32963\n",
      "[12]\tvalidation_0-mae:4.09058\n",
      "[13]\tvalidation_0-mae:3.85615\n",
      "[14]\tvalidation_0-mae:3.70972\n",
      "[15]\tvalidation_0-mae:3.58570\n",
      "[16]\tvalidation_0-mae:3.42916\n",
      "[17]\tvalidation_0-mae:3.26637\n",
      "[18]\tvalidation_0-mae:3.14917\n",
      "[19]\tvalidation_0-mae:3.07679\n",
      "[20]\tvalidation_0-mae:2.93328\n",
      "[21]\tvalidation_0-mae:2.80802\n",
      "[22]\tvalidation_0-mae:2.66674\n",
      "[23]\tvalidation_0-mae:2.59836\n",
      "[24]\tvalidation_0-mae:2.44341\n",
      "[25]\tvalidation_0-mae:2.33996\n",
      "[26]\tvalidation_0-mae:2.28450\n",
      "[27]\tvalidation_0-mae:2.25199\n",
      "[28]\tvalidation_0-mae:2.16306\n",
      "[29]\tvalidation_0-mae:2.10428\n",
      "[30]\tvalidation_0-mae:2.02826\n",
      "[31]\tvalidation_0-mae:1.98407\n",
      "[32]\tvalidation_0-mae:1.92276\n",
      "[33]\tvalidation_0-mae:1.89985\n",
      "[34]\tvalidation_0-mae:1.85455\n",
      "[35]\tvalidation_0-mae:1.81013\n",
      "[36]\tvalidation_0-mae:1.77903\n",
      "[37]\tvalidation_0-mae:1.73774\n",
      "[38]\tvalidation_0-mae:1.68013\n",
      "[39]\tvalidation_0-mae:1.63242\n",
      "[40]\tvalidation_0-mae:1.59532\n",
      "[41]\tvalidation_0-mae:1.58301\n",
      "[42]\tvalidation_0-mae:1.56440\n",
      "[43]\tvalidation_0-mae:1.55220\n",
      "[44]\tvalidation_0-mae:1.50684\n",
      "[45]\tvalidation_0-mae:1.47432\n",
      "[46]\tvalidation_0-mae:1.45053\n",
      "[47]\tvalidation_0-mae:1.42851\n",
      "[48]\tvalidation_0-mae:1.39628\n",
      "[49]\tvalidation_0-mae:1.38960\n",
      "[50]\tvalidation_0-mae:1.37558\n",
      "[51]\tvalidation_0-mae:1.36481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52]\tvalidation_0-mae:1.35716\n",
      "[53]\tvalidation_0-mae:1.33163\n",
      "[54]\tvalidation_0-mae:1.30167\n",
      "[55]\tvalidation_0-mae:1.29095\n",
      "[56]\tvalidation_0-mae:1.27911\n",
      "[57]\tvalidation_0-mae:1.26255\n",
      "[58]\tvalidation_0-mae:1.26254\n",
      "[59]\tvalidation_0-mae:1.25609\n",
      "[60]\tvalidation_0-mae:1.23512\n",
      "[61]\tvalidation_0-mae:1.23521\n",
      "[62]\tvalidation_0-mae:1.22878\n",
      "[63]\tvalidation_0-mae:1.22151\n",
      "[64]\tvalidation_0-mae:1.22136\n",
      "[65]\tvalidation_0-mae:1.20681\n",
      "[66]\tvalidation_0-mae:1.20115\n",
      "[67]\tvalidation_0-mae:1.20118\n",
      "[68]\tvalidation_0-mae:1.20115\n",
      "[69]\tvalidation_0-mae:1.19670\n",
      "[70]\tvalidation_0-mae:1.18127\n",
      "[71]\tvalidation_0-mae:1.17596\n",
      "[72]\tvalidation_0-mae:1.17596\n",
      "[73]\tvalidation_0-mae:1.17596\n",
      "Stopping. Best iteration:\n",
      "[71]\tvalidation_0-mae:1.17596\n",
      "\n",
      "[0]\tvalidation_0-mae:0.20806\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.17753\n",
      "[2]\tvalidation_0-mae:0.16366\n",
      "[3]\tvalidation_0-mae:0.15756\n",
      "[4]\tvalidation_0-mae:0.15344\n",
      "[5]\tvalidation_0-mae:0.15025\n",
      "[6]\tvalidation_0-mae:0.14808\n",
      "[7]\tvalidation_0-mae:0.14646\n",
      "[8]\tvalidation_0-mae:0.14541\n",
      "[9]\tvalidation_0-mae:0.14421\n",
      "[10]\tvalidation_0-mae:0.14399\n",
      "[11]\tvalidation_0-mae:0.14308\n",
      "[12]\tvalidation_0-mae:0.14268\n",
      "[13]\tvalidation_0-mae:0.14264\n",
      "[14]\tvalidation_0-mae:0.14199\n",
      "[15]\tvalidation_0-mae:0.14195\n",
      "[16]\tvalidation_0-mae:0.14209\n",
      "[17]\tvalidation_0-mae:0.14190\n",
      "[18]\tvalidation_0-mae:0.14185\n",
      "[19]\tvalidation_0-mae:0.14183\n",
      "[20]\tvalidation_0-mae:0.14198\n",
      "[21]\tvalidation_0-mae:0.14191\n",
      "Stopping. Best iteration:\n",
      "[19]\tvalidation_0-mae:0.14183\n",
      "\n",
      "[0]\tvalidation_0-mae:21.49785\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:15.22496\n",
      "[2]\tvalidation_0-mae:11.11046\n",
      "[3]\tvalidation_0-mae:8.64146\n",
      "[4]\tvalidation_0-mae:7.00155\n",
      "[5]\tvalidation_0-mae:5.91285\n",
      "[6]\tvalidation_0-mae:5.07842\n",
      "[7]\tvalidation_0-mae:4.48422\n",
      "[8]\tvalidation_0-mae:4.01005\n",
      "[9]\tvalidation_0-mae:3.65603\n",
      "[10]\tvalidation_0-mae:3.41054\n",
      "[11]\tvalidation_0-mae:3.17918\n",
      "[12]\tvalidation_0-mae:3.00649\n",
      "[13]\tvalidation_0-mae:2.88735\n",
      "[14]\tvalidation_0-mae:2.70919\n",
      "[15]\tvalidation_0-mae:2.61611\n",
      "[16]\tvalidation_0-mae:2.53430\n",
      "[17]\tvalidation_0-mae:2.45290\n",
      "[18]\tvalidation_0-mae:2.36857\n",
      "[19]\tvalidation_0-mae:2.29782\n",
      "[20]\tvalidation_0-mae:2.19097\n",
      "[21]\tvalidation_0-mae:2.14272\n",
      "[22]\tvalidation_0-mae:2.03645\n",
      "[23]\tvalidation_0-mae:1.96901\n",
      "[24]\tvalidation_0-mae:1.91293\n",
      "[25]\tvalidation_0-mae:1.85069\n",
      "[26]\tvalidation_0-mae:1.79277\n",
      "[27]\tvalidation_0-mae:1.75685\n",
      "[28]\tvalidation_0-mae:1.69852\n",
      "[29]\tvalidation_0-mae:1.66106\n",
      "[30]\tvalidation_0-mae:1.61917\n",
      "[31]\tvalidation_0-mae:1.55509\n",
      "[32]\tvalidation_0-mae:1.50310\n",
      "[33]\tvalidation_0-mae:1.46951\n",
      "[34]\tvalidation_0-mae:1.44820\n",
      "[35]\tvalidation_0-mae:1.41816\n",
      "[36]\tvalidation_0-mae:1.38712\n",
      "[37]\tvalidation_0-mae:1.35344\n",
      "[38]\tvalidation_0-mae:1.34155\n",
      "[39]\tvalidation_0-mae:1.32172\n",
      "[40]\tvalidation_0-mae:1.30324\n",
      "[41]\tvalidation_0-mae:1.28695\n",
      "[42]\tvalidation_0-mae:1.26653\n",
      "[43]\tvalidation_0-mae:1.24904\n",
      "[44]\tvalidation_0-mae:1.22534\n",
      "[45]\tvalidation_0-mae:1.18963\n",
      "[46]\tvalidation_0-mae:1.17325\n",
      "[47]\tvalidation_0-mae:1.15825\n",
      "[48]\tvalidation_0-mae:1.15905\n",
      "[49]\tvalidation_0-mae:1.15894\n",
      "Stopping. Best iteration:\n",
      "[47]\tvalidation_0-mae:1.15825\n",
      "\n",
      "[0]\tvalidation_0-mae:4.26580\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:3.22452\n",
      "[2]\tvalidation_0-mae:2.47182\n",
      "[3]\tvalidation_0-mae:1.88096\n",
      "[4]\tvalidation_0-mae:1.45453\n",
      "[5]\tvalidation_0-mae:1.17007\n",
      "[6]\tvalidation_0-mae:0.97362\n",
      "[7]\tvalidation_0-mae:0.80882\n",
      "[8]\tvalidation_0-mae:0.72626\n",
      "[9]\tvalidation_0-mae:0.66100\n",
      "[10]\tvalidation_0-mae:0.60805\n",
      "[11]\tvalidation_0-mae:0.58000\n",
      "[12]\tvalidation_0-mae:0.55316\n",
      "[13]\tvalidation_0-mae:0.54090\n",
      "[14]\tvalidation_0-mae:0.54004\n",
      "[15]\tvalidation_0-mae:0.51940\n",
      "[16]\tvalidation_0-mae:0.52042\n",
      "[17]\tvalidation_0-mae:0.52268\n",
      "Stopping. Best iteration:\n",
      "[15]\tvalidation_0-mae:0.51940\n",
      "\n",
      "[0]\tvalidation_0-mae:1287.32092\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:910.55920\n",
      "[2]\tvalidation_0-mae:657.00287\n",
      "[3]\tvalidation_0-mae:478.57498\n",
      "[4]\tvalidation_0-mae:356.46185\n",
      "[5]\tvalidation_0-mae:278.49615\n",
      "[6]\tvalidation_0-mae:224.03194\n",
      "[7]\tvalidation_0-mae:189.75452\n",
      "[8]\tvalidation_0-mae:164.78032\n",
      "[9]\tvalidation_0-mae:147.91284\n",
      "[10]\tvalidation_0-mae:134.69351\n",
      "[11]\tvalidation_0-mae:124.05472\n",
      "[12]\tvalidation_0-mae:115.32709\n",
      "[13]\tvalidation_0-mae:110.50000\n",
      "[14]\tvalidation_0-mae:102.50576\n",
      "[15]\tvalidation_0-mae:97.57936\n",
      "[16]\tvalidation_0-mae:92.88197\n",
      "[17]\tvalidation_0-mae:89.10771\n",
      "[18]\tvalidation_0-mae:84.78884\n",
      "[19]\tvalidation_0-mae:80.03093\n",
      "[20]\tvalidation_0-mae:77.26412\n",
      "[21]\tvalidation_0-mae:74.44390\n",
      "[22]\tvalidation_0-mae:70.15992\n",
      "[23]\tvalidation_0-mae:67.75149\n",
      "[24]\tvalidation_0-mae:63.79799\n",
      "[25]\tvalidation_0-mae:61.03571\n",
      "[26]\tvalidation_0-mae:57.51296\n",
      "[27]\tvalidation_0-mae:54.88002\n",
      "[28]\tvalidation_0-mae:52.22107\n",
      "[29]\tvalidation_0-mae:51.11577\n",
      "[30]\tvalidation_0-mae:49.87442\n",
      "[31]\tvalidation_0-mae:48.26140\n",
      "[32]\tvalidation_0-mae:46.86606\n",
      "[33]\tvalidation_0-mae:45.23032\n",
      "[34]\tvalidation_0-mae:43.36262\n",
      "[35]\tvalidation_0-mae:41.03056\n",
      "[36]\tvalidation_0-mae:39.50730\n",
      "[37]\tvalidation_0-mae:37.93160\n",
      "[38]\tvalidation_0-mae:36.72982\n",
      "[39]\tvalidation_0-mae:35.85543\n",
      "[40]\tvalidation_0-mae:33.69165\n",
      "[41]\tvalidation_0-mae:32.67542\n",
      "[42]\tvalidation_0-mae:30.81995\n",
      "[43]\tvalidation_0-mae:29.91014\n",
      "[44]\tvalidation_0-mae:28.00346\n",
      "[45]\tvalidation_0-mae:27.26609\n",
      "[46]\tvalidation_0-mae:26.31873\n",
      "[47]\tvalidation_0-mae:25.27889\n",
      "[48]\tvalidation_0-mae:24.47055\n",
      "[49]\tvalidation_0-mae:24.00890\n",
      "[50]\tvalidation_0-mae:23.17510\n",
      "[51]\tvalidation_0-mae:22.54318\n",
      "[52]\tvalidation_0-mae:22.24363\n",
      "[53]\tvalidation_0-mae:21.88928\n",
      "[54]\tvalidation_0-mae:21.54585\n",
      "[55]\tvalidation_0-mae:20.47129\n",
      "[56]\tvalidation_0-mae:20.05989\n",
      "[57]\tvalidation_0-mae:19.43669\n",
      "[58]\tvalidation_0-mae:18.87200\n",
      "[59]\tvalidation_0-mae:18.37255\n",
      "[60]\tvalidation_0-mae:17.76626\n",
      "[61]\tvalidation_0-mae:17.41489\n",
      "[62]\tvalidation_0-mae:17.03093\n",
      "[63]\tvalidation_0-mae:16.58940\n",
      "[64]\tvalidation_0-mae:16.22030\n",
      "[65]\tvalidation_0-mae:15.37096\n",
      "[66]\tvalidation_0-mae:14.87181\n",
      "[67]\tvalidation_0-mae:14.44443\n",
      "[68]\tvalidation_0-mae:13.98434\n",
      "[69]\tvalidation_0-mae:13.56413\n",
      "[70]\tvalidation_0-mae:13.21457\n",
      "[71]\tvalidation_0-mae:12.83328\n",
      "[72]\tvalidation_0-mae:12.51817\n",
      "[73]\tvalidation_0-mae:12.34930\n",
      "[74]\tvalidation_0-mae:11.97380\n",
      "[75]\tvalidation_0-mae:11.72638\n",
      "[76]\tvalidation_0-mae:11.51023\n",
      "[77]\tvalidation_0-mae:11.14067\n",
      "[78]\tvalidation_0-mae:10.77015\n",
      "[79]\tvalidation_0-mae:10.45785\n",
      "[80]\tvalidation_0-mae:10.05975\n",
      "[81]\tvalidation_0-mae:9.86063\n",
      "[82]\tvalidation_0-mae:9.64089\n",
      "[83]\tvalidation_0-mae:9.44774\n",
      "[84]\tvalidation_0-mae:9.21048\n",
      "[85]\tvalidation_0-mae:9.04040\n",
      "[86]\tvalidation_0-mae:8.91688\n",
      "[87]\tvalidation_0-mae:8.82584\n",
      "[88]\tvalidation_0-mae:8.57760\n",
      "[89]\tvalidation_0-mae:8.37819\n",
      "[90]\tvalidation_0-mae:8.08417\n",
      "[91]\tvalidation_0-mae:7.86934\n",
      "[92]\tvalidation_0-mae:7.60265\n",
      "[93]\tvalidation_0-mae:7.54357\n",
      "[94]\tvalidation_0-mae:7.32650\n",
      "[95]\tvalidation_0-mae:7.11593\n",
      "[96]\tvalidation_0-mae:6.99830\n",
      "[97]\tvalidation_0-mae:6.82918\n",
      "[98]\tvalidation_0-mae:6.67030\n",
      "[99]\tvalidation_0-mae:6.53604\n",
      "[100]\tvalidation_0-mae:6.38858\n",
      "[101]\tvalidation_0-mae:6.24540\n",
      "[102]\tvalidation_0-mae:6.05723\n",
      "[103]\tvalidation_0-mae:5.93096\n",
      "[104]\tvalidation_0-mae:5.79290\n",
      "[105]\tvalidation_0-mae:5.67588\n",
      "[106]\tvalidation_0-mae:5.54436\n",
      "[107]\tvalidation_0-mae:5.37560\n",
      "[108]\tvalidation_0-mae:5.28613\n",
      "[109]\tvalidation_0-mae:5.15673\n",
      "[110]\tvalidation_0-mae:5.04399\n",
      "[111]\tvalidation_0-mae:4.99217\n",
      "[112]\tvalidation_0-mae:4.91521\n",
      "[113]\tvalidation_0-mae:4.80558\n",
      "[114]\tvalidation_0-mae:4.73139\n",
      "[115]\tvalidation_0-mae:4.64276\n",
      "[116]\tvalidation_0-mae:4.58104\n",
      "[117]\tvalidation_0-mae:4.50625\n",
      "[118]\tvalidation_0-mae:4.38558\n",
      "[119]\tvalidation_0-mae:4.26945\n",
      "[120]\tvalidation_0-mae:4.18382\n",
      "[121]\tvalidation_0-mae:4.13402\n",
      "[122]\tvalidation_0-mae:4.03122\n",
      "[123]\tvalidation_0-mae:3.89193\n",
      "[124]\tvalidation_0-mae:3.78221\n",
      "[125]\tvalidation_0-mae:3.68896\n",
      "[126]\tvalidation_0-mae:3.58504\n",
      "[127]\tvalidation_0-mae:3.50349\n",
      "[128]\tvalidation_0-mae:3.38009\n",
      "[129]\tvalidation_0-mae:3.29867\n",
      "[130]\tvalidation_0-mae:3.24208\n",
      "[131]\tvalidation_0-mae:3.14395\n",
      "[132]\tvalidation_0-mae:3.06664\n",
      "[133]\tvalidation_0-mae:2.99594\n",
      "[134]\tvalidation_0-mae:2.97103\n",
      "[135]\tvalidation_0-mae:2.92971\n",
      "[136]\tvalidation_0-mae:2.88715\n",
      "[137]\tvalidation_0-mae:2.84587\n",
      "[138]\tvalidation_0-mae:2.79599\n",
      "[139]\tvalidation_0-mae:2.74607\n",
      "[140]\tvalidation_0-mae:2.68834\n",
      "[141]\tvalidation_0-mae:2.62296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142]\tvalidation_0-mae:2.57165\n",
      "[143]\tvalidation_0-mae:2.51906\n",
      "[144]\tvalidation_0-mae:2.47987\n",
      "[145]\tvalidation_0-mae:2.42111\n",
      "[146]\tvalidation_0-mae:2.34404\n",
      "[147]\tvalidation_0-mae:2.29670\n",
      "[148]\tvalidation_0-mae:2.28016\n",
      "[149]\tvalidation_0-mae:2.24796\n",
      "[0]\tvalidation_0-mae:38.97459\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:27.55362\n",
      "[2]\tvalidation_0-mae:19.58620\n",
      "[3]\tvalidation_0-mae:13.94943\n",
      "[4]\tvalidation_0-mae:9.99901\n",
      "[5]\tvalidation_0-mae:7.18935\n",
      "[6]\tvalidation_0-mae:5.22799\n",
      "[7]\tvalidation_0-mae:3.83829\n",
      "[8]\tvalidation_0-mae:2.84407\n",
      "[9]\tvalidation_0-mae:2.13704\n",
      "[10]\tvalidation_0-mae:1.65585\n",
      "[11]\tvalidation_0-mae:1.29336\n",
      "[12]\tvalidation_0-mae:1.04364\n",
      "[13]\tvalidation_0-mae:0.86862\n",
      "[14]\tvalidation_0-mae:0.74101\n",
      "[15]\tvalidation_0-mae:0.67564\n",
      "[16]\tvalidation_0-mae:0.63425\n",
      "[17]\tvalidation_0-mae:0.58019\n",
      "[18]\tvalidation_0-mae:0.52449\n",
      "[19]\tvalidation_0-mae:0.51414\n",
      "[20]\tvalidation_0-mae:0.50506\n",
      "[21]\tvalidation_0-mae:0.49666\n",
      "[22]\tvalidation_0-mae:0.48756\n",
      "[23]\tvalidation_0-mae:0.48250\n",
      "[24]\tvalidation_0-mae:0.46723\n",
      "[25]\tvalidation_0-mae:0.45447\n",
      "[26]\tvalidation_0-mae:0.44507\n",
      "[27]\tvalidation_0-mae:0.44119\n",
      "[28]\tvalidation_0-mae:0.44035\n",
      "[29]\tvalidation_0-mae:0.43333\n",
      "[30]\tvalidation_0-mae:0.43295\n",
      "[31]\tvalidation_0-mae:0.43281\n",
      "[32]\tvalidation_0-mae:0.43297\n",
      "[33]\tvalidation_0-mae:0.42470\n",
      "[34]\tvalidation_0-mae:0.42454\n",
      "[35]\tvalidation_0-mae:0.42433\n",
      "[36]\tvalidation_0-mae:0.42387\n",
      "[37]\tvalidation_0-mae:0.42392\n",
      "[38]\tvalidation_0-mae:0.42389\n",
      "Stopping. Best iteration:\n",
      "[36]\tvalidation_0-mae:0.42387\n",
      "\n",
      "[0]\tvalidation_0-mae:26.90248\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:19.04974\n",
      "[2]\tvalidation_0-mae:13.46454\n",
      "[3]\tvalidation_0-mae:9.54387\n",
      "[4]\tvalidation_0-mae:6.81022\n",
      "[5]\tvalidation_0-mae:4.92790\n",
      "[6]\tvalidation_0-mae:3.62154\n",
      "[7]\tvalidation_0-mae:2.68931\n",
      "[8]\tvalidation_0-mae:2.04912\n",
      "[9]\tvalidation_0-mae:1.63284\n",
      "[10]\tvalidation_0-mae:1.34929\n",
      "[11]\tvalidation_0-mae:1.12603\n",
      "[12]\tvalidation_0-mae:0.96817\n",
      "[13]\tvalidation_0-mae:0.85157\n",
      "[14]\tvalidation_0-mae:0.76594\n",
      "[15]\tvalidation_0-mae:0.71411\n",
      "[16]\tvalidation_0-mae:0.67608\n",
      "[17]\tvalidation_0-mae:0.62813\n",
      "[18]\tvalidation_0-mae:0.60643\n",
      "[19]\tvalidation_0-mae:0.56910\n",
      "[20]\tvalidation_0-mae:0.56176\n",
      "[21]\tvalidation_0-mae:0.55946\n",
      "[22]\tvalidation_0-mae:0.55112\n",
      "[23]\tvalidation_0-mae:0.54964\n",
      "[24]\tvalidation_0-mae:0.54227\n",
      "[25]\tvalidation_0-mae:0.54197\n",
      "[26]\tvalidation_0-mae:0.53978\n",
      "[27]\tvalidation_0-mae:0.53944\n",
      "[28]\tvalidation_0-mae:0.52135\n",
      "[29]\tvalidation_0-mae:0.52013\n",
      "[30]\tvalidation_0-mae:0.51957\n",
      "[31]\tvalidation_0-mae:0.51897\n",
      "[32]\tvalidation_0-mae:0.51802\n",
      "[33]\tvalidation_0-mae:0.51803\n",
      "[34]\tvalidation_0-mae:0.51802\n",
      "Stopping. Best iteration:\n",
      "[32]\tvalidation_0-mae:0.51802\n",
      "\n",
      "[0]\tvalidation_0-mae:1.43937\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.39323\n",
      "[2]\tvalidation_0-mae:1.34062\n",
      "[3]\tvalidation_0-mae:1.28939\n",
      "[4]\tvalidation_0-mae:1.27195\n",
      "[5]\tvalidation_0-mae:1.22637\n",
      "[6]\tvalidation_0-mae:1.19773\n",
      "[7]\tvalidation_0-mae:1.16055\n",
      "[8]\tvalidation_0-mae:1.14718\n",
      "[9]\tvalidation_0-mae:1.11898\n",
      "[10]\tvalidation_0-mae:1.09148\n",
      "[11]\tvalidation_0-mae:1.08864\n",
      "[12]\tvalidation_0-mae:1.06925\n",
      "[13]\tvalidation_0-mae:1.05030\n",
      "[14]\tvalidation_0-mae:1.04003\n",
      "[15]\tvalidation_0-mae:1.04618\n",
      "[16]\tvalidation_0-mae:1.03705\n",
      "[17]\tvalidation_0-mae:1.01261\n",
      "[18]\tvalidation_0-mae:1.00144\n",
      "[19]\tvalidation_0-mae:1.00175\n",
      "[20]\tvalidation_0-mae:0.98869\n",
      "[21]\tvalidation_0-mae:0.98105\n",
      "[22]\tvalidation_0-mae:0.96298\n",
      "[23]\tvalidation_0-mae:0.95127\n",
      "[24]\tvalidation_0-mae:0.95492\n",
      "[25]\tvalidation_0-mae:0.93667\n",
      "[26]\tvalidation_0-mae:0.93745\n",
      "[27]\tvalidation_0-mae:0.92629\n",
      "[28]\tvalidation_0-mae:0.92613\n",
      "[29]\tvalidation_0-mae:0.92165\n",
      "[30]\tvalidation_0-mae:0.92179\n",
      "[31]\tvalidation_0-mae:0.92079\n",
      "[32]\tvalidation_0-mae:0.91807\n",
      "[33]\tvalidation_0-mae:0.92096\n",
      "[34]\tvalidation_0-mae:0.92041\n",
      "Stopping. Best iteration:\n",
      "[32]\tvalidation_0-mae:0.91807\n",
      "\n",
      "[0]\tvalidation_0-mae:0.39236\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.29952\n",
      "[2]\tvalidation_0-mae:0.23441\n",
      "[3]\tvalidation_0-mae:0.18771\n",
      "[4]\tvalidation_0-mae:0.15508\n",
      "[5]\tvalidation_0-mae:0.12956\n",
      "[6]\tvalidation_0-mae:0.10637\n",
      "[7]\tvalidation_0-mae:0.09470\n",
      "[8]\tvalidation_0-mae:0.08871\n",
      "[9]\tvalidation_0-mae:0.08476\n",
      "[10]\tvalidation_0-mae:0.07947\n",
      "[11]\tvalidation_0-mae:0.07898\n",
      "[12]\tvalidation_0-mae:0.07765\n",
      "[13]\tvalidation_0-mae:0.07648\n",
      "[14]\tvalidation_0-mae:0.07632\n",
      "[15]\tvalidation_0-mae:0.07586\n",
      "[16]\tvalidation_0-mae:0.07545\n",
      "[17]\tvalidation_0-mae:0.07505\n",
      "[18]\tvalidation_0-mae:0.07532\n",
      "[19]\tvalidation_0-mae:0.07256\n",
      "[20]\tvalidation_0-mae:0.07351\n",
      "[21]\tvalidation_0-mae:0.07451\n",
      "Stopping. Best iteration:\n",
      "[19]\tvalidation_0-mae:0.07256\n",
      "\n",
      "[0]\tvalidation_0-mae:1.36505\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.99377\n",
      "[2]\tvalidation_0-mae:0.75934\n",
      "[3]\tvalidation_0-mae:0.60879\n",
      "[4]\tvalidation_0-mae:0.51611\n",
      "[5]\tvalidation_0-mae:0.45876\n",
      "[6]\tvalidation_0-mae:0.41809\n",
      "[7]\tvalidation_0-mae:0.38993\n",
      "[8]\tvalidation_0-mae:0.37274\n",
      "[9]\tvalidation_0-mae:0.36056\n",
      "[10]\tvalidation_0-mae:0.35259\n",
      "[11]\tvalidation_0-mae:0.34475\n",
      "[12]\tvalidation_0-mae:0.34048\n",
      "[13]\tvalidation_0-mae:0.33779\n",
      "[14]\tvalidation_0-mae:0.33700\n",
      "[15]\tvalidation_0-mae:0.33492\n",
      "[16]\tvalidation_0-mae:0.33301\n",
      "[17]\tvalidation_0-mae:0.32811\n",
      "[18]\tvalidation_0-mae:0.32933\n",
      "[19]\tvalidation_0-mae:0.32950\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-mae:0.32811\n",
      "\n",
      "[0]\tvalidation_0-mae:0.39407\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.32779\n",
      "[2]\tvalidation_0-mae:0.28783\n",
      "[3]\tvalidation_0-mae:0.26649\n",
      "[4]\tvalidation_0-mae:0.25185\n",
      "[5]\tvalidation_0-mae:0.24627\n",
      "[6]\tvalidation_0-mae:0.24247\n",
      "[7]\tvalidation_0-mae:0.23935\n",
      "[8]\tvalidation_0-mae:0.23779\n",
      "[9]\tvalidation_0-mae:0.23693\n",
      "[10]\tvalidation_0-mae:0.23528\n",
      "[11]\tvalidation_0-mae:0.23435\n",
      "[12]\tvalidation_0-mae:0.23429\n",
      "[13]\tvalidation_0-mae:0.23395\n",
      "[14]\tvalidation_0-mae:0.23348\n",
      "[15]\tvalidation_0-mae:0.23386\n",
      "[16]\tvalidation_0-mae:0.23364\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-mae:0.23348\n",
      "\n",
      "[0]\tvalidation_0-mae:24.21638\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:17.11817\n",
      "[2]\tvalidation_0-mae:12.13746\n",
      "[3]\tvalidation_0-mae:8.63354\n",
      "[4]\tvalidation_0-mae:6.26535\n",
      "[5]\tvalidation_0-mae:4.68522\n",
      "[6]\tvalidation_0-mae:3.68154\n",
      "[7]\tvalidation_0-mae:3.07264\n",
      "[8]\tvalidation_0-mae:2.66775\n",
      "[9]\tvalidation_0-mae:2.41795\n",
      "[10]\tvalidation_0-mae:2.20564\n",
      "[11]\tvalidation_0-mae:2.04275\n",
      "[12]\tvalidation_0-mae:1.91708\n",
      "[13]\tvalidation_0-mae:1.79601\n",
      "[14]\tvalidation_0-mae:1.71338\n",
      "[15]\tvalidation_0-mae:1.62504\n",
      "[16]\tvalidation_0-mae:1.55378\n",
      "[17]\tvalidation_0-mae:1.46354\n",
      "[18]\tvalidation_0-mae:1.41348\n",
      "[19]\tvalidation_0-mae:1.40061\n",
      "[20]\tvalidation_0-mae:1.35537\n",
      "[21]\tvalidation_0-mae:1.31485\n",
      "[22]\tvalidation_0-mae:1.29123\n",
      "[23]\tvalidation_0-mae:1.26192\n",
      "[24]\tvalidation_0-mae:1.23029\n",
      "[25]\tvalidation_0-mae:1.20050\n",
      "[26]\tvalidation_0-mae:1.19169\n",
      "[27]\tvalidation_0-mae:1.18309\n",
      "[28]\tvalidation_0-mae:1.16463\n",
      "[29]\tvalidation_0-mae:1.15012\n",
      "[30]\tvalidation_0-mae:1.14309\n",
      "[31]\tvalidation_0-mae:1.12239\n",
      "[32]\tvalidation_0-mae:1.10764\n",
      "[33]\tvalidation_0-mae:1.08937\n",
      "[34]\tvalidation_0-mae:1.08094\n",
      "[35]\tvalidation_0-mae:1.08082\n",
      "[36]\tvalidation_0-mae:1.08078\n",
      "[37]\tvalidation_0-mae:1.06949\n",
      "[38]\tvalidation_0-mae:1.05060\n",
      "[39]\tvalidation_0-mae:1.05068\n",
      "[40]\tvalidation_0-mae:1.04647\n",
      "[41]\tvalidation_0-mae:1.02763\n",
      "[42]\tvalidation_0-mae:1.01939\n",
      "[43]\tvalidation_0-mae:1.01928\n",
      "[44]\tvalidation_0-mae:1.01930\n",
      "[45]\tvalidation_0-mae:1.01231\n",
      "[46]\tvalidation_0-mae:1.01244\n",
      "[47]\tvalidation_0-mae:1.00267\n",
      "[48]\tvalidation_0-mae:1.00268\n",
      "[49]\tvalidation_0-mae:1.00250\n",
      "[50]\tvalidation_0-mae:0.98654\n",
      "[51]\tvalidation_0-mae:0.98652\n",
      "[52]\tvalidation_0-mae:0.98657\n",
      "[53]\tvalidation_0-mae:0.98667\n",
      "Stopping. Best iteration:\n",
      "[51]\tvalidation_0-mae:0.98652\n",
      "\n",
      "[0]\tvalidation_0-mae:27.22628\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:19.23096\n",
      "[2]\tvalidation_0-mae:13.58654\n",
      "[3]\tvalidation_0-mae:9.57942\n",
      "[4]\tvalidation_0-mae:6.82008\n",
      "[5]\tvalidation_0-mae:4.96784\n",
      "[6]\tvalidation_0-mae:3.81802\n",
      "[7]\tvalidation_0-mae:3.03736\n",
      "[8]\tvalidation_0-mae:2.46010\n",
      "[9]\tvalidation_0-mae:2.00481\n",
      "[10]\tvalidation_0-mae:1.65104\n",
      "[11]\tvalidation_0-mae:1.39533\n",
      "[12]\tvalidation_0-mae:1.19792\n",
      "[13]\tvalidation_0-mae:1.03487\n",
      "[14]\tvalidation_0-mae:0.94138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\tvalidation_0-mae:0.86401\n",
      "[16]\tvalidation_0-mae:0.79721\n",
      "[17]\tvalidation_0-mae:0.78331\n",
      "[18]\tvalidation_0-mae:0.75056\n",
      "[19]\tvalidation_0-mae:0.72052\n",
      "[20]\tvalidation_0-mae:0.71006\n",
      "[21]\tvalidation_0-mae:0.70925\n",
      "[22]\tvalidation_0-mae:0.70781\n",
      "[23]\tvalidation_0-mae:0.70307\n",
      "[24]\tvalidation_0-mae:0.69722\n",
      "[25]\tvalidation_0-mae:0.69714\n",
      "[26]\tvalidation_0-mae:0.69323\n",
      "[27]\tvalidation_0-mae:0.69321\n",
      "[28]\tvalidation_0-mae:0.69309\n",
      "[29]\tvalidation_0-mae:0.69312\n",
      "[30]\tvalidation_0-mae:0.69086\n",
      "[31]\tvalidation_0-mae:0.69104\n",
      "[32]\tvalidation_0-mae:0.69095\n",
      "Stopping. Best iteration:\n",
      "[30]\tvalidation_0-mae:0.69086\n",
      "\n",
      "[0]\tvalidation_0-mae:72.40811\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:51.11698\n",
      "[2]\tvalidation_0-mae:36.09253\n",
      "[3]\tvalidation_0-mae:25.49885\n",
      "[4]\tvalidation_0-mae:18.03033\n",
      "[5]\tvalidation_0-mae:12.77881\n",
      "[6]\tvalidation_0-mae:9.17167\n",
      "[7]\tvalidation_0-mae:6.72905\n",
      "[8]\tvalidation_0-mae:5.28773\n",
      "[9]\tvalidation_0-mae:4.36414\n",
      "[10]\tvalidation_0-mae:3.83503\n",
      "[11]\tvalidation_0-mae:3.42816\n",
      "[12]\tvalidation_0-mae:3.13539\n",
      "[13]\tvalidation_0-mae:2.89961\n",
      "[14]\tvalidation_0-mae:2.70009\n",
      "[15]\tvalidation_0-mae:2.58856\n",
      "[16]\tvalidation_0-mae:2.48228\n",
      "[17]\tvalidation_0-mae:2.34635\n",
      "[18]\tvalidation_0-mae:2.26797\n",
      "[19]\tvalidation_0-mae:2.20376\n",
      "[20]\tvalidation_0-mae:2.12674\n",
      "[21]\tvalidation_0-mae:2.06756\n",
      "[22]\tvalidation_0-mae:1.98866\n",
      "[23]\tvalidation_0-mae:1.89952\n",
      "[24]\tvalidation_0-mae:1.83114\n",
      "[25]\tvalidation_0-mae:1.76225\n",
      "[26]\tvalidation_0-mae:1.70032\n",
      "[27]\tvalidation_0-mae:1.66146\n",
      "[28]\tvalidation_0-mae:1.62287\n",
      "[29]\tvalidation_0-mae:1.60681\n",
      "[30]\tvalidation_0-mae:1.56369\n",
      "[31]\tvalidation_0-mae:1.50610\n",
      "[32]\tvalidation_0-mae:1.45795\n",
      "[33]\tvalidation_0-mae:1.42382\n",
      "[34]\tvalidation_0-mae:1.39358\n",
      "[35]\tvalidation_0-mae:1.38127\n",
      "[36]\tvalidation_0-mae:1.35134\n",
      "[37]\tvalidation_0-mae:1.31844\n",
      "[38]\tvalidation_0-mae:1.29544\n",
      "[39]\tvalidation_0-mae:1.27450\n",
      "[40]\tvalidation_0-mae:1.26628\n",
      "[41]\tvalidation_0-mae:1.24765\n",
      "[42]\tvalidation_0-mae:1.24023\n",
      "[43]\tvalidation_0-mae:1.22329\n",
      "[44]\tvalidation_0-mae:1.20685\n",
      "[45]\tvalidation_0-mae:1.18817\n",
      "[46]\tvalidation_0-mae:1.17816\n",
      "[47]\tvalidation_0-mae:1.17822\n",
      "[48]\tvalidation_0-mae:1.16976\n",
      "[49]\tvalidation_0-mae:1.15631\n",
      "[50]\tvalidation_0-mae:1.14823\n",
      "[51]\tvalidation_0-mae:1.14261\n",
      "[52]\tvalidation_0-mae:1.14069\n",
      "[53]\tvalidation_0-mae:1.13225\n",
      "[54]\tvalidation_0-mae:1.12668\n",
      "[55]\tvalidation_0-mae:1.11341\n",
      "[56]\tvalidation_0-mae:1.11349\n",
      "[57]\tvalidation_0-mae:1.11363\n",
      "Stopping. Best iteration:\n",
      "[55]\tvalidation_0-mae:1.11341\n",
      "\n",
      "[0]\tvalidation_0-mae:69.99423\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:49.43825\n",
      "[2]\tvalidation_0-mae:34.91856\n",
      "[3]\tvalidation_0-mae:24.64908\n",
      "[4]\tvalidation_0-mae:17.41016\n",
      "[5]\tvalidation_0-mae:12.32618\n",
      "[6]\tvalidation_0-mae:8.76508\n",
      "[7]\tvalidation_0-mae:6.31590\n",
      "[8]\tvalidation_0-mae:4.58976\n",
      "[9]\tvalidation_0-mae:3.40975\n",
      "[10]\tvalidation_0-mae:2.56048\n",
      "[11]\tvalidation_0-mae:1.94232\n",
      "[12]\tvalidation_0-mae:1.50383\n",
      "[13]\tvalidation_0-mae:1.15936\n",
      "[14]\tvalidation_0-mae:0.90457\n",
      "[15]\tvalidation_0-mae:0.72673\n",
      "[16]\tvalidation_0-mae:0.58364\n",
      "[17]\tvalidation_0-mae:0.50086\n",
      "[18]\tvalidation_0-mae:0.44787\n",
      "[19]\tvalidation_0-mae:0.40286\n",
      "[20]\tvalidation_0-mae:0.37916\n",
      "[21]\tvalidation_0-mae:0.36348\n",
      "[22]\tvalidation_0-mae:0.35813\n",
      "[23]\tvalidation_0-mae:0.35733\n",
      "[24]\tvalidation_0-mae:0.35688\n",
      "[25]\tvalidation_0-mae:0.35670\n",
      "[26]\tvalidation_0-mae:0.35802\n",
      "[27]\tvalidation_0-mae:0.35829\n",
      "Stopping. Best iteration:\n",
      "[25]\tvalidation_0-mae:0.35670\n",
      "\n",
      "DataFrame: bands imput_method :trees model :KNN score:  0.59\n",
      "DataFrame: bands imput_method :trees model :XGB score:  0.62\n",
      "DataFrame: bands imput_method :trees model :MLP score:  0.59\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 44.9213 - mae: 44.9214 - val_loss: 44.6832 - val_mae: 44.6832\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 44.8058 - mae: 44.8058 - val_loss: 44.5523 - val_mae: 44.5523\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 44.6701 - mae: 44.6701 - val_loss: 44.3997 - val_mae: 44.3997\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 44.5096 - mae: 44.5096 - val_loss: 44.2256 - val_mae: 44.2256\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 44.3210 - mae: 44.3210 - val_loss: 44.0241 - val_mae: 44.0241\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 44.1016 - mae: 44.1016 - val_loss: 43.7944 - val_mae: 43.7944\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 43.8497 - mae: 43.8497 - val_loss: 43.5328 - val_mae: 43.5328\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 43.5638 - mae: 43.5638 - val_loss: 43.2436 - val_mae: 43.2436\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 43.2430 - mae: 43.2430 - val_loss: 42.9190 - val_mae: 42.9190\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 125us/sample - loss: 42.8865 - mae: 42.8865 - val_loss: 42.5571 - val_mae: 42.5571\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 42.4936 - mae: 42.4936 - val_loss: 42.1644 - val_mae: 42.1644\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 42.0640 - mae: 42.0640 - val_loss: 41.7327 - val_mae: 41.7327\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 41.5972 - mae: 41.5972 - val_loss: 41.2650 - val_mae: 41.2650\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 41.0930 - mae: 41.0930 - val_loss: 40.7572 - val_mae: 40.7572\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 40.5512 - mae: 40.5512 - val_loss: 40.2189 - val_mae: 40.2189\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 39.9716 - mae: 39.9716 - val_loss: 39.6377 - val_mae: 39.6377\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 39.3541 - mae: 39.3541 - val_loss: 39.0131 - val_mae: 39.0131\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 38.6987 - mae: 38.6987 - val_loss: 38.3462 - val_mae: 38.3462\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 38.0053 - mae: 38.0053 - val_loss: 37.6339 - val_mae: 37.6339\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 37.2740 - mae: 37.2740 - val_loss: 36.8747 - val_mae: 36.8747\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 36.5047 - mae: 36.5047 - val_loss: 36.0831 - val_mae: 36.0831\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 35.6974 - mae: 35.6974 - val_loss: 35.2689 - val_mae: 35.2689\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 34.8523 - mae: 34.8523 - val_loss: 34.3984 - val_mae: 34.3984\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 33.9695 - mae: 33.9695 - val_loss: 33.4889 - val_mae: 33.4889\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 33.0489 - mae: 33.0489 - val_loss: 32.5498 - val_mae: 32.5498\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 32.0907 - mae: 32.0907 - val_loss: 31.5815 - val_mae: 31.5815\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 31.0950 - mae: 31.0950 - val_loss: 30.5762 - val_mae: 30.5762\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 30.0618 - mae: 30.0618 - val_loss: 29.5075 - val_mae: 29.5075\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 28.9914 - mae: 28.9914 - val_loss: 28.4166 - val_mae: 28.4166\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 27.8838 - mae: 27.8838 - val_loss: 27.2812 - val_mae: 27.2812\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 26.7392 - mae: 26.7392 - val_loss: 26.1068 - val_mae: 26.1068\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 25.5576 - mae: 25.5576 - val_loss: 24.9370 - val_mae: 24.9370\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 24.3409 - mae: 24.3409 - val_loss: 23.5354 - val_mae: 23.5354\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 23.0985 - mae: 23.0985 - val_loss: 21.6439 - val_mae: 21.6439\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 21.8136 - mae: 21.8136 - val_loss: 20.2638 - val_mae: 20.2638\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 20.5023 - mae: 20.5023 - val_loss: 18.9212 - val_mae: 18.9212\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 19.1536 - mae: 19.1536 - val_loss: 17.2674 - val_mae: 17.2674\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 17.7860 - mae: 17.7860 - val_loss: 15.8465 - val_mae: 15.8465\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 16.3992 - mae: 16.3992 - val_loss: 14.2513 - val_mae: 14.2513\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 14.9739 - mae: 14.9739 - val_loss: 12.5753 - val_mae: 12.5753\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 13.4980 - mae: 13.4980 - val_loss: 11.2645 - val_mae: 11.2645\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 12.2234 - mae: 12.2234 - val_loss: 9.8317 - val_mae: 9.8317\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 10.8203 - mae: 10.8203 - val_loss: 8.2846 - val_mae: 8.2846\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 9.3591 - mae: 9.3591 - val_loss: 7.3648 - val_mae: 7.3648\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 8.3134 - mae: 8.3134 - val_loss: 6.3371 - val_mae: 6.3371\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 7.2130 - mae: 7.2130 - val_loss: 5.5631 - val_mae: 5.5631\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 6.4407 - mae: 6.4407 - val_loss: 5.1288 - val_mae: 5.1288\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 5.8221 - mae: 5.8221 - val_loss: 4.8098 - val_mae: 4.8098\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 5.4433 - mae: 5.4433 - val_loss: 4.5854 - val_mae: 4.5854\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 4.9655 - mae: 4.9655 - val_loss: 4.4650 - val_mae: 4.4650\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 4.7905 - mae: 4.7905 - val_loss: 4.2609 - val_mae: 4.2609\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 4.6137 - mae: 4.6137 - val_loss: 4.0848 - val_mae: 4.0848\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 4.1135 - mae: 4.1135 - val_loss: 3.9680 - val_mae: 3.9680\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.2988 - mae: 4.2988 - val_loss: 3.7879 - val_mae: 3.7879\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 4.2561 - mae: 4.2561 - val_loss: 3.6562 - val_mae: 3.6562\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 4.0529 - mae: 4.0529 - val_loss: 3.6768 - val_mae: 3.6768\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 134us/sample - loss: 3.9543 - mae: 3.9543 - val_loss: 3.4789 - val_mae: 3.4789\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.7401 - mae: 3.7401 - val_loss: 3.3164 - val_mae: 3.3164\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.7259 - mae: 3.7259 - val_loss: 3.1972 - val_mae: 3.1972\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.8365 - mae: 3.8365 - val_loss: 3.1150 - val_mae: 3.1150\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.4881 - mae: 3.4881 - val_loss: 2.9807 - val_mae: 2.9807\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.6753 - mae: 3.6753 - val_loss: 2.9210 - val_mae: 2.9210\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.2519 - mae: 3.2519 - val_loss: 2.8503 - val_mae: 2.8503\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.3403 - mae: 3.3403 - val_loss: 2.8132 - val_mae: 2.8132\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.3959 - mae: 3.3959 - val_loss: 2.7526 - val_mae: 2.7526\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.3058 - mae: 3.3058 - val_loss: 2.7731 - val_mae: 2.7731\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.0835 - mae: 3.0835 - val_loss: 2.7009 - val_mae: 2.7009\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.1874 - mae: 3.1874 - val_loss: 2.5240 - val_mae: 2.5240\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.2425 - mae: 3.2425 - val_loss: 2.5437 - val_mae: 2.5437\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.1518 - mae: 3.1518 - val_loss: 2.3860 - val_mae: 2.3860\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.1905 - mae: 3.1905 - val_loss: 2.3657 - val_mae: 2.3657\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.9274 - mae: 2.9274 - val_loss: 2.3651 - val_mae: 2.3651\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 2.9067 - mae: 2.9067 - val_loss: 2.2594 - val_mae: 2.2594\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.8264 - mae: 2.8264 - val_loss: 2.2832 - val_mae: 2.2832\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.0314 - mae: 3.0314 - val_loss: 2.1764 - val_mae: 2.1764\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 2.8681 - mae: 2.8681 - val_loss: 2.2146 - val_mae: 2.2146\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 2.9816 - mae: 2.9816 - val_loss: 2.2166 - val_mae: 2.2166\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 2.2166 - mae: 2.2166\n",
      "Val score is 2.2165896892547607\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 50.7324 - mae: 50.7324 - val_loss: 50.3324 - val_mae: 50.3324\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 50.6045 - mae: 50.6045 - val_loss: 50.2246 - val_mae: 50.2246\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 50.4565 - mae: 50.4565 - val_loss: 50.0965 - val_mae: 50.0965\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 50.2840 - mae: 50.2840 - val_loss: 49.9362 - val_mae: 49.9362\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 50.0837 - mae: 50.0837 - val_loss: 49.7490 - val_mae: 49.7490\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 49.8531 - mae: 49.8531 - val_loss: 49.5312 - val_mae: 49.5312\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 49.5905 - mae: 49.5905 - val_loss: 49.2776 - val_mae: 49.2776\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 49.2945 - mae: 49.2945 - val_loss: 48.9933 - val_mae: 48.9933\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 48.9641 - mae: 48.9641 - val_loss: 48.6727 - val_mae: 48.6727\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 48.5986 - mae: 48.5986 - val_loss: 48.3126 - val_mae: 48.3126\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 48.1973 - mae: 48.1973 - val_loss: 47.9181 - val_mae: 47.9181\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 47.7596 - mae: 47.7596 - val_loss: 47.4768 - val_mae: 47.4768\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 47.2853 - mae: 47.2853 - val_loss: 47.0074 - val_mae: 47.0074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 46.7740 - mae: 46.7740 - val_loss: 46.4982 - val_mae: 46.4982\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 46.2255 - mae: 46.2255 - val_loss: 45.9551 - val_mae: 45.9550\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 45.6396 - mae: 45.6396 - val_loss: 45.3647 - val_mae: 45.3647\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 45.0162 - mae: 45.0162 - val_loss: 44.7364 - val_mae: 44.7364\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 44.3552 - mae: 44.3552 - val_loss: 44.0751 - val_mae: 44.0751\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 43.6566 - mae: 43.6566 - val_loss: 43.3647 - val_mae: 43.3647\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 42.9202 - mae: 42.9202 - val_loss: 42.6155 - val_mae: 42.6155\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 42.1461 - mae: 42.1461 - val_loss: 41.8203 - val_mae: 41.8203\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 41.3344 - mae: 41.3344 - val_loss: 40.9897 - val_mae: 40.9897\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 40.4851 - mae: 40.4851 - val_loss: 40.1126 - val_mae: 40.1126\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 39.5981 - mae: 39.5981 - val_loss: 39.2141 - val_mae: 39.2141\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 38.6737 - mae: 38.6737 - val_loss: 38.2798 - val_mae: 38.2798\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 37.7118 - mae: 37.7118 - val_loss: 37.2991 - val_mae: 37.2991\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 36.7126 - mae: 36.7126 - val_loss: 36.2754 - val_mae: 36.2754\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 35.6761 - mae: 35.6761 - val_loss: 35.2090 - val_mae: 35.2090\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 34.6024 - mae: 34.6024 - val_loss: 34.1172 - val_mae: 34.1172\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 33.4918 - mae: 33.4918 - val_loss: 32.9802 - val_mae: 32.9802\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 32.3442 - mae: 32.3442 - val_loss: 31.7934 - val_mae: 31.7934\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 31.1598 - mae: 31.1598 - val_loss: 30.6316 - val_mae: 30.6316\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 29.9387 - mae: 29.9387 - val_loss: 29.4094 - val_mae: 29.4094\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 28.6811 - mae: 28.6811 - val_loss: 28.1062 - val_mae: 28.1062\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 27.3870 - mae: 27.3870 - val_loss: 26.8046 - val_mae: 26.8046\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 26.0567 - mae: 26.0567 - val_loss: 25.4438 - val_mae: 25.4438\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 24.6915 - mae: 24.6915 - val_loss: 23.3598 - val_mae: 23.3598\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 23.3038 - mae: 23.3038 - val_loss: 20.8907 - val_mae: 20.8907\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 21.8933 - mae: 21.8933 - val_loss: 18.6928 - val_mae: 18.6928\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 20.4201 - mae: 20.4201 - val_loss: 17.0719 - val_mae: 17.0719\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 18.9686 - mae: 18.9686 - val_loss: 15.6521 - val_mae: 15.6521\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 17.4566 - mae: 17.4566 - val_loss: 14.4398 - val_mae: 14.4398\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 15.9398 - mae: 15.9398 - val_loss: 12.8253 - val_mae: 12.8253\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 14.3568 - mae: 14.3568 - val_loss: 11.7519 - val_mae: 11.7519\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 12.8222 - mae: 12.8222 - val_loss: 10.5351 - val_mae: 10.5351\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 11.2560 - mae: 11.2560 - val_loss: 9.0270 - val_mae: 9.0270\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 9.8396 - mae: 9.8396 - val_loss: 7.4004 - val_mae: 7.4004\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 8.4650 - mae: 8.4650 - val_loss: 6.6747 - val_mae: 6.6747\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 7.4110 - mae: 7.4110 - val_loss: 5.6297 - val_mae: 5.6297\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 6.3388 - mae: 6.3388 - val_loss: 5.0055 - val_mae: 5.0055\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 5.8466 - mae: 5.8466 - val_loss: 4.6898 - val_mae: 4.6898\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 5.2129 - mae: 5.2129 - val_loss: 4.4074 - val_mae: 4.4074\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 4.7014 - mae: 4.7014 - val_loss: 4.1441 - val_mae: 4.1441\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.5571 - mae: 4.5571 - val_loss: 4.0641 - val_mae: 4.0641\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 4.5262 - mae: 4.5262 - val_loss: 3.9065 - val_mae: 3.9065\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.1458 - mae: 4.1458 - val_loss: 3.6955 - val_mae: 3.6955\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 131us/sample - loss: 4.1317 - mae: 4.1317 - val_loss: 3.6119 - val_mae: 3.6119\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 4.0473 - mae: 4.0473 - val_loss: 3.3703 - val_mae: 3.3703\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.7357 - mae: 3.7357 - val_loss: 3.3149 - val_mae: 3.3149\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.6151 - mae: 3.6151 - val_loss: 3.1938 - val_mae: 3.1938\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.6408 - mae: 3.6408 - val_loss: 3.1025 - val_mae: 3.1025\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.3764 - mae: 3.3764 - val_loss: 3.1063 - val_mae: 3.1063\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 3.5386 - mae: 3.5386 - val_loss: 2.9297 - val_mae: 2.9297\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 3.5384 - mae: 3.5384 - val_loss: 2.9444 - val_mae: 2.9444\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.3325 - mae: 3.3325 - val_loss: 2.9152 - val_mae: 2.9152\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.2974 - mae: 3.2974 - val_loss: 2.8167 - val_mae: 2.8167\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.2209 - mae: 3.2209 - val_loss: 2.7952 - val_mae: 2.7952\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.3641 - mae: 3.3641 - val_loss: 2.6694 - val_mae: 2.6694\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 3.2537 - mae: 3.2537 - val_loss: 2.7085 - val_mae: 2.7085\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 126us/sample - loss: 3.1090 - mae: 3.1090 - val_loss: 2.6202 - val_mae: 2.6202\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.0975 - mae: 3.0975 - val_loss: 2.5865 - val_mae: 2.5865\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 3.0382 - mae: 3.0382 - val_loss: 2.5957 - val_mae: 2.5957\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.0689 - mae: 3.0689 - val_loss: 2.4413 - val_mae: 2.4413\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 2.9303 - mae: 2.9303 - val_loss: 2.3888 - val_mae: 2.3888\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.8301 - mae: 2.8301 - val_loss: 2.3572 - val_mae: 2.3572\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 2.9386 - mae: 2.9386 - val_loss: 2.3073 - val_mae: 2.3073\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 3.0482 - mae: 3.0482 - val_loss: 2.4166 - val_mae: 2.4166\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.9278 - mae: 2.9278 - val_loss: 2.1900 - val_mae: 2.1900\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 2.9371 - mae: 2.9371 - val_loss: 2.2473 - val_mae: 2.2473\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 2.8696 - mae: 2.8696 - val_loss: 2.2170 - val_mae: 2.2170\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 2.2170 - mae: 2.2170\n",
      "Val score is 2.2169570922851562\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 0.8226 - mae: 0.8226 - val_loss: 0.1861 - val_mae: 0.1861\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 0.5585 - mae: 0.5585 - val_loss: 0.1435 - val_mae: 0.1435\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 122us/sample - loss: 0.4395 - mae: 0.4395 - val_loss: 0.1394 - val_mae: 0.1394\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 0.3896 - mae: 0.3896 - val_loss: 0.1393 - val_mae: 0.1393\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 0.3213 - mae: 0.3213 - val_loss: 0.1296 - val_mae: 0.1296\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 0.2911 - mae: 0.2911 - val_loss: 0.1285 - val_mae: 0.1285\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 0.2742 - mae: 0.2742 - val_loss: 0.1229 - val_mae: 0.1229\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 0.2715 - mae: 0.2715 - val_loss: 0.1228 - val_mae: 0.1228\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 0.2478 - mae: 0.2478 - val_loss: 0.1237 - val_mae: 0.1237\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 0.2474 - mae: 0.2474 - val_loss: 0.1283 - val_mae: 0.1283\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 0.1283 - mae: 0.1283\n",
      "Val score is 0.12831193208694458\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 15.3086 - mae: 15.3086 - val_loss: 15.3662 - val_mae: 15.3662\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 15.1769 - mae: 15.1769 - val_loss: 15.2032 - val_mae: 15.2032\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 15.0251 - mae: 15.0251 - val_loss: 15.0151 - val_mae: 15.0151\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 14.8486 - mae: 14.8486 - val_loss: 14.8021 - val_mae: 14.8021\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 14.6441 - mae: 14.6441 - val_loss: 14.5633 - val_mae: 14.5633\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 14.4092 - mae: 14.4092 - val_loss: 14.2927 - val_mae: 14.2927\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 14.1420 - mae: 14.1420 - val_loss: 13.9996 - val_mae: 13.9996\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 13.8413 - mae: 13.8413 - val_loss: 13.6682 - val_mae: 13.6682\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 13.5061 - mae: 13.5061 - val_loss: 13.3052 - val_mae: 13.3052\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 13.1357 - mae: 13.1357 - val_loss: 12.9059 - val_mae: 12.9059\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 12.7295 - mae: 12.7295 - val_loss: 12.4708 - val_mae: 12.4708\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 12.2870 - mae: 12.2870 - val_loss: 12.0131 - val_mae: 12.0131\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 11.8079 - mae: 11.8079 - val_loss: 11.4984 - val_mae: 11.4984\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 11.2918 - mae: 11.2918 - val_loss: 10.9579 - val_mae: 10.9579\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 134us/sample - loss: 10.7406 - mae: 10.7406 - val_loss: 10.1925 - val_mae: 10.1925\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 10.1532 - mae: 10.1532 - val_loss: 9.2198 - val_mae: 9.2198\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 9.5414 - mae: 9.5414 - val_loss: 8.4413 - val_mae: 8.4413\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 8.8814 - mae: 8.8814 - val_loss: 7.6413 - val_mae: 7.6413\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 8.1833 - mae: 8.1833 - val_loss: 6.8414 - val_mae: 6.8414\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 7.4593 - mae: 7.4593 - val_loss: 6.0537 - val_mae: 6.0537\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 6.6813 - mae: 6.6813 - val_loss: 5.2242 - val_mae: 5.2242\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.8990 - mae: 5.8990 - val_loss: 4.4964 - val_mae: 4.4964\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.1011 - mae: 5.1011 - val_loss: 3.9048 - val_mae: 3.9048\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.3221 - mae: 4.3221 - val_loss: 2.9977 - val_mae: 2.9977\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.4839 - mae: 3.4839 - val_loss: 2.1790 - val_mae: 2.1790\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.6777 - mae: 2.6777 - val_loss: 1.7364 - val_mae: 1.7364\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 2.0895 - mae: 2.0895 - val_loss: 1.3226 - val_mae: 1.3226\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.6552 - mae: 1.6552 - val_loss: 1.2663 - val_mae: 1.2663\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.3908 - mae: 1.3908 - val_loss: 1.1521 - val_mae: 1.1521\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.2796 - mae: 1.2796 - val_loss: 0.9373 - val_mae: 0.9373\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.1258 - mae: 1.1258 - val_loss: 0.8654 - val_mae: 0.8654\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.1446 - mae: 1.1446 - val_loss: 0.7915 - val_mae: 0.7915\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.0096 - mae: 1.0096 - val_loss: 0.7556 - val_mae: 0.7556\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.9507 - mae: 0.9507 - val_loss: 0.7352 - val_mae: 0.7352\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 0.9364 - mae: 0.9364 - val_loss: 0.7595 - val_mae: 0.7595\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 0.9303 - mae: 0.9303 - val_loss: 0.7088 - val_mae: 0.7088\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 0.9164 - mae: 0.9164 - val_loss: 0.6832 - val_mae: 0.6832\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 124us/sample - loss: 0.9213 - mae: 0.9213 - val_loss: 0.6941 - val_mae: 0.6941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 122us/sample - loss: 0.8282 - mae: 0.8282 - val_loss: 0.6624 - val_mae: 0.6624\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.8491 - mae: 0.8491 - val_loss: 0.5882 - val_mae: 0.5882\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 0.8636 - mae: 0.8636 - val_loss: 0.5520 - val_mae: 0.5520\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.8079 - mae: 0.8079 - val_loss: 0.5817 - val_mae: 0.5817\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 0.7995 - mae: 0.7995 - val_loss: 0.5690 - val_mae: 0.5690\n",
      "365/365 [==============================] - 0s 37us/sample - loss: 0.5690 - mae: 0.5690\n",
      "Val score is 0.5690214037895203\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 78.6310 - mae: 78.6310 - val_loss: 78.4216 - val_mae: 78.4216\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 78.4965 - mae: 78.4965 - val_loss: 78.2884 - val_mae: 78.2884\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 78.3420 - mae: 78.3420 - val_loss: 78.1321 - val_mae: 78.1321\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 78.1630 - mae: 78.1630 - val_loss: 77.9536 - val_mae: 77.9537\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 77.9561 - mae: 77.9561 - val_loss: 77.7454 - val_mae: 77.7454\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 77.7189 - mae: 77.7189 - val_loss: 77.5060 - val_mae: 77.5060\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 77.4497 - mae: 77.4497 - val_loss: 77.2399 - val_mae: 77.2399\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 77.1471 - mae: 77.1471 - val_loss: 76.9325 - val_mae: 76.9325\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 76.8102 - mae: 76.8102 - val_loss: 76.5953 - val_mae: 76.5953\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 76.4381 - mae: 76.4381 - val_loss: 76.2244 - val_mae: 76.2244\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 76.0304 - mae: 76.0304 - val_loss: 75.8183 - val_mae: 75.8183\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 75.5864 - mae: 75.5864 - val_loss: 75.3677 - val_mae: 75.3677\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 122us/sample - loss: 75.1058 - mae: 75.1058 - val_loss: 74.8848 - val_mae: 74.8848\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 74.5884 - mae: 74.5884 - val_loss: 74.3552 - val_mae: 74.3552\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 74.0339 - mae: 74.0339 - val_loss: 73.7871 - val_mae: 73.7871\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 73.4421 - mae: 73.4421 - val_loss: 73.1786 - val_mae: 73.1786\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 72.8129 - mae: 72.8129 - val_loss: 72.5272 - val_mae: 72.5272\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 72.1463 - mae: 72.1463 - val_loss: 71.8471 - val_mae: 71.8471\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 71.4421 - mae: 71.4421 - val_loss: 71.1363 - val_mae: 71.1363\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 70.7003 - mae: 70.7003 - val_loss: 70.3795 - val_mae: 70.3795\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 69.9210 - mae: 69.9210 - val_loss: 69.5932 - val_mae: 69.5932\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 69.1041 - mae: 69.1041 - val_loss: 68.7543 - val_mae: 68.7543\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 68.2498 - mae: 68.2498 - val_loss: 67.8920 - val_mae: 67.8920\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 67.3580 - mae: 67.3580 - val_loss: 66.9895 - val_mae: 66.9895\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 66.4287 - mae: 66.4287 - val_loss: 66.0387 - val_mae: 66.0387\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 65.4622 - mae: 65.4622 - val_loss: 65.0596 - val_mae: 65.0596\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 64.4584 - mae: 64.4584 - val_loss: 64.0376 - val_mae: 64.0376\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 63.4175 - mae: 63.4175 - val_loss: 62.9821 - val_mae: 62.9821\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 62.3395 - mae: 62.3395 - val_loss: 61.8901 - val_mae: 61.8901\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 61.2246 - mae: 61.2246 - val_loss: 60.7616 - val_mae: 60.7616\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 60.0729 - mae: 60.0729 - val_loss: 59.5824 - val_mae: 59.5824\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 58.8844 - mae: 58.8844 - val_loss: 58.3751 - val_mae: 58.3751\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 57.6594 - mae: 57.6594 - val_loss: 57.1438 - val_mae: 57.1438\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 56.3978 - mae: 56.3978 - val_loss: 55.8639 - val_mae: 55.8639\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 55.1000 - mae: 55.1000 - val_loss: 54.5460 - val_mae: 54.5460\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 53.7659 - mae: 53.7659 - val_loss: 53.1749 - val_mae: 53.1749\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 52.3958 - mae: 52.3958 - val_loss: 51.7878 - val_mae: 51.7878\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 50.9897 - mae: 50.9897 - val_loss: 50.3611 - val_mae: 50.3611\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 49.5478 - mae: 49.5478 - val_loss: 48.9378 - val_mae: 48.9378\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 48.0702 - mae: 48.0702 - val_loss: 47.4211 - val_mae: 47.4211\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 46.5571 - mae: 46.5571 - val_loss: 45.8740 - val_mae: 45.8740\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 45.0086 - mae: 45.0086 - val_loss: 44.2679 - val_mae: 44.2679\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 43.4249 - mae: 43.4249 - val_loss: 42.6682 - val_mae: 42.6682\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 41.8060 - mae: 41.8060 - val_loss: 41.0374 - val_mae: 41.0374\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 40.1522 - mae: 40.1522 - val_loss: 39.3403 - val_mae: 39.3403\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 38.4635 - mae: 38.4635 - val_loss: 37.5864 - val_mae: 37.5864\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 36.7402 - mae: 36.7402 - val_loss: 35.8139 - val_mae: 35.8139\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 34.9823 - mae: 34.9823 - val_loss: 34.0437 - val_mae: 34.0437\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 33.1900 - mae: 33.1900 - val_loss: 32.2912 - val_mae: 32.2912\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 31.3866 - mae: 31.3866 - val_loss: 30.3527 - val_mae: 30.3527\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 29.5213 - mae: 29.5213 - val_loss: 27.8092 - val_mae: 27.8092\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 27.6346 - mae: 27.6346 - val_loss: 25.7160 - val_mae: 25.7160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 25.7115 - mae: 25.7115 - val_loss: 23.6481 - val_mae: 23.6481\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 23.8430 - mae: 23.8430 - val_loss: 21.5954 - val_mae: 21.5954\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 21.8498 - mae: 21.8498 - val_loss: 18.9540 - val_mae: 18.9540\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 19.8765 - mae: 19.8764 - val_loss: 16.5572 - val_mae: 16.5572\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 17.8570 - mae: 17.8570 - val_loss: 14.2506 - val_mae: 14.2506\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 15.8003 - mae: 15.8003 - val_loss: 12.2259 - val_mae: 12.2259\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 13.8375 - mae: 13.8375 - val_loss: 10.4025 - val_mae: 10.4025\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 11.9224 - mae: 11.9224 - val_loss: 9.1618 - val_mae: 9.1619\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 10.2383 - mae: 10.2383 - val_loss: 7.7652 - val_mae: 7.7652\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 8.6952 - mae: 8.6952 - val_loss: 6.2221 - val_mae: 6.2221\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 131us/sample - loss: 7.3020 - mae: 7.3020 - val_loss: 5.4365 - val_mae: 5.4365\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 6.1828 - mae: 6.1828 - val_loss: 4.8595 - val_mae: 4.8595\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 5.3841 - mae: 5.3841 - val_loss: 4.5102 - val_mae: 4.5102\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 4.8690 - mae: 4.8690 - val_loss: 4.2829 - val_mae: 4.2829\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 4.6603 - mae: 4.6603 - val_loss: 3.9924 - val_mae: 3.9924\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 4.2440 - mae: 4.2440 - val_loss: 3.8949 - val_mae: 3.8949\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 4.0381 - mae: 4.0381 - val_loss: 3.7423 - val_mae: 3.7423\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.0905 - mae: 4.0905 - val_loss: 3.8049 - val_mae: 3.8049\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 3.9296 - mae: 3.9296 - val_loss: 3.6715 - val_mae: 3.6715\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 3.7923 - mae: 3.7923 - val_loss: 3.6582 - val_mae: 3.6582\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 125us/sample - loss: 3.7431 - mae: 3.7431 - val_loss: 3.3989 - val_mae: 3.3989\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.5201 - mae: 3.5201 - val_loss: 3.2097 - val_mae: 3.2097\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.7609 - mae: 3.7609 - val_loss: 3.2282 - val_mae: 3.2282\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.6715 - mae: 3.6715 - val_loss: 3.1286 - val_mae: 3.1286\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.7789 - mae: 3.7789 - val_loss: 2.9422 - val_mae: 2.9422\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.3732 - mae: 3.3732 - val_loss: 2.9061 - val_mae: 2.9061\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.4719 - mae: 3.4719 - val_loss: 2.8576 - val_mae: 2.8576\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.2489 - mae: 3.2489 - val_loss: 2.7841 - val_mae: 2.7841\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.3658 - mae: 3.3658 - val_loss: 2.6915 - val_mae: 2.6915\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.3174 - mae: 3.3174 - val_loss: 2.6543 - val_mae: 2.6543\n",
      "Epoch 83/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 3.1614 - mae: 3.1614 - val_loss: 2.7269 - val_mae: 2.7269\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 3.3869 - mae: 3.3869 - val_loss: 2.7187 - val_mae: 2.7187\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 2.7187 - mae: 2.7187\n",
      "Val score is 2.7186765670776367\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 1.0498 - mae: 1.0498 - val_loss: 0.5166 - val_mae: 0.5166\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 0.7443 - mae: 0.7443 - val_loss: 0.4461 - val_mae: 0.4461\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.6236 - mae: 0.6236 - val_loss: 0.3683 - val_mae: 0.3683\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.5400 - mae: 0.5400 - val_loss: 0.2907 - val_mae: 0.2907\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 0.4650 - mae: 0.4650 - val_loss: 0.2449 - val_mae: 0.2449\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.3995 - mae: 0.3995 - val_loss: 0.2175 - val_mae: 0.2175\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 0.3534 - mae: 0.3534 - val_loss: 0.2012 - val_mae: 0.2012\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.3255 - mae: 0.3255 - val_loss: 0.1993 - val_mae: 0.1993\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 0.2917 - mae: 0.2917 - val_loss: 0.1833 - val_mae: 0.1833\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 0.2915 - mae: 0.2915 - val_loss: 0.1790 - val_mae: 0.1790\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 0.2808 - mae: 0.2808 - val_loss: 0.1765 - val_mae: 0.1765\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2591 - mae: 0.2591 - val_loss: 0.1713 - val_mae: 0.1713\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 0.2456 - mae: 0.2456 - val_loss: 0.1625 - val_mae: 0.1625\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 0.2472 - mae: 0.2472 - val_loss: 0.1632 - val_mae: 0.1632\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 0.2340 - mae: 0.2340 - val_loss: 0.1560 - val_mae: 0.1560\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 0.2298 - mae: 0.2298 - val_loss: 0.1485 - val_mae: 0.1485\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 0.2114 - mae: 0.2114 - val_loss: 0.1485 - val_mae: 0.1485\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 0.2033 - mae: 0.2033 - val_loss: 0.1374 - val_mae: 0.1374\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2169 - mae: 0.2169 - val_loss: 0.1560 - val_mae: 0.1560\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 0.1972 - mae: 0.1972 - val_loss: 0.1362 - val_mae: 0.1362\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.2079 - mae: 0.2079 - val_loss: 0.1276 - val_mae: 0.1276\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.1961 - mae: 0.1961 - val_loss: 0.1267 - val_mae: 0.1267\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.1892 - mae: 0.1892 - val_loss: 0.1255 - val_mae: 0.1255\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2132 - mae: 0.2132 - val_loss: 0.1276 - val_mae: 0.1276\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.1825 - mae: 0.1825 - val_loss: 0.1106 - val_mae: 0.1106\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.1848 - mae: 0.1848 - val_loss: 0.1166 - val_mae: 0.1166\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 112us/sample - loss: 0.1741 - mae: 0.1741 - val_loss: 0.1136 - val_mae: 0.1136\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.1136 - mae: 0.1136\n",
      "Val score is 0.11359177529811859\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 30.8458 - mae: 30.8458 - val_loss: 30.3390 - val_mae: 30.3390\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 30.7330 - mae: 30.7330 - val_loss: 30.2698 - val_mae: 30.2698\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 30.5999 - mae: 30.5999 - val_loss: 30.1776 - val_mae: 30.1776\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 30.4421 - mae: 30.4421 - val_loss: 30.0575 - val_mae: 30.0575\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 30.2560 - mae: 30.2560 - val_loss: 29.9058 - val_mae: 29.9058\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 30.0389 - mae: 30.0389 - val_loss: 29.7180 - val_mae: 29.7180\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 29.7890 - mae: 29.7890 - val_loss: 29.4941 - val_mae: 29.4941\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 29.5051 - mae: 29.5051 - val_loss: 29.2386 - val_mae: 29.2386\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 29.1859 - mae: 29.1859 - val_loss: 28.9435 - val_mae: 28.9435\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 28.8309 - mae: 28.8309 - val_loss: 28.6060 - val_mae: 28.6060\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 28.4393 - mae: 28.4393 - val_loss: 28.2275 - val_mae: 28.2275\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 28.0108 - mae: 28.0108 - val_loss: 27.8174 - val_mae: 27.8174\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 27.5450 - mae: 27.5450 - val_loss: 27.3512 - val_mae: 27.3512\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 27.0416 - mae: 27.0416 - val_loss: 26.8570 - val_mae: 26.8570\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 26.5004 - mae: 26.5005 - val_loss: 26.3184 - val_mae: 26.3184\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 25.9214 - mae: 25.9214 - val_loss: 25.7288 - val_mae: 25.7288\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 25.3044 - mae: 25.3044 - val_loss: 25.1026 - val_mae: 25.1026\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 24.6493 - mae: 24.6493 - val_loss: 24.4459 - val_mae: 24.4459\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 23.9562 - mae: 23.9562 - val_loss: 23.7510 - val_mae: 23.7510\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 23.2532 - mae: 23.2532 - val_loss: 22.8178 - val_mae: 22.8178\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 22.4600 - mae: 22.4600 - val_loss: 21.9148 - val_mae: 21.9148\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 21.6540 - mae: 21.6540 - val_loss: 21.0857 - val_mae: 21.0857\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 20.8304 - mae: 20.8304 - val_loss: 19.9970 - val_mae: 19.9970\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 19.9374 - mae: 19.9374 - val_loss: 18.9765 - val_mae: 18.9765\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 19.0151 - mae: 19.0151 - val_loss: 18.0162 - val_mae: 18.0162\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 18.0814 - mae: 18.0814 - val_loss: 17.0926 - val_mae: 17.0926\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 17.1078 - mae: 17.1078 - val_loss: 15.6800 - val_mae: 15.6800\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 16.0627 - mae: 16.0627 - val_loss: 14.4245 - val_mae: 14.4245\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 15.0124 - mae: 15.0124 - val_loss: 13.2976 - val_mae: 13.2976\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 14.0538 - mae: 14.0538 - val_loss: 12.0246 - val_mae: 12.0246\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 12.8709 - mae: 12.8709 - val_loss: 10.9806 - val_mae: 10.9806\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 130us/sample - loss: 11.7756 - mae: 11.7756 - val_loss: 9.9290 - val_mae: 9.9290\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 10.6781 - mae: 10.6781 - val_loss: 8.9808 - val_mae: 8.9808\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 9.5813 - mae: 9.5813 - val_loss: 7.9876 - val_mae: 7.9876\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 8.5951 - mae: 8.5951 - val_loss: 7.1208 - val_mae: 7.1208\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 7.6656 - mae: 7.6656 - val_loss: 6.3985 - val_mae: 6.3985\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 6.7867 - mae: 6.7867 - val_loss: 5.8106 - val_mae: 5.8106\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 6.0794 - mae: 6.0794 - val_loss: 5.3283 - val_mae: 5.3283\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.7391 - mae: 5.7391 - val_loss: 4.9555 - val_mae: 4.9555\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 5.3747 - mae: 5.3747 - val_loss: 4.9905 - val_mae: 4.9905\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.1068 - mae: 5.1068 - val_loss: 4.6430 - val_mae: 4.6430\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.0467 - mae: 5.0467 - val_loss: 4.5864 - val_mae: 4.5864\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 4.7763 - mae: 4.7763 - val_loss: 4.2297 - val_mae: 4.2297\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.5986 - mae: 4.5986 - val_loss: 4.1935 - val_mae: 4.1935\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.6834 - mae: 4.6834 - val_loss: 4.0050 - val_mae: 4.0050\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.2085 - mae: 4.2085 - val_loss: 3.8658 - val_mae: 3.8658\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.1490 - mae: 4.1490 - val_loss: 3.8603 - val_mae: 3.8603\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 4.0707 - mae: 4.0707 - val_loss: 3.6255 - val_mae: 3.6255\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 101us/sample - loss: 4.0917 - mae: 4.0917 - val_loss: 3.5799 - val_mae: 3.5799\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.9198 - mae: 3.9198 - val_loss: 3.4197 - val_mae: 3.4197\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.8615 - mae: 3.8615 - val_loss: 3.3004 - val_mae: 3.3004\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.7519 - mae: 3.7519 - val_loss: 3.2220 - val_mae: 3.2220\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 3.6707 - mae: 3.6707 - val_loss: 3.1797 - val_mae: 3.1797\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 3.7910 - mae: 3.7910 - val_loss: 3.1568 - val_mae: 3.1568\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 3.5157 - mae: 3.5157 - val_loss: 3.1885 - val_mae: 3.1885\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.5517 - mae: 3.5517 - val_loss: 2.9636 - val_mae: 2.9636\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.2396 - mae: 3.2396 - val_loss: 2.8740 - val_mae: 2.8740\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 3.4126 - mae: 3.4126 - val_loss: 2.9622 - val_mae: 2.9622\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.4124 - mae: 3.4124 - val_loss: 2.7703 - val_mae: 2.7703\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.0947 - mae: 3.0947 - val_loss: 2.7495 - val_mae: 2.7495\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.4203 - mae: 3.4203 - val_loss: 2.6123 - val_mae: 2.6123\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 3.1887 - mae: 3.1887 - val_loss: 2.6677 - val_mae: 2.6677\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 3.1266 - mae: 3.1266 - val_loss: 2.6489 - val_mae: 2.6489\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 2.6489 - mae: 2.6489\n",
      "Val score is 2.6488938331604004\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 6.2953 - mae: 6.2953 - val_loss: 5.6650 - val_mae: 5.6650\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 5.8667 - mae: 5.8667 - val_loss: 5.5563 - val_mae: 5.5563\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.6329 - mae: 5.6329 - val_loss: 5.4649 - val_mae: 5.4649\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.4617 - mae: 5.4617 - val_loss: 5.3760 - val_mae: 5.3760\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 5.3307 - mae: 5.3307 - val_loss: 5.2831 - val_mae: 5.2831\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 5.1890 - mae: 5.1890 - val_loss: 5.1754 - val_mae: 5.1754\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.0300 - mae: 5.0300 - val_loss: 5.0386 - val_mae: 5.0386\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.8754 - mae: 4.8754 - val_loss: 4.8927 - val_mae: 4.8927\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 122us/sample - loss: 4.7246 - mae: 4.7246 - val_loss: 4.7295 - val_mae: 4.7295\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 126us/sample - loss: 4.5159 - mae: 4.5159 - val_loss: 4.5427 - val_mae: 4.5427\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 131us/sample - loss: 4.3105 - mae: 4.3105 - val_loss: 4.3585 - val_mae: 4.3585\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 4.0884 - mae: 4.0884 - val_loss: 4.0934 - val_mae: 4.0934\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 3.8063 - mae: 3.8063 - val_loss: 3.8140 - val_mae: 3.8140\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.5671 - mae: 3.5671 - val_loss: 3.4782 - val_mae: 3.4782\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.2865 - mae: 3.2865 - val_loss: 3.0813 - val_mae: 3.0813\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 2.9356 - mae: 2.9356 - val_loss: 2.8004 - val_mae: 2.8004\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 126us/sample - loss: 2.5907 - mae: 2.5907 - val_loss: 2.4970 - val_mae: 2.4970\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.3328 - mae: 2.3328 - val_loss: 2.2944 - val_mae: 2.2944\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 2.3991 - mae: 2.3991 - val_loss: 2.0377 - val_mae: 2.0377\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 2.1437 - mae: 2.1437 - val_loss: 1.7651 - val_mae: 1.7651\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.9696 - mae: 1.9696 - val_loss: 1.6335 - val_mae: 1.6335\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.6359 - mae: 1.6359 - val_loss: 1.4631 - val_mae: 1.4631\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.3267 - mae: 1.3267 - val_loss: 1.2731 - val_mae: 1.2731\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.7171 - mae: 1.7171 - val_loss: 1.1308 - val_mae: 1.1308\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.5971 - mae: 1.5971 - val_loss: 1.1156 - val_mae: 1.1156\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.2422 - mae: 1.2422 - val_loss: 1.0527 - val_mae: 1.0527\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.6540 - mae: 1.6540 - val_loss: 0.9851 - val_mae: 0.9851\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.4322 - mae: 1.4322 - val_loss: 0.8608 - val_mae: 0.8608\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.5487 - mae: 1.5487 - val_loss: 0.7668 - val_mae: 0.7668\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5577 - mae: 1.5577 - val_loss: 0.7691 - val_mae: 0.7691\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.2069 - mae: 1.2069 - val_loss: 0.7596 - val_mae: 0.7596\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.1798 - mae: 1.1798 - val_loss: 0.6298 - val_mae: 0.6298\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.1735 - mae: 1.1735 - val_loss: 0.6425 - val_mae: 0.6425\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5127 - mae: 1.5127 - val_loss: 0.6723 - val_mae: 0.6723\n",
      "365/365 [==============================] - 0s 41us/sample - loss: 0.6723 - mae: 0.6723\n",
      "Val score is 0.6723084449768066\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 1851.1519 - mae: 1851.1519 - val_loss: 1851.6323 - val_mae: 1851.6323\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1851.0184 - mae: 1851.0183 - val_loss: 1851.4182 - val_mae: 1851.4181\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 1850.8651 - mae: 1850.8651 - val_loss: 1851.1685 - val_mae: 1851.1687\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1850.6905 - mae: 1850.6904 - val_loss: 1850.8442 - val_mae: 1850.8442\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1850.4915 - mae: 1850.4916 - val_loss: 1850.5385 - val_mae: 1850.5387\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1850.2621 - mae: 1850.2622 - val_loss: 1850.2163 - val_mae: 1850.2163\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1850.0008 - mae: 1850.0009 - val_loss: 1849.8600 - val_mae: 1849.8601\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1849.7062 - mae: 1849.7062 - val_loss: 1849.4767 - val_mae: 1849.4767\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1849.3771 - mae: 1849.3771 - val_loss: 1849.0591 - val_mae: 1849.0591\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1849.0126 - mae: 1849.0125 - val_loss: 1848.6107 - val_mae: 1848.6106\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1848.6123 - mae: 1848.6123 - val_loss: 1848.1329 - val_mae: 1848.1327\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1848.1754 - mae: 1848.1753 - val_loss: 1847.6277 - val_mae: 1847.6277\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1847.7017 - mae: 1847.7017 - val_loss: 1847.0921 - val_mae: 1847.0922\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1847.1909 - mae: 1847.1909 - val_loss: 1846.5227 - val_mae: 1846.5226\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1846.6428 - mae: 1846.6428 - val_loss: 1845.9221 - val_mae: 1845.9221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 134us/sample - loss: 1846.0571 - mae: 1846.0570 - val_loss: 1845.2836 - val_mae: 1845.2836\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1845.4337 - mae: 1845.4337 - val_loss: 1844.6147 - val_mae: 1844.6145\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1844.7726 - mae: 1844.7726 - val_loss: 1843.9223 - val_mae: 1843.9222\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1844.0739 - mae: 1844.0739 - val_loss: 1843.1902 - val_mae: 1843.1901\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1843.3373 - mae: 1843.3372 - val_loss: 1842.4252 - val_mae: 1842.4252\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1842.5630 - mae: 1842.5629 - val_loss: 1841.6155 - val_mae: 1841.6157\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1841.7619 - mae: 1841.7620 - val_loss: 1840.6283 - val_mae: 1840.6281\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1840.9028 - mae: 1840.9027 - val_loss: 1839.6362 - val_mae: 1839.6364\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1840.0161 - mae: 1840.0161 - val_loss: 1838.7393 - val_mae: 1838.7393\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1839.0913 - mae: 1839.0913 - val_loss: 1837.8189 - val_mae: 1837.8192\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1838.1288 - mae: 1838.1288 - val_loss: 1836.8715 - val_mae: 1836.8712\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1837.1289 - mae: 1837.1289 - val_loss: 1835.8602 - val_mae: 1835.8602\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1836.0917 - mae: 1836.0916 - val_loss: 1834.8197 - val_mae: 1834.8197\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1835.0173 - mae: 1835.0175 - val_loss: 1833.7805 - val_mae: 1833.7803\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1833.9058 - mae: 1833.9058 - val_loss: 1832.6410 - val_mae: 1832.6409\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1832.7574 - mae: 1832.7573 - val_loss: 1831.5033 - val_mae: 1831.5033\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1831.5722 - mae: 1831.5721 - val_loss: 1830.3327 - val_mae: 1830.3325\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1830.3502 - mae: 1830.3502 - val_loss: 1829.1070 - val_mae: 1829.1068\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1829.0917 - mae: 1829.0918 - val_loss: 1827.8472 - val_mae: 1827.8473\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1827.7967 - mae: 1827.7968 - val_loss: 1826.5565 - val_mae: 1826.5565\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1826.4654 - mae: 1826.4655 - val_loss: 1825.2273 - val_mae: 1825.2274\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1825.0980 - mae: 1825.0979 - val_loss: 1823.8402 - val_mae: 1823.8405\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1823.6945 - mae: 1823.6946 - val_loss: 1822.4340 - val_mae: 1822.4341\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1822.2551 - mae: 1822.2552 - val_loss: 1820.9854 - val_mae: 1820.9855\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 1820.7801 - mae: 1820.7802 - val_loss: 1819.5155 - val_mae: 1819.5157\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1819.2692 - mae: 1819.2694 - val_loss: 1817.9649 - val_mae: 1817.9648\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1817.7231 - mae: 1817.7231 - val_loss: 1816.4148 - val_mae: 1816.4147\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1816.1415 - mae: 1816.1416 - val_loss: 1814.8278 - val_mae: 1814.8279\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1814.5249 - mae: 1814.5250 - val_loss: 1813.1926 - val_mae: 1813.1926\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1812.8931 - mae: 1812.8929 - val_loss: 1811.1375 - val_mae: 1811.1375\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1811.1893 - mae: 1811.1892 - val_loss: 1808.9607 - val_mae: 1808.9608\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1809.4695 - mae: 1809.4695 - val_loss: 1807.2436 - val_mae: 1807.2435\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1807.7138 - mae: 1807.7137 - val_loss: 1805.5103 - val_mae: 1805.5101\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1805.9234 - mae: 1805.9236 - val_loss: 1803.7771 - val_mae: 1803.7769\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1804.0986 - mae: 1804.0986 - val_loss: 1801.9748 - val_mae: 1801.9746\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 1802.2397 - mae: 1802.2395 - val_loss: 1800.1393 - val_mae: 1800.1392\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1800.3466 - mae: 1800.3468 - val_loss: 1798.3209 - val_mae: 1798.3209\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1798.4198 - mae: 1798.4197 - val_loss: 1796.4375 - val_mae: 1796.4375\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1796.4591 - mae: 1796.4591 - val_loss: 1794.5121 - val_mae: 1794.5120\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1794.4649 - mae: 1794.4648 - val_loss: 1792.5281 - val_mae: 1792.5282\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1792.4372 - mae: 1792.4374 - val_loss: 1790.5593 - val_mae: 1790.5594\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1790.3795 - mae: 1790.3794 - val_loss: 1788.2519 - val_mae: 1788.2518\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1788.2885 - mae: 1788.2887 - val_loss: 1785.2683 - val_mae: 1785.2683\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1786.1667 - mae: 1786.1667 - val_loss: 1783.1389 - val_mae: 1783.1390\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1784.0080 - mae: 1784.0081 - val_loss: 1781.0825 - val_mae: 1781.0825\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1781.8154 - mae: 1781.8154 - val_loss: 1779.0703 - val_mae: 1779.0702\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1779.5899 - mae: 1779.5898 - val_loss: 1776.9785 - val_mae: 1776.9786\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1777.3318 - mae: 1777.3319 - val_loss: 1774.7246 - val_mae: 1774.7246\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 1775.0413 - mae: 1775.0413 - val_loss: 1772.5306 - val_mae: 1772.5305\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1772.7186 - mae: 1772.7186 - val_loss: 1770.3034 - val_mae: 1770.3032\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1770.3638 - mae: 1770.3639 - val_loss: 1767.9984 - val_mae: 1767.9983\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 1767.9771 - mae: 1767.9771 - val_loss: 1765.6605 - val_mae: 1765.6604\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1765.5585 - mae: 1765.5586 - val_loss: 1763.2399 - val_mae: 1763.2401\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1763.1083 - mae: 1763.1084 - val_loss: 1760.8696 - val_mae: 1760.8694\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1760.6265 - mae: 1760.6267 - val_loss: 1758.4584 - val_mae: 1758.4584\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1758.1133 - mae: 1758.1135 - val_loss: 1755.9906 - val_mae: 1755.9906\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1755.5688 - mae: 1755.5686 - val_loss: 1753.4349 - val_mae: 1753.4348\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1752.9965 - mae: 1752.9965 - val_loss: 1750.3889 - val_mae: 1750.3890\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1750.3943 - mae: 1750.3945 - val_loss: 1745.6676 - val_mae: 1745.6678\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1747.7634 - mae: 1747.7635 - val_loss: 1742.7723 - val_mae: 1742.7722\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1745.0967 - mae: 1745.0969 - val_loss: 1740.2686 - val_mae: 1740.2687\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1742.3979 - mae: 1742.3979 - val_loss: 1737.8573 - val_mae: 1737.8574\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1739.6682 - mae: 1739.6681 - val_loss: 1735.3822 - val_mae: 1735.3823\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1736.9080 - mae: 1736.9078 - val_loss: 1732.7849 - val_mae: 1732.7848\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1734.1174 - mae: 1734.1173 - val_loss: 1730.1654 - val_mae: 1730.1654\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1731.2966 - mae: 1731.2966 - val_loss: 1727.5321 - val_mae: 1727.5320\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1728.4458 - mae: 1728.4459 - val_loss: 1724.8629 - val_mae: 1724.8628\n",
      "Epoch 83/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1725.5650 - mae: 1725.5651 - val_loss: 1722.1977 - val_mae: 1722.1974\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1722.6544 - mae: 1722.6544 - val_loss: 1719.4125 - val_mae: 1719.4127\n",
      "Epoch 85/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1719.7141 - mae: 1719.7140 - val_loss: 1716.5415 - val_mae: 1716.5416\n",
      "Epoch 86/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1716.7442 - mae: 1716.7441 - val_loss: 1713.7466 - val_mae: 1713.7465\n",
      "Epoch 87/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1713.7448 - mae: 1713.7448 - val_loss: 1710.9009 - val_mae: 1710.9012\n",
      "Epoch 88/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1710.7161 - mae: 1710.7161 - val_loss: 1707.9126 - val_mae: 1707.9127\n",
      "Epoch 89/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1707.6581 - mae: 1707.6582 - val_loss: 1704.8302 - val_mae: 1704.8303\n",
      "Epoch 90/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1704.5710 - mae: 1704.5709 - val_loss: 1701.8401 - val_mae: 1701.8402\n",
      "Epoch 91/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1701.4548 - mae: 1701.4548 - val_loss: 1698.8152 - val_mae: 1698.8151\n",
      "Epoch 92/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1698.4088 - mae: 1698.4091 - val_loss: 1694.9953 - val_mae: 1694.9954\n",
      "Epoch 93/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1695.1464 - mae: 1695.1464 - val_loss: 1688.4965 - val_mae: 1688.4967\n",
      "Epoch 94/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1691.9528 - mae: 1691.9529 - val_loss: 1685.0719 - val_mae: 1685.0721\n",
      "Epoch 95/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1688.7239 - mae: 1688.7238 - val_loss: 1682.0580 - val_mae: 1682.0579\n",
      "Epoch 96/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1685.4647 - mae: 1685.4647 - val_loss: 1679.2195 - val_mae: 1679.2194\n",
      "Epoch 97/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1682.1765 - mae: 1682.1764 - val_loss: 1676.3571 - val_mae: 1676.3572\n",
      "Epoch 98/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1678.8600 - mae: 1678.8600 - val_loss: 1673.3361 - val_mae: 1673.3362\n",
      "Epoch 99/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1675.5152 - mae: 1675.5153 - val_loss: 1670.2657 - val_mae: 1670.2657\n",
      "Epoch 100/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1672.1424 - mae: 1672.1423 - val_loss: 1667.2014 - val_mae: 1667.2014\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 1667.2014 - mae: 1667.2014\n",
      "Val score is 1667.201416015625\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 55.5919 - mae: 55.5919 - val_loss: 54.7543 - val_mae: 54.7543\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 55.4541 - mae: 55.4541 - val_loss: 54.6974 - val_mae: 54.6974\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 55.2963 - mae: 55.2963 - val_loss: 54.6166 - val_mae: 54.6166\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 55.1138 - mae: 55.1138 - val_loss: 54.5044 - val_mae: 54.5044\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 54.9035 - mae: 54.9035 - val_loss: 54.3636 - val_mae: 54.3636\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 54.6628 - mae: 54.6628 - val_loss: 54.1890 - val_mae: 54.1890\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 54.3901 - mae: 54.3901 - val_loss: 53.9753 - val_mae: 53.9753\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 54.0841 - mae: 54.0841 - val_loss: 53.7231 - val_mae: 53.7231\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 53.7438 - mae: 53.7438 - val_loss: 53.4340 - val_mae: 53.4340\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 53.3684 - mae: 53.3684 - val_loss: 53.1091 - val_mae: 53.1091\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 52.9575 - mae: 52.9575 - val_loss: 52.7369 - val_mae: 52.7369\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 52.5105 - mae: 52.5105 - val_loss: 52.3266 - val_mae: 52.3266\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 52.0270 - mae: 52.0270 - val_loss: 51.8676 - val_mae: 51.8676\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 51.5068 - mae: 51.5068 - val_loss: 51.3713 - val_mae: 51.3713\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 50.9497 - mae: 50.9497 - val_loss: 50.8291 - val_mae: 50.8291\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 50.3554 - mae: 50.3554 - val_loss: 50.2338 - val_mae: 50.2338\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 49.7239 - mae: 49.7239 - val_loss: 49.6045 - val_mae: 49.6045\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 49.0550 - mae: 49.0550 - val_loss: 48.9298 - val_mae: 48.9298\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 48.3488 - mae: 48.3488 - val_loss: 48.2139 - val_mae: 48.2139\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 47.6051 - mae: 47.6051 - val_loss: 47.4463 - val_mae: 47.4463\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 46.8239 - mae: 46.8239 - val_loss: 46.6409 - val_mae: 46.6409\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 46.0053 - mae: 46.0053 - val_loss: 45.8063 - val_mae: 45.8063\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 45.1493 - mae: 45.1493 - val_loss: 44.9384 - val_mae: 44.9384\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 44.2560 - mae: 44.2560 - val_loss: 44.0255 - val_mae: 44.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 43.3253 - mae: 43.3253 - val_loss: 43.0883 - val_mae: 43.0883\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 42.3574 - mae: 42.3574 - val_loss: 42.1060 - val_mae: 42.1060\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 144us/sample - loss: 41.3523 - mae: 41.3523 - val_loss: 41.0867 - val_mae: 41.0867\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 40.3102 - mae: 40.3102 - val_loss: 40.0329 - val_mae: 40.0329\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 39.2310 - mae: 39.2310 - val_loss: 38.9183 - val_mae: 38.9183\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 38.1150 - mae: 38.1150 - val_loss: 37.7689 - val_mae: 37.7689\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 36.9623 - mae: 36.9623 - val_loss: 36.5857 - val_mae: 36.5857\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 35.8009 - mae: 35.8009 - val_loss: 34.6943 - val_mae: 34.6943\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 34.5530 - mae: 34.5530 - val_loss: 32.9587 - val_mae: 32.9586\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 33.2925 - mae: 33.2925 - val_loss: 31.6072 - val_mae: 31.6072\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 31.9942 - mae: 31.9942 - val_loss: 30.3203 - val_mae: 30.3203\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 30.6593 - mae: 30.6593 - val_loss: 29.0402 - val_mae: 29.0402\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 29.2882 - mae: 29.2882 - val_loss: 27.7272 - val_mae: 27.7272\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 27.8812 - mae: 27.8812 - val_loss: 26.3671 - val_mae: 26.3671\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 26.4384 - mae: 26.4384 - val_loss: 24.9839 - val_mae: 24.9839\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 24.9599 - mae: 24.9599 - val_loss: 23.5798 - val_mae: 23.5798\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 23.4464 - mae: 23.4464 - val_loss: 21.9580 - val_mae: 21.9580\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 21.9011 - mae: 21.9011 - val_loss: 19.9733 - val_mae: 19.9733\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 20.3196 - mae: 20.3196 - val_loss: 18.3183 - val_mae: 18.3183\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 18.7130 - mae: 18.7130 - val_loss: 16.3195 - val_mae: 16.3195\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 17.0857 - mae: 17.0857 - val_loss: 14.4159 - val_mae: 14.4159\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 15.4734 - mae: 15.4734 - val_loss: 11.7723 - val_mae: 11.7723\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 13.7206 - mae: 13.7206 - val_loss: 9.9768 - val_mae: 9.9768\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 12.0401 - mae: 12.0401 - val_loss: 7.9616 - val_mae: 7.9616\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 10.2834 - mae: 10.2834 - val_loss: 6.7044 - val_mae: 6.7044\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 8.5710 - mae: 8.5710 - val_loss: 5.4793 - val_mae: 5.4793\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 7.0170 - mae: 7.0170 - val_loss: 4.0068 - val_mae: 4.0068\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.4963 - mae: 5.4963 - val_loss: 3.6013 - val_mae: 3.6013\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 4.3705 - mae: 4.3705 - val_loss: 2.7808 - val_mae: 2.7808\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.3910 - mae: 3.3910 - val_loss: 2.6099 - val_mae: 2.6099\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 125us/sample - loss: 2.7004 - mae: 2.7004 - val_loss: 2.9088 - val_mae: 2.9088\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.4692 - mae: 2.4692 - val_loss: 2.5334 - val_mae: 2.5334\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 2.1013 - mae: 2.1013 - val_loss: 2.5004 - val_mae: 2.5004\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.0200 - mae: 2.0200 - val_loss: 2.6045 - val_mae: 2.6045\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.9670 - mae: 1.9670 - val_loss: 2.3497 - val_mae: 2.3497\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.9997 - mae: 1.9997 - val_loss: 2.1100 - val_mae: 2.1100\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.8826 - mae: 1.8826 - val_loss: 1.9798 - val_mae: 1.9798\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.8216 - mae: 1.8216 - val_loss: 2.0000 - val_mae: 2.0000\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.8764 - mae: 1.8764 - val_loss: 1.8309 - val_mae: 1.8309\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.6480 - mae: 1.6480 - val_loss: 1.8923 - val_mae: 1.8923\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.7646 - mae: 1.7646 - val_loss: 1.7546 - val_mae: 1.7546\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.6854 - mae: 1.6854 - val_loss: 1.6774 - val_mae: 1.6774\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.5540 - mae: 1.5540 - val_loss: 1.5930 - val_mae: 1.5930\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.7066 - mae: 1.7066 - val_loss: 1.6140 - val_mae: 1.6140\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.4479 - mae: 1.4479 - val_loss: 1.3504 - val_mae: 1.3504\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.6481 - mae: 1.6481 - val_loss: 1.3388 - val_mae: 1.3388\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.6193 - mae: 1.6193 - val_loss: 1.2967 - val_mae: 1.2967\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1.3807 - mae: 1.3807 - val_loss: 1.1509 - val_mae: 1.1509\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4768 - mae: 1.4768 - val_loss: 1.1943 - val_mae: 1.1943\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.5572 - mae: 1.5572 - val_loss: 1.2682 - val_mae: 1.2682\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 1.2682 - mae: 1.2682\n",
      "Val score is 1.2681578397750854\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 38.5040 - mae: 38.5040 - val_loss: 38.1794 - val_mae: 38.1794\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 38.3703 - mae: 38.3703 - val_loss: 38.0330 - val_mae: 38.0330\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 38.2166 - mae: 38.2166 - val_loss: 37.8601 - val_mae: 37.8601\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 38.0385 - mae: 38.0385 - val_loss: 37.6574 - val_mae: 37.6574\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 37.8326 - mae: 37.8326 - val_loss: 37.4251 - val_mae: 37.4251\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 37.5966 - mae: 37.5966 - val_loss: 37.1621 - val_mae: 37.1621\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 37.3288 - mae: 37.3288 - val_loss: 36.8590 - val_mae: 36.8590\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 37.0278 - mae: 37.0278 - val_loss: 36.5374 - val_mae: 36.5374\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 36.6926 - mae: 36.6926 - val_loss: 36.1774 - val_mae: 36.1774\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 36.3225 - mae: 36.3225 - val_loss: 35.7879 - val_mae: 35.7879\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 35.9169 - mae: 35.9169 - val_loss: 35.3568 - val_mae: 35.3568\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 35.4751 - mae: 35.4751 - val_loss: 34.8844 - val_mae: 34.8844\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 34.9970 - mae: 34.9970 - val_loss: 34.3866 - val_mae: 34.3866\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 34.4820 - mae: 34.4820 - val_loss: 33.8543 - val_mae: 33.8543\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 33.9301 - mae: 33.9301 - val_loss: 33.2938 - val_mae: 33.2938\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 33.3409 - mae: 33.3409 - val_loss: 32.6828 - val_mae: 32.6828\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 32.7144 - mae: 32.7144 - val_loss: 32.0428 - val_mae: 32.0428\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 32.0505 - mae: 32.0505 - val_loss: 31.3634 - val_mae: 31.3634\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 31.3490 - mae: 31.3490 - val_loss: 30.6560 - val_mae: 30.6560\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 30.6101 - mae: 30.6101 - val_loss: 29.9204 - val_mae: 29.9204\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 29.8335 - mae: 29.8335 - val_loss: 29.1472 - val_mae: 29.1471\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 29.0195 - mae: 29.0195 - val_loss: 28.3364 - val_mae: 28.3364\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 28.1679 - mae: 28.1679 - val_loss: 27.4877 - val_mae: 27.4877\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 27.2788 - mae: 27.2788 - val_loss: 26.5936 - val_mae: 26.5936\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 26.3524 - mae: 26.3524 - val_loss: 25.6622 - val_mae: 25.6622\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 25.3904 - mae: 25.3904 - val_loss: 24.1422 - val_mae: 24.1422\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 24.3977 - mae: 24.3977 - val_loss: 22.7225 - val_mae: 22.7225\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 23.3602 - mae: 23.3602 - val_loss: 21.5132 - val_mae: 21.5132\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 22.2870 - mae: 22.2870 - val_loss: 20.4661 - val_mae: 20.4661\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 21.1752 - mae: 21.1752 - val_loss: 19.4160 - val_mae: 19.4160\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 20.0260 - mae: 20.0260 - val_loss: 18.3420 - val_mae: 18.3420\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 18.8603 - mae: 18.8603 - val_loss: 17.0965 - val_mae: 17.0965\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 17.6242 - mae: 17.6242 - val_loss: 15.6665 - val_mae: 15.6665\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 16.3763 - mae: 16.3763 - val_loss: 14.2415 - val_mae: 14.2415\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 15.0914 - mae: 15.0914 - val_loss: 12.5443 - val_mae: 12.5443\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 13.7743 - mae: 13.7743 - val_loss: 11.4510 - val_mae: 11.4510\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 12.4333 - mae: 12.4333 - val_loss: 10.4084 - val_mae: 10.4084\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 11.0833 - mae: 11.0833 - val_loss: 9.2954 - val_mae: 9.2954\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 9.6613 - mae: 9.6613 - val_loss: 8.0804 - val_mae: 8.0804\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 8.2585 - mae: 8.2585 - val_loss: 6.4248 - val_mae: 6.4248\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 6.8742 - mae: 6.8742 - val_loss: 4.8795 - val_mae: 4.8795\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 5.5628 - mae: 5.5628 - val_loss: 3.8607 - val_mae: 3.8607\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.1479 - mae: 4.1479 - val_loss: 2.9426 - val_mae: 2.9426\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 3.2637 - mae: 3.2637 - val_loss: 2.2279 - val_mae: 2.2279\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 2.5489 - mae: 2.5489 - val_loss: 1.8817 - val_mae: 1.8817\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.2536 - mae: 2.2536 - val_loss: 1.7555 - val_mae: 1.7555\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.8739 - mae: 1.8739 - val_loss: 1.7285 - val_mae: 1.7285\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.7139 - mae: 1.7139 - val_loss: 1.4648 - val_mae: 1.4648\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.6006 - mae: 1.6006 - val_loss: 1.4005 - val_mae: 1.4005\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.6972 - mae: 1.6972 - val_loss: 1.3638 - val_mae: 1.3638\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.5338 - mae: 1.5338 - val_loss: 1.3982 - val_mae: 1.3982\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4459 - mae: 1.4459 - val_loss: 1.1448 - val_mae: 1.1448\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.5454 - mae: 1.5454 - val_loss: 1.1386 - val_mae: 1.1386\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.4758 - mae: 1.4758 - val_loss: 1.2010 - val_mae: 1.2010\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.4680 - mae: 1.4680 - val_loss: 1.0078 - val_mae: 1.0078\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.5127 - mae: 1.5127 - val_loss: 0.9897 - val_mae: 0.9897\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 1.3328 - mae: 1.3328 - val_loss: 0.8702 - val_mae: 0.8702\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.3654 - mae: 1.3654 - val_loss: 0.8424 - val_mae: 0.8424\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.2360 - mae: 1.2360 - val_loss: 0.9149 - val_mae: 0.9149\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.4385 - mae: 1.4385 - val_loss: 0.9400 - val_mae: 0.9400\n",
      "365/365 [==============================] - 0s 37us/sample - loss: 0.9400 - mae: 0.9400\n",
      "Val score is 0.9399617910385132\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 2.1873 - mae: 2.1873 - val_loss: 1.5856 - val_mae: 1.5856\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.8122 - mae: 1.8122 - val_loss: 1.5741 - val_mae: 1.5741\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.7423 - mae: 1.7423 - val_loss: 1.5727 - val_mae: 1.5727\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.6800 - mae: 1.6800 - val_loss: 1.5640 - val_mae: 1.5640\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 126us/sample - loss: 1.6208 - mae: 1.6208 - val_loss: 1.5625 - val_mae: 1.5625\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5767 - mae: 1.5767 - val_loss: 1.5534 - val_mae: 1.5534\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.5604 - mae: 1.5604 - val_loss: 1.5423 - val_mae: 1.5423\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5167 - mae: 1.5167 - val_loss: 1.5341 - val_mae: 1.5341\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.5088 - mae: 1.5088 - val_loss: 1.5224 - val_mae: 1.5224\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.4844 - mae: 1.4844 - val_loss: 1.5078 - val_mae: 1.5078\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.4598 - mae: 1.4598 - val_loss: 1.4935 - val_mae: 1.4935\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.4649 - mae: 1.4649 - val_loss: 1.4786 - val_mae: 1.4786\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.4735 - mae: 1.4735 - val_loss: 1.4630 - val_mae: 1.4630\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.4212 - mae: 1.4212 - val_loss: 1.4482 - val_mae: 1.4482\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.4017 - mae: 1.4017 - val_loss: 1.4334 - val_mae: 1.4334\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.3932 - mae: 1.3932 - val_loss: 1.4139 - val_mae: 1.4139\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.3918 - mae: 1.3918 - val_loss: 1.3952 - val_mae: 1.3952\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 137us/sample - loss: 1.3679 - mae: 1.3679 - val_loss: 1.3709 - val_mae: 1.3709\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 126us/sample - loss: 1.3473 - mae: 1.3473 - val_loss: 1.3557 - val_mae: 1.3557\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.3371 - mae: 1.3371 - val_loss: 1.3341 - val_mae: 1.3341\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.3271 - mae: 1.3271 - val_loss: 1.3258 - val_mae: 1.3258\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.3132 - mae: 1.3132 - val_loss: 1.3087 - val_mae: 1.3087\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.2981 - mae: 1.2981 - val_loss: 1.2936 - val_mae: 1.2936\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 1.2837 - mae: 1.2837 - val_loss: 1.2705 - val_mae: 1.2705\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.2716 - mae: 1.2716 - val_loss: 1.2561 - val_mae: 1.2561\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1.2503 - mae: 1.2503 - val_loss: 1.2366 - val_mae: 1.2366\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.2461 - mae: 1.2461 - val_loss: 1.2164 - val_mae: 1.2164\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 1.2255 - mae: 1.2255 - val_loss: 1.2023 - val_mae: 1.2023\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.2556 - mae: 1.2556 - val_loss: 1.1887 - val_mae: 1.1887\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.1907 - mae: 1.1907 - val_loss: 1.1664 - val_mae: 1.1664\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.1909 - mae: 1.1909 - val_loss: 1.1634 - val_mae: 1.1634\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.1879 - mae: 1.1879 - val_loss: 1.1312 - val_mae: 1.1312\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.1705 - mae: 1.1705 - val_loss: 1.1177 - val_mae: 1.1177\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.1426 - mae: 1.1426 - val_loss: 1.1068 - val_mae: 1.1068\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.1680 - mae: 1.1680 - val_loss: 1.0891 - val_mae: 1.0891\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.1254 - mae: 1.1254 - val_loss: 1.0869 - val_mae: 1.0869\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.1642 - mae: 1.1642 - val_loss: 1.0648 - val_mae: 1.0648\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.0932 - mae: 1.0932 - val_loss: 1.0583 - val_mae: 1.0583\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.0991 - mae: 1.0991 - val_loss: 1.0385 - val_mae: 1.0385\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.0860 - mae: 1.0860 - val_loss: 1.0211 - val_mae: 1.0211\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.0796 - mae: 1.0796 - val_loss: 1.0328 - val_mae: 1.0328\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.0783 - mae: 1.0783 - val_loss: 0.9943 - val_mae: 0.9943\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.0454 - mae: 1.0454 - val_loss: 0.9894 - val_mae: 0.9894\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.0326 - mae: 1.0326 - val_loss: 0.9893 - val_mae: 0.9893\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.0386 - mae: 1.0386 - val_loss: 0.9744 - val_mae: 0.9744\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.0319 - mae: 1.0319 - val_loss: 0.9711 - val_mae: 0.9711\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.0369 - mae: 1.0369 - val_loss: 0.9601 - val_mae: 0.9601\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.0951 - mae: 1.0951 - val_loss: 0.9495 - val_mae: 0.9495\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.0165 - mae: 1.0165 - val_loss: 0.9496 - val_mae: 0.9496\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 0.9936 - mae: 0.9936 - val_loss: 0.9386 - val_mae: 0.9386\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 0.9854 - mae: 0.9854 - val_loss: 0.9380 - val_mae: 0.9380\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 0.9903 - mae: 0.9903 - val_loss: 0.9302 - val_mae: 0.9302\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.9590 - mae: 0.9590 - val_loss: 0.8991 - val_mae: 0.8991\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 0.9251 - mae: 0.9251 - val_loss: 0.8878 - val_mae: 0.8878\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.9855 - mae: 0.9855 - val_loss: 0.8681 - val_mae: 0.8681\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 1.0287 - mae: 1.0287 - val_loss: 0.8531 - val_mae: 0.8531\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 0.9199 - mae: 0.9199 - val_loss: 0.8225 - val_mae: 0.8225\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 0.9162 - mae: 0.9162 - val_loss: 0.8426 - val_mae: 0.8426\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 0.9384 - mae: 0.9384 - val_loss: 0.8337 - val_mae: 0.8337\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.8337 - mae: 0.8337\n",
      "Val score is 0.8337215185165405\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 0.9631 - mae: 0.9631 - val_loss: 0.2694 - val_mae: 0.2694\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.6601 - mae: 0.6601 - val_loss: 0.2049 - val_mae: 0.2049\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 0.5116 - mae: 0.5116 - val_loss: 0.1974 - val_mae: 0.1974\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.4193 - mae: 0.4193 - val_loss: 0.1823 - val_mae: 0.1823\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 0.3921 - mae: 0.3921 - val_loss: 0.1648 - val_mae: 0.1648\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.3464 - mae: 0.3464 - val_loss: 0.1524 - val_mae: 0.1524\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.3337 - mae: 0.3337 - val_loss: 0.1498 - val_mae: 0.1498\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 170us/sample - loss: 0.3168 - mae: 0.3168 - val_loss: 0.1449 - val_mae: 0.1449\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 129us/sample - loss: 0.2817 - mae: 0.2817 - val_loss: 0.1422 - val_mae: 0.1422\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2815 - mae: 0.2815 - val_loss: 0.1381 - val_mae: 0.1381\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.2611 - mae: 0.2611 - val_loss: 0.1352 - val_mae: 0.1352\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2254 - mae: 0.2254 - val_loss: 0.1246 - val_mae: 0.1246\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 0.2527 - mae: 0.2527 - val_loss: 0.1258 - val_mae: 0.1258\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.2353 - mae: 0.2353 - val_loss: 0.1179 - val_mae: 0.1179\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.2254 - mae: 0.2254 - val_loss: 0.1270 - val_mae: 0.1270\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2086 - mae: 0.2086 - val_loss: 0.1217 - val_mae: 0.1217\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 0.1217 - mae: 0.1217\n",
      "Val score is 0.12166343629360199\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 2.3946 - mae: 2.3946 - val_loss: 1.8716 - val_mae: 1.8716\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.2454 - mae: 2.2454 - val_loss: 1.8057 - val_mae: 1.8057\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 2.1253 - mae: 2.1253 - val_loss: 1.7172 - val_mae: 1.7172\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 1.9867 - mae: 1.9867 - val_loss: 1.6135 - val_mae: 1.6135\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.8451 - mae: 1.8451 - val_loss: 1.4759 - val_mae: 1.4759\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.7120 - mae: 1.7120 - val_loss: 1.3346 - val_mae: 1.3346\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.5243 - mae: 1.5243 - val_loss: 1.2050 - val_mae: 1.2050\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.3520 - mae: 1.3520 - val_loss: 1.0468 - val_mae: 1.0468\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.1634 - mae: 1.1634 - val_loss: 0.8457 - val_mae: 0.8457\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.9703 - mae: 0.9703 - val_loss: 0.6697 - val_mae: 0.6697\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.8540 - mae: 0.8540 - val_loss: 0.5624 - val_mae: 0.5624\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 0.6952 - mae: 0.6952 - val_loss: 0.5017 - val_mae: 0.5017\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 0.6158 - mae: 0.6158 - val_loss: 0.3606 - val_mae: 0.3606\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 0.5422 - mae: 0.5422 - val_loss: 0.3688 - val_mae: 0.3688\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.4897 - mae: 0.4897 - val_loss: 0.3420 - val_mae: 0.3420\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 0.4607 - mae: 0.4607 - val_loss: 0.3247 - val_mae: 0.3247\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 0.4716 - mae: 0.4716 - val_loss: 0.3440 - val_mae: 0.3440\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.4586 - mae: 0.4586 - val_loss: 0.3012 - val_mae: 0.3012\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 0.4261 - mae: 0.4261 - val_loss: 0.3065 - val_mae: 0.3065\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 0.4239 - mae: 0.4239 - val_loss: 0.3100 - val_mae: 0.3100\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.3100 - mae: 0.3100\n",
      "Val score is 0.30995917320251465\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 1.4182 - mae: 1.4182 - val_loss: 0.5610 - val_mae: 0.5610\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.0301 - mae: 1.0301 - val_loss: 0.5812 - val_mae: 0.5812\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.8563 - mae: 0.8563 - val_loss: 0.5766 - val_mae: 0.5766\n",
      "365/365 [==============================] - 0s 37us/sample - loss: 0.5766 - mae: 0.5766\n",
      "Val score is 0.5765613317489624\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 34.4591 - mae: 34.4591 - val_loss: 33.9314 - val_mae: 33.9314\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 34.3202 - mae: 34.3202 - val_loss: 33.8692 - val_mae: 33.8692\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 34.1611 - mae: 34.1611 - val_loss: 33.7909 - val_mae: 33.7909\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 33.9776 - mae: 33.9776 - val_loss: 33.6897 - val_mae: 33.6897\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 33.7661 - mae: 33.7661 - val_loss: 33.5572 - val_mae: 33.5572\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 33.5245 - mae: 33.5245 - val_loss: 33.3918 - val_mae: 33.3918\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 33.2508 - mae: 33.2508 - val_loss: 33.1931 - val_mae: 33.1931\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 32.9440 - mae: 32.9440 - val_loss: 32.9489 - val_mae: 32.9489\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 32.6029 - mae: 32.6029 - val_loss: 32.6683 - val_mae: 32.6683\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 32.2268 - mae: 32.2268 - val_loss: 32.3482 - val_mae: 32.3482\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 31.8153 - mae: 31.8153 - val_loss: 31.9784 - val_mae: 31.9784\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 31.3677 - mae: 31.3677 - val_loss: 31.5710 - val_mae: 31.5710\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 30.8837 - mae: 30.8837 - val_loss: 31.1037 - val_mae: 31.1037\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 30.3630 - mae: 30.3630 - val_loss: 30.5891 - val_mae: 30.5891\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 29.8054 - mae: 29.8054 - val_loss: 30.0322 - val_mae: 30.0322\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 29.2107 - mae: 29.2107 - val_loss: 29.4418 - val_mae: 29.4417\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 28.5787 - mae: 28.5787 - val_loss: 28.7975 - val_mae: 28.7975\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 27.9094 - mae: 27.9094 - val_loss: 28.1109 - val_mae: 28.1109\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 27.2028 - mae: 27.2028 - val_loss: 27.3772 - val_mae: 27.3772\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 106us/sample - loss: 26.4587 - mae: 26.4587 - val_loss: 26.6241 - val_mae: 26.6241\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 25.6772 - mae: 25.6772 - val_loss: 25.8667 - val_mae: 25.8667\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 24.8582 - mae: 24.8582 - val_loss: 25.0328 - val_mae: 25.0328\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 24.0019 - mae: 24.0019 - val_loss: 24.1493 - val_mae: 24.1493\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 23.1082 - mae: 23.1082 - val_loss: 23.2198 - val_mae: 23.2198\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 22.1772 - mae: 22.1772 - val_loss: 22.2374 - val_mae: 22.2374\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 21.2090 - mae: 21.2090 - val_loss: 21.2309 - val_mae: 21.2309\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 20.2036 - mae: 20.2036 - val_loss: 20.1663 - val_mae: 20.1663\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 19.1670 - mae: 19.1670 - val_loss: 18.8565 - val_mae: 18.8565\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 18.0875 - mae: 18.0875 - val_loss: 16.4032 - val_mae: 16.4032\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 16.9911 - mae: 16.9911 - val_loss: 14.3856 - val_mae: 14.3856\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 15.8370 - mae: 15.8370 - val_loss: 13.0527 - val_mae: 13.0527\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 14.6662 - mae: 14.6662 - val_loss: 11.8385 - val_mae: 11.8385\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 13.4792 - mae: 13.4792 - val_loss: 10.0727 - val_mae: 10.0727\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 12.2107 - mae: 12.2107 - val_loss: 8.7400 - val_mae: 8.7400\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 10.9475 - mae: 10.9475 - val_loss: 7.4455 - val_mae: 7.4455\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 9.7104 - mae: 9.7104 - val_loss: 6.4316 - val_mae: 6.4316\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 8.3505 - mae: 8.3505 - val_loss: 5.4157 - val_mae: 5.4157\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 7.1035 - mae: 7.1035 - val_loss: 4.3953 - val_mae: 4.3953\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 5.9451 - mae: 5.9451 - val_loss: 3.7553 - val_mae: 3.7553\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 131us/sample - loss: 5.0257 - mae: 5.0257 - val_loss: 3.2640 - val_mae: 3.2640\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.0904 - mae: 4.0904 - val_loss: 2.8276 - val_mae: 2.8276\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.3356 - mae: 3.3356 - val_loss: 2.6296 - val_mae: 2.6296\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 2.8036 - mae: 2.8036 - val_loss: 2.5038 - val_mae: 2.5038\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 2.5965 - mae: 2.5965 - val_loss: 2.4384 - val_mae: 2.4384\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 2.5510 - mae: 2.5510 - val_loss: 2.5710 - val_mae: 2.5710\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.4337 - mae: 2.4337 - val_loss: 2.3759 - val_mae: 2.3759\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 2.2993 - mae: 2.2993 - val_loss: 2.2843 - val_mae: 2.2843\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.0619 - mae: 2.0619 - val_loss: 2.0923 - val_mae: 2.0923\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 2.1067 - mae: 2.1067 - val_loss: 1.9157 - val_mae: 1.9157\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 1.9353 - mae: 1.9353 - val_loss: 1.8757 - val_mae: 1.8757\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 2.0405 - mae: 2.0405 - val_loss: 1.7790 - val_mae: 1.7790\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.9407 - mae: 1.9407 - val_loss: 1.7220 - val_mae: 1.7220\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.6928 - mae: 1.6928 - val_loss: 1.6775 - val_mae: 1.6775\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.8123 - mae: 1.8123 - val_loss: 1.6451 - val_mae: 1.6451\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.8067 - mae: 1.8067 - val_loss: 1.5154 - val_mae: 1.5154\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.7541 - mae: 1.7541 - val_loss: 1.5437 - val_mae: 1.5437\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.7119 - mae: 1.7119 - val_loss: 1.3963 - val_mae: 1.3963\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.6782 - mae: 1.6782 - val_loss: 1.3356 - val_mae: 1.3356\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5820 - mae: 1.5820 - val_loss: 1.3189 - val_mae: 1.3189\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.6319 - mae: 1.6319 - val_loss: 1.3022 - val_mae: 1.3022\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.7258 - mae: 1.7258 - val_loss: 1.2938 - val_mae: 1.2938\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.5827 - mae: 1.5827 - val_loss: 1.2246 - val_mae: 1.2246\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.5433 - mae: 1.5433 - val_loss: 1.1034 - val_mae: 1.1034\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.4947 - mae: 1.4947 - val_loss: 1.0724 - val_mae: 1.0724\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 1.5631 - mae: 1.5631 - val_loss: 1.1446 - val_mae: 1.1446\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.6606 - mae: 1.6606 - val_loss: 1.0615 - val_mae: 1.0615\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.5455 - mae: 1.5455 - val_loss: 1.0489 - val_mae: 1.0489\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.5153 - mae: 1.5153 - val_loss: 1.0091 - val_mae: 1.0091\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4916 - mae: 1.4916 - val_loss: 1.0431 - val_mae: 1.0431\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.5799 - mae: 1.5799 - val_loss: 0.9862 - val_mae: 0.9862\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.3861 - mae: 1.3861 - val_loss: 1.0098 - val_mae: 1.0098\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.4053 - mae: 1.4053 - val_loss: 1.1004 - val_mae: 1.1004\n",
      "365/365 [==============================] - 0s 37us/sample - loss: 1.1004 - mae: 1.1004\n",
      "Val score is 1.10038423538208\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 38.9687 - mae: 38.9687 - val_loss: 39.1877 - val_mae: 39.1877\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 38.8485 - mae: 38.8485 - val_loss: 38.9894 - val_mae: 38.9894\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 38.7083 - mae: 38.7083 - val_loss: 38.7715 - val_mae: 38.7715\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 38.5434 - mae: 38.5434 - val_loss: 38.5260 - val_mae: 38.5260\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 38.3505 - mae: 38.3505 - val_loss: 38.2547 - val_mae: 38.2547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 38.1271 - mae: 38.1271 - val_loss: 37.9590 - val_mae: 37.9590\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 37.8714 - mae: 37.8714 - val_loss: 37.6303 - val_mae: 37.6303\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 37.5821 - mae: 37.5821 - val_loss: 37.2662 - val_mae: 37.2662\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 37.2580 - mae: 37.2580 - val_loss: 36.8728 - val_mae: 36.8728\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 36.8985 - mae: 36.8985 - val_loss: 36.4435 - val_mae: 36.4435\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 36.5029 - mae: 36.5029 - val_loss: 35.9914 - val_mae: 35.9914\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 36.0708 - mae: 36.0708 - val_loss: 35.4970 - val_mae: 35.4970\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 35.6017 - mae: 35.6017 - val_loss: 34.9715 - val_mae: 34.9715\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 35.0953 - mae: 35.0953 - val_loss: 34.4220 - val_mae: 34.4220\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 34.5515 - mae: 34.5515 - val_loss: 33.8405 - val_mae: 33.8405\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 33.9701 - mae: 33.9701 - val_loss: 33.2228 - val_mae: 33.2228\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 33.3509 - mae: 33.3509 - val_loss: 32.5706 - val_mae: 32.5706\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 32.6940 - mae: 32.6940 - val_loss: 31.8897 - val_mae: 31.8897\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 31.9992 - mae: 31.9992 - val_loss: 31.1582 - val_mae: 31.1582\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 31.2665 - mae: 31.2665 - val_loss: 30.4087 - val_mae: 30.4087\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 30.4959 - mae: 30.4959 - val_loss: 29.6135 - val_mae: 29.6135\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 29.6876 - mae: 29.6876 - val_loss: 28.7931 - val_mae: 28.7931\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 28.8414 - mae: 28.8414 - val_loss: 27.9339 - val_mae: 27.9339\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 27.9576 - mae: 27.9576 - val_loss: 27.0375 - val_mae: 27.0375\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 27.0361 - mae: 27.0361 - val_loss: 26.1115 - val_mae: 26.1115\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 26.0770 - mae: 26.0770 - val_loss: 25.1451 - val_mae: 25.1451\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 25.0805 - mae: 25.0805 - val_loss: 24.1430 - val_mae: 24.1430\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 24.0466 - mae: 24.0466 - val_loss: 23.1056 - val_mae: 23.1056\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 22.9755 - mae: 22.9755 - val_loss: 22.0339 - val_mae: 22.0339\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 21.8673 - mae: 21.8673 - val_loss: 20.9219 - val_mae: 20.9219\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 20.7220 - mae: 20.7220 - val_loss: 19.8000 - val_mae: 19.8000\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 19.5443 - mae: 19.5443 - val_loss: 18.0023 - val_mae: 18.0023\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 18.3269 - mae: 18.3269 - val_loss: 16.4606 - val_mae: 16.4606\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 17.0776 - mae: 17.0776 - val_loss: 15.0383 - val_mae: 15.0383\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 15.7855 - mae: 15.7855 - val_loss: 13.4524 - val_mae: 13.4524\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 14.4596 - mae: 14.4596 - val_loss: 12.1744 - val_mae: 12.1744\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 13.0954 - mae: 13.0954 - val_loss: 10.9347 - val_mae: 10.9347\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 11.7291 - mae: 11.7291 - val_loss: 9.7996 - val_mae: 9.7996\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 10.3265 - mae: 10.3265 - val_loss: 8.1995 - val_mae: 8.1995\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 8.8869 - mae: 8.8869 - val_loss: 6.6061 - val_mae: 6.6061\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 7.4590 - mae: 7.4590 - val_loss: 5.5090 - val_mae: 5.5090\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 6.1229 - mae: 6.1229 - val_loss: 4.5670 - val_mae: 4.5670\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.9388 - mae: 4.9388 - val_loss: 3.5237 - val_mae: 3.5237\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 3.8200 - mae: 3.8200 - val_loss: 3.0142 - val_mae: 3.0142\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.0366 - mae: 3.0366 - val_loss: 2.5476 - val_mae: 2.5476\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 2.3743 - mae: 2.3743 - val_loss: 2.1614 - val_mae: 2.1614\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 2.2266 - mae: 2.2266 - val_loss: 1.9103 - val_mae: 1.9103\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.8381 - mae: 1.8381 - val_loss: 1.6824 - val_mae: 1.6824\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.7198 - mae: 1.7198 - val_loss: 1.4702 - val_mae: 1.4702\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5582 - mae: 1.5582 - val_loss: 1.4683 - val_mae: 1.4683\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.6940 - mae: 1.6940 - val_loss: 1.3816 - val_mae: 1.3816\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.4670 - mae: 1.4670 - val_loss: 1.2053 - val_mae: 1.2053\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.4339 - mae: 1.4339 - val_loss: 1.2846 - val_mae: 1.2846\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.5762 - mae: 1.5762 - val_loss: 1.1316 - val_mae: 1.1316\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.3230 - mae: 1.3230 - val_loss: 1.1042 - val_mae: 1.1042\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.3102 - mae: 1.3102 - val_loss: 1.1167 - val_mae: 1.1167\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.3271 - mae: 1.3271 - val_loss: 1.0661 - val_mae: 1.0661\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.2671 - mae: 1.2671 - val_loss: 1.0705 - val_mae: 1.0705\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 1.2216 - mae: 1.2216 - val_loss: 1.0119 - val_mae: 1.0119\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.2509 - mae: 1.2509 - val_loss: 0.9601 - val_mae: 0.9601\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.2133 - mae: 1.2133 - val_loss: 0.9292 - val_mae: 0.9292\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.2163 - mae: 1.2163 - val_loss: 0.8908 - val_mae: 0.8908\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.2325 - mae: 1.2325 - val_loss: 0.9333 - val_mae: 0.9333\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.2277 - mae: 1.2277 - val_loss: 0.8213 - val_mae: 0.8213\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.3335 - mae: 1.3335 - val_loss: 0.8714 - val_mae: 0.8714\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.2547 - mae: 1.2547 - val_loss: 0.9584 - val_mae: 0.9584\n",
      "365/365 [==============================] - 0s 37us/sample - loss: 0.9584 - mae: 0.9584\n",
      "Val score is 0.9583938121795654\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 102.8555 - mae: 102.8555 - val_loss: 102.4612 - val_mae: 102.4612\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 102.7114 - mae: 102.7114 - val_loss: 102.3476 - val_mae: 102.3476\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 102.5473 - mae: 102.5473 - val_loss: 102.2053 - val_mae: 102.2053\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 102.3588 - mae: 102.3588 - val_loss: 102.0285 - val_mae: 102.0285\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 102.1425 - mae: 102.1425 - val_loss: 101.8145 - val_mae: 101.8145\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 101.8961 - mae: 101.8961 - val_loss: 101.5688 - val_mae: 101.5688\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 101.6180 - mae: 101.6180 - val_loss: 101.2837 - val_mae: 101.2837\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 101.3068 - mae: 101.3068 - val_loss: 100.9583 - val_mae: 100.9583\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 100.9615 - mae: 100.9615 - val_loss: 100.6000 - val_mae: 100.6000\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 100.5815 - mae: 100.5815 - val_loss: 100.2065 - val_mae: 100.2066\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 100.1662 - mae: 100.1662 - val_loss: 99.7784 - val_mae: 99.7784\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 99.7150 - mae: 99.7150 - val_loss: 99.3118 - val_mae: 99.3118\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 99.2277 - mae: 99.2277 - val_loss: 98.8123 - val_mae: 98.8123\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 98.7037 - mae: 98.7038 - val_loss: 98.2745 - val_mae: 98.2745\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 98.1431 - mae: 98.1431 - val_loss: 97.6997 - val_mae: 97.6997\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 97.5455 - mae: 97.5455 - val_loss: 97.0803 - val_mae: 97.0803\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 96.9108 - mae: 96.9108 - val_loss: 96.4308 - val_mae: 96.4308\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 96.2389 - mae: 96.2389 - val_loss: 95.7510 - val_mae: 95.7510\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 122us/sample - loss: 95.5298 - mae: 95.5298 - val_loss: 95.0354 - val_mae: 95.0354\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 94.7833 - mae: 94.7833 - val_loss: 94.2814 - val_mae: 94.2814\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 93.9995 - mae: 93.9995 - val_loss: 93.4811 - val_mae: 93.4811\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 93.1784 - mae: 93.1784 - val_loss: 92.6501 - val_mae: 92.6501\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 92.3200 - mae: 92.3200 - val_loss: 91.7762 - val_mae: 91.7762\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 91.4244 - mae: 91.4244 - val_loss: 90.8712 - val_mae: 90.8712\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 90.4915 - mae: 90.4915 - val_loss: 89.9269 - val_mae: 89.9269\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 89.5215 - mae: 89.5215 - val_loss: 88.9552 - val_mae: 88.9552\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 88.5144 - mae: 88.5144 - val_loss: 87.9854 - val_mae: 87.9854\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 87.4703 - mae: 87.4703 - val_loss: 86.9605 - val_mae: 86.9604\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 86.3893 - mae: 86.3893 - val_loss: 85.8494 - val_mae: 85.8494\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 85.2714 - mae: 85.2714 - val_loss: 84.7079 - val_mae: 84.7079\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 84.1169 - mae: 84.1169 - val_loss: 83.5398 - val_mae: 83.5398\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 82.9257 - mae: 82.9257 - val_loss: 82.3271 - val_mae: 82.3271\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 81.6981 - mae: 81.6981 - val_loss: 81.1013 - val_mae: 81.1013\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 80.4341 - mae: 80.4341 - val_loss: 79.8131 - val_mae: 79.8131\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 79.1338 - mae: 79.1338 - val_loss: 78.4808 - val_mae: 78.4808\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 77.7974 - mae: 77.7974 - val_loss: 77.1367 - val_mae: 77.1367\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 76.4250 - mae: 76.4250 - val_loss: 75.7274 - val_mae: 75.7274\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 75.0168 - mae: 75.0168 - val_loss: 74.2631 - val_mae: 74.2631\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 73.5728 - mae: 73.5728 - val_loss: 72.7977 - val_mae: 72.7977\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 72.0931 - mae: 72.0931 - val_loss: 71.3275 - val_mae: 71.3275\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 70.5780 - mae: 70.5780 - val_loss: 69.7980 - val_mae: 69.7980\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 69.0276 - mae: 69.0276 - val_loss: 68.1581 - val_mae: 68.1581\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 126us/sample - loss: 67.4420 - mae: 67.4419 - val_loss: 66.5304 - val_mae: 66.5304\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 65.8212 - mae: 65.8212 - val_loss: 64.8745 - val_mae: 64.8745\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 64.1656 - mae: 64.1656 - val_loss: 63.1462 - val_mae: 63.1462\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 62.4752 - mae: 62.4752 - val_loss: 61.4076 - val_mae: 61.4076\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 60.7501 - mae: 60.7501 - val_loss: 59.6786 - val_mae: 59.6786\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 58.9905 - mae: 58.9905 - val_loss: 57.9206 - val_mae: 57.9206\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 57.1965 - mae: 57.1965 - val_loss: 56.1083 - val_mae: 56.1083\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 55.3683 - mae: 55.3683 - val_loss: 54.3066 - val_mae: 54.3066\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 53.5060 - mae: 53.5060 - val_loss: 52.4289 - val_mae: 52.4289\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 51.6098 - mae: 51.6097 - val_loss: 50.6027 - val_mae: 50.6027\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 49.6797 - mae: 49.6797 - val_loss: 48.6480 - val_mae: 48.6480\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 111us/sample - loss: 47.7159 - mae: 47.7159 - val_loss: 46.6422 - val_mae: 46.6422\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 45.7186 - mae: 45.7186 - val_loss: 44.6054 - val_mae: 44.6054\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 43.6879 - mae: 43.6879 - val_loss: 42.5493 - val_mae: 42.5493\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 41.6240 - mae: 41.6240 - val_loss: 40.5386 - val_mae: 40.5386\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 39.5269 - mae: 39.5269 - val_loss: 38.4430 - val_mae: 38.4430\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 157us/sample - loss: 37.3968 - mae: 37.3968 - val_loss: 36.3257 - val_mae: 36.3257\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 35.2338 - mae: 35.2338 - val_loss: 34.1709 - val_mae: 34.1709\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 33.0382 - mae: 33.0382 - val_loss: 31.9481 - val_mae: 31.9481\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 30.8099 - mae: 30.8099 - val_loss: 29.7261 - val_mae: 29.7261\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 28.5492 - mae: 28.5492 - val_loss: 27.4338 - val_mae: 27.4338\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 26.2562 - mae: 26.2562 - val_loss: 25.0887 - val_mae: 25.0887\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 23.9539 - mae: 23.9539 - val_loss: 22.6251 - val_mae: 22.6251\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 21.6455 - mae: 21.6455 - val_loss: 19.3866 - val_mae: 19.3866\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 122us/sample - loss: 19.3346 - mae: 19.3346 - val_loss: 16.1749 - val_mae: 16.1749\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 17.0175 - mae: 17.0175 - val_loss: 13.3242 - val_mae: 13.3242\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 14.6411 - mae: 14.6411 - val_loss: 10.7054 - val_mae: 10.7054\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 12.4054 - mae: 12.4054 - val_loss: 8.3943 - val_mae: 8.3943\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 9.9548 - mae: 9.9548 - val_loss: 6.8093 - val_mae: 6.8093\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 7.9602 - mae: 7.9602 - val_loss: 5.5473 - val_mae: 5.5473\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 6.2071 - mae: 6.2071 - val_loss: 4.2573 - val_mae: 4.2573\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.9877 - mae: 4.9877 - val_loss: 3.9359 - val_mae: 3.9359\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.1416 - mae: 4.1416 - val_loss: 3.5542 - val_mae: 3.5542\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.8136 - mae: 3.8136 - val_loss: 3.5047 - val_mae: 3.5047\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 3.7226 - mae: 3.7226 - val_loss: 2.9838 - val_mae: 2.9838\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.3449 - mae: 3.3449 - val_loss: 2.8929 - val_mae: 2.8930\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 3.0418 - mae: 3.0418 - val_loss: 2.7035 - val_mae: 2.7035\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.0297 - mae: 3.0297 - val_loss: 2.6043 - val_mae: 2.6043\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 2.7312 - mae: 2.7312 - val_loss: 2.3262 - val_mae: 2.3262\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 2.5883 - mae: 2.5883 - val_loss: 2.3980 - val_mae: 2.3980\n",
      "Epoch 83/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 2.5702 - mae: 2.5702 - val_loss: 2.0984 - val_mae: 2.0984\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.5646 - mae: 2.5646 - val_loss: 2.3221 - val_mae: 2.3221\n",
      "Epoch 85/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 2.6817 - mae: 2.6817 - val_loss: 2.0410 - val_mae: 2.0410\n",
      "Epoch 86/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.3838 - mae: 2.3838 - val_loss: 2.1175 - val_mae: 2.1175\n",
      "Epoch 87/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 2.5617 - mae: 2.5617 - val_loss: 1.9422 - val_mae: 1.9422\n",
      "Epoch 88/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 2.4084 - mae: 2.4084 - val_loss: 1.8408 - val_mae: 1.8408\n",
      "Epoch 89/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.3883 - mae: 2.3883 - val_loss: 2.0634 - val_mae: 2.0634\n",
      "Epoch 90/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 2.4217 - mae: 2.4217 - val_loss: 1.9358 - val_mae: 1.9358\n",
      "365/365 [==============================] - 0s 34us/sample - loss: 1.9358 - mae: 1.9358\n",
      "Val score is 1.93581223487854\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 99.5679 - mae: 99.5679 - val_loss: 99.3634 - val_mae: 99.3634\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 99.4458 - mae: 99.4458 - val_loss: 99.2846 - val_mae: 99.2846\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 99.3036 - mae: 99.3036 - val_loss: 99.1791 - val_mae: 99.1791\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 99.1367 - mae: 99.1367 - val_loss: 99.0478 - val_mae: 99.0478\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 98.9416 - mae: 98.9417 - val_loss: 98.8838 - val_mae: 98.8838\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 98.7159 - mae: 98.7159 - val_loss: 98.6848 - val_mae: 98.6848\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 98.4575 - mae: 98.4575 - val_loss: 98.4512 - val_mae: 98.4512\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 98.1654 - mae: 98.1654 - val_loss: 98.1749 - val_mae: 98.1749\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 97.8384 - mae: 97.8384 - val_loss: 97.8595 - val_mae: 97.8595\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 97.4758 - mae: 97.4758 - val_loss: 97.5053 - val_mae: 97.5053\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 97.0770 - mae: 97.0770 - val_loss: 97.1067 - val_mae: 97.1068\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 96.6415 - mae: 96.6415 - val_loss: 96.6752 - val_mae: 96.6751\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 96.1690 - mae: 96.1690 - val_loss: 96.2066 - val_mae: 96.2066\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 95.6593 - mae: 95.6593 - val_loss: 95.6861 - val_mae: 95.6861\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 95.1120 - mae: 95.1120 - val_loss: 95.1134 - val_mae: 95.1134\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 94.5271 - mae: 94.5272 - val_loss: 94.5089 - val_mae: 94.5089\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 93.9046 - mae: 93.9046 - val_loss: 93.8587 - val_mae: 93.8587\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 93.2442 - mae: 93.2441 - val_loss: 93.1700 - val_mae: 93.1700\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 92.5459 - mae: 92.5459 - val_loss: 92.4263 - val_mae: 92.4263\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 91.8099 - mae: 91.8099 - val_loss: 91.6654 - val_mae: 91.6654\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 91.0360 - mae: 91.0360 - val_loss: 90.8659 - val_mae: 90.8659\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 90.2243 - mae: 90.2243 - val_loss: 90.0108 - val_mae: 90.0108\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 89.3749 - mae: 89.3749 - val_loss: 89.1258 - val_mae: 89.1258\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 88.4879 - mae: 88.4879 - val_loss: 88.2020 - val_mae: 88.2020\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 87.5632 - mae: 87.5632 - val_loss: 87.2262 - val_mae: 87.2262\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 86.6010 - mae: 86.6010 - val_loss: 86.2308 - val_mae: 86.2308\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 85.6015 - mae: 85.6015 - val_loss: 85.1942 - val_mae: 85.1942\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 84.5646 - mae: 84.5646 - val_loss: 84.1312 - val_mae: 84.1312\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 83.4905 - mae: 83.4905 - val_loss: 83.0621 - val_mae: 83.0621\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 82.3794 - mae: 82.3794 - val_loss: 81.9244 - val_mae: 81.9244\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 81.2312 - mae: 81.2312 - val_loss: 80.7651 - val_mae: 80.7651\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 80.0463 - mae: 80.0463 - val_loss: 79.5407 - val_mae: 79.5407\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 78.8246 - mae: 78.8246 - val_loss: 78.2875 - val_mae: 78.2875\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 77.5664 - mae: 77.5664 - val_loss: 76.9588 - val_mae: 76.9588\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 76.2718 - mae: 76.2718 - val_loss: 75.6160 - val_mae: 75.6160\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 74.9408 - mae: 74.9408 - val_loss: 74.2516 - val_mae: 74.2516\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 73.5737 - mae: 73.5737 - val_loss: 72.8819 - val_mae: 72.8819\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 72.1705 - mae: 72.1705 - val_loss: 71.4191 - val_mae: 71.4191\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 70.7315 - mae: 70.7315 - val_loss: 69.9547 - val_mae: 69.9547\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 69.2567 - mae: 69.2567 - val_loss: 68.4144 - val_mae: 68.4144\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 67.7463 - mae: 67.7464 - val_loss: 66.9175 - val_mae: 66.9175\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 66.2005 - mae: 66.2005 - val_loss: 65.3446 - val_mae: 65.3446\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 64.6194 - mae: 64.6194 - val_loss: 63.7762 - val_mae: 63.7762\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 63.0031 - mae: 63.0031 - val_loss: 62.1990 - val_mae: 62.1990\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 61.3518 - mae: 61.3518 - val_loss: 60.5313 - val_mae: 60.5313\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 59.6656 - mae: 59.6656 - val_loss: 58.7772 - val_mae: 58.7772\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 57.9447 - mae: 57.9447 - val_loss: 57.0149 - val_mae: 57.0149\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 56.1892 - mae: 56.1892 - val_loss: 55.1795 - val_mae: 55.1795\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 54.3993 - mae: 54.3993 - val_loss: 53.3800 - val_mae: 53.3800\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 52.5750 - mae: 52.5750 - val_loss: 51.5235 - val_mae: 51.5235\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 50.7167 - mae: 50.7167 - val_loss: 49.6634 - val_mae: 49.6634\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 48.8243 - mae: 48.8243 - val_loss: 47.7023 - val_mae: 47.7024\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 46.8980 - mae: 46.8980 - val_loss: 45.6961 - val_mae: 45.6961\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 44.9380 - mae: 44.9380 - val_loss: 43.7110 - val_mae: 43.7110\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 42.9445 - mae: 42.9445 - val_loss: 41.7028 - val_mae: 41.7028\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 40.9175 - mae: 40.9175 - val_loss: 39.7506 - val_mae: 39.7506\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 38.8572 - mae: 38.8572 - val_loss: 37.8069 - val_mae: 37.8069\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 36.7637 - mae: 36.7637 - val_loss: 35.7790 - val_mae: 35.7790\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 34.6373 - mae: 34.6373 - val_loss: 33.6638 - val_mae: 33.6638\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 32.4779 - mae: 32.4779 - val_loss: 31.5467 - val_mae: 31.5467\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 30.2858 - mae: 30.2858 - val_loss: 29.3511 - val_mae: 29.3511\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 28.0611 - mae: 28.0611 - val_loss: 27.0501 - val_mae: 27.0501\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 25.8039 - mae: 25.8039 - val_loss: 24.7542 - val_mae: 24.7542\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 23.5234 - mae: 23.5234 - val_loss: 21.6123 - val_mae: 21.6123\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 21.2817 - mae: 21.2817 - val_loss: 18.3136 - val_mae: 18.3136\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 18.9528 - mae: 18.9528 - val_loss: 14.8683 - val_mae: 14.8683\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 16.6180 - mae: 16.6180 - val_loss: 12.0665 - val_mae: 12.0665\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 14.2915 - mae: 14.2915 - val_loss: 10.3604 - val_mae: 10.3604\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 12.0699 - mae: 12.0699 - val_loss: 8.1056 - val_mae: 8.1056\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 9.7665 - mae: 9.7665 - val_loss: 6.3048 - val_mae: 6.3048\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 7.4395 - mae: 7.4395 - val_loss: 4.8861 - val_mae: 4.8861\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 5.5951 - mae: 5.5951 - val_loss: 3.4319 - val_mae: 3.4319\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.8589 - mae: 3.8589 - val_loss: 2.8885 - val_mae: 2.8885\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.9627 - mae: 2.9627 - val_loss: 2.9592 - val_mae: 2.9592\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.7583 - mae: 2.7583 - val_loss: 2.7791 - val_mae: 2.7791\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 2.3595 - mae: 2.3595 - val_loss: 2.3652 - val_mae: 2.3652\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.1832 - mae: 2.1832 - val_loss: 2.2472 - val_mae: 2.2472\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 2.1549 - mae: 2.1549 - val_loss: 2.0963 - val_mae: 2.0963\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 107us/sample - loss: 2.0722 - mae: 2.0722 - val_loss: 1.9757 - val_mae: 1.9757\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 105us/sample - loss: 1.6982 - mae: 1.6982 - val_loss: 1.6620 - val_mae: 1.6620\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.8530 - mae: 1.8530 - val_loss: 1.7556 - val_mae: 1.7556\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.8064 - mae: 1.8064 - val_loss: 1.7370 - val_mae: 1.7370\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 1.7370 - mae: 1.7370\n",
      "Val score is 1.7369580268859863\n",
      "DataFrame: bands imput_method :MLP model :KNN score:  0.57\n",
      "DataFrame: bands imput_method :MLP model :XGB score:  0.55\n",
      "DataFrame: bands imput_method :MLP model :MLP score:  0.53\n",
      "DataFrame: cleveland imput_method :LOCF model :KNN score:  0.52\n",
      "DataFrame: cleveland imput_method :LOCF model :XGB score:  0.56\n",
      "DataFrame: cleveland imput_method :LOCF model :MLP score:  0.53\n",
      "DataFrame: cleveland imput_method :mean_mode model :KNN score:  0.52\n",
      "DataFrame: cleveland imput_method :mean_mode model :XGB score:  0.57\n",
      "DataFrame: cleveland imput_method :mean_mode model :MLP score:  0.53\n",
      "DataFrame: cleveland imput_method :knn model :KNN score:  0.52\n",
      "DataFrame: cleveland imput_method :knn model :XGB score:  0.57\n",
      "DataFrame: cleveland imput_method :knn model :MLP score:  0.54\n",
      "[0]\tvalidation_0-mae:0.71423\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.66212\n",
      "[2]\tvalidation_0-mae:0.63325\n",
      "[3]\tvalidation_0-mae:0.62462\n",
      "[4]\tvalidation_0-mae:0.63145\n",
      "[5]\tvalidation_0-mae:0.62051\n",
      "[6]\tvalidation_0-mae:0.62360\n",
      "[7]\tvalidation_0-mae:0.62632\n",
      "Stopping. Best iteration:\n",
      "[5]\tvalidation_0-mae:0.62051\n",
      "\n",
      "[0]\tvalidation_0-mae:3.05931\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:2.20889\n",
      "[2]\tvalidation_0-mae:1.74359\n",
      "[3]\tvalidation_0-mae:1.51404\n",
      "[4]\tvalidation_0-mae:1.41093\n",
      "[5]\tvalidation_0-mae:1.30091\n",
      "[6]\tvalidation_0-mae:1.21934\n",
      "[7]\tvalidation_0-mae:1.19841\n",
      "[8]\tvalidation_0-mae:1.17358\n",
      "[9]\tvalidation_0-mae:1.17752\n",
      "[10]\tvalidation_0-mae:1.16002\n",
      "[11]\tvalidation_0-mae:1.12882\n",
      "[12]\tvalidation_0-mae:1.11326\n",
      "[13]\tvalidation_0-mae:1.09899\n",
      "[14]\tvalidation_0-mae:1.10125\n",
      "[15]\tvalidation_0-mae:1.10171\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-mae:1.09899\n",
      "\n",
      "DataFrame: cleveland imput_method :trees model :KNN score:  0.52\n",
      "DataFrame: cleveland imput_method :trees model :XGB score:  0.57\n",
      "DataFrame: cleveland imput_method :trees model :MLP score:  0.54\n",
      "Train on 297 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "297/297 [==============================] - 1s 5ms/sample - loss: 1.2322 - mae: 1.2322 - val_loss: 0.6897 - val_mae: 0.6897\n",
      "Epoch 2/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.9681 - mae: 0.9681 - val_loss: 0.6500 - val_mae: 0.6500\n",
      "Epoch 3/100\n",
      "297/297 [==============================] - 0s 154us/sample - loss: 0.8236 - mae: 0.8236 - val_loss: 0.6490 - val_mae: 0.6490\n",
      "Epoch 4/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.7386 - mae: 0.7386 - val_loss: 0.6650 - val_mae: 0.6650\n",
      "Epoch 5/100\n",
      "297/297 [==============================] - 0s 148us/sample - loss: 0.6570 - mae: 0.6570 - val_loss: 0.6453 - val_mae: 0.6453\n",
      "Epoch 6/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.6096 - mae: 0.6096 - val_loss: 0.6291 - val_mae: 0.6291\n",
      "Epoch 7/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.5882 - mae: 0.5882 - val_loss: 0.6326 - val_mae: 0.6326\n",
      "Epoch 8/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.5882 - mae: 0.5882 - val_loss: 0.6271 - val_mae: 0.6271\n",
      "Epoch 9/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.5369 - mae: 0.5369 - val_loss: 0.6043 - val_mae: 0.6043\n",
      "Epoch 10/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.5184 - mae: 0.5184 - val_loss: 0.5833 - val_mae: 0.5833\n",
      "Epoch 11/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.5408 - mae: 0.5408 - val_loss: 0.5772 - val_mae: 0.5772\n",
      "Epoch 12/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.5145 - mae: 0.5145 - val_loss: 0.5622 - val_mae: 0.5622\n",
      "Epoch 13/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.5182 - mae: 0.5182 - val_loss: 0.5520 - val_mae: 0.5520\n",
      "Epoch 14/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.4841 - mae: 0.4841 - val_loss: 0.5421 - val_mae: 0.5421\n",
      "Epoch 15/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.5110 - mae: 0.5110 - val_loss: 0.5122 - val_mae: 0.5122\n",
      "Epoch 16/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.4796 - mae: 0.4796 - val_loss: 0.4973 - val_mae: 0.4973\n",
      "Epoch 17/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.4660 - mae: 0.4660 - val_loss: 0.4909 - val_mae: 0.4909\n",
      "Epoch 18/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.4659 - mae: 0.4659 - val_loss: 0.4687 - val_mae: 0.4687\n",
      "Epoch 19/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.4534 - mae: 0.4534 - val_loss: 0.4694 - val_mae: 0.4694\n",
      "Epoch 20/100\n",
      "297/297 [==============================] - 0s 154us/sample - loss: 0.4241 - mae: 0.4241 - val_loss: 0.4696 - val_mae: 0.4696\n",
      "297/297 [==============================] - 0s 57us/sample - loss: 0.4696 - mae: 0.4696\n",
      "Val score is 0.46957314014434814\n",
      "Train on 297 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "297/297 [==============================] - 1s 4ms/sample - loss: 4.6938 - mae: 4.6938 - val_loss: 4.4253 - val_mae: 4.4253\n",
      "Epoch 2/100\n",
      "297/297 [==============================] - 0s 175us/sample - loss: 4.5902 - mae: 4.5902 - val_loss: 4.3508 - val_mae: 4.3508\n",
      "Epoch 3/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 4.4843 - mae: 4.4843 - val_loss: 4.2706 - val_mae: 4.2706\n",
      "Epoch 4/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 4.3682 - mae: 4.3682 - val_loss: 4.1661 - val_mae: 4.1661\n",
      "Epoch 5/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 4.2370 - mae: 4.2370 - val_loss: 4.0223 - val_mae: 4.0223\n",
      "Epoch 6/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 4.0877 - mae: 4.0877 - val_loss: 3.8700 - val_mae: 3.8700\n",
      "Epoch 7/100\n",
      "297/297 [==============================] - 0s 182us/sample - loss: 3.9241 - mae: 3.9241 - val_loss: 3.6953 - val_mae: 3.6953\n",
      "Epoch 8/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 3.7316 - mae: 3.7316 - val_loss: 3.5012 - val_mae: 3.5012\n",
      "Epoch 9/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 3.5327 - mae: 3.5327 - val_loss: 3.2774 - val_mae: 3.2774\n",
      "Epoch 10/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 3.3127 - mae: 3.3127 - val_loss: 3.0304 - val_mae: 3.0304\n",
      "Epoch 11/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 3.1013 - mae: 3.1013 - val_loss: 2.8154 - val_mae: 2.8154\n",
      "Epoch 12/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 2.8531 - mae: 2.8531 - val_loss: 2.5440 - val_mae: 2.5440\n",
      "Epoch 13/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 2.5726 - mae: 2.5726 - val_loss: 2.2588 - val_mae: 2.2588\n",
      "Epoch 14/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 2.3183 - mae: 2.3183 - val_loss: 2.0017 - val_mae: 2.0017\n",
      "Epoch 15/100\n",
      "297/297 [==============================] - 0s 185us/sample - loss: 2.1402 - mae: 2.1402 - val_loss: 1.8084 - val_mae: 1.8084\n",
      "Epoch 16/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 1.8901 - mae: 1.8901 - val_loss: 1.6449 - val_mae: 1.6449\n",
      "Epoch 17/100\n",
      "297/297 [==============================] - 0s 182us/sample - loss: 1.6807 - mae: 1.6807 - val_loss: 1.4795 - val_mae: 1.4795\n",
      "Epoch 18/100\n",
      "297/297 [==============================] - 0s 195us/sample - loss: 1.5086 - mae: 1.5086 - val_loss: 1.2980 - val_mae: 1.2980\n",
      "Epoch 19/100\n",
      "297/297 [==============================] - 0s 188us/sample - loss: 1.3482 - mae: 1.3482 - val_loss: 1.1782 - val_mae: 1.1782\n",
      "Epoch 20/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 1.2214 - mae: 1.2214 - val_loss: 1.1039 - val_mae: 1.1039\n",
      "Epoch 21/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 1.1143 - mae: 1.1143 - val_loss: 1.0475 - val_mae: 1.0475\n",
      "Epoch 22/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 1.1009 - mae: 1.1009 - val_loss: 0.9776 - val_mae: 0.9776\n",
      "Epoch 23/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 1.0819 - mae: 1.0819 - val_loss: 0.9522 - val_mae: 0.9522\n",
      "Epoch 24/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.9622 - mae: 0.9622 - val_loss: 0.9311 - val_mae: 0.9311\n",
      "Epoch 25/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.9580 - mae: 0.9580 - val_loss: 0.8905 - val_mae: 0.8905\n",
      "Epoch 26/100\n",
      "297/297 [==============================] - 0s 175us/sample - loss: 0.9741 - mae: 0.9741 - val_loss: 0.9183 - val_mae: 0.9183\n",
      "Epoch 27/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.9112 - mae: 0.9112 - val_loss: 0.8145 - val_mae: 0.8145\n",
      "Epoch 28/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.9556 - mae: 0.9556 - val_loss: 0.7946 - val_mae: 0.7946\n",
      "Epoch 29/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.9262 - mae: 0.9262 - val_loss: 0.7865 - val_mae: 0.7865\n",
      "Epoch 30/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.8079 - mae: 0.8079 - val_loss: 0.7396 - val_mae: 0.7396\n",
      "Epoch 31/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.8254 - mae: 0.8254 - val_loss: 0.7227 - val_mae: 0.7227\n",
      "Epoch 32/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.8711 - mae: 0.8711 - val_loss: 0.7363 - val_mae: 0.7363\n",
      "Epoch 33/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.8425 - mae: 0.8425 - val_loss: 0.6976 - val_mae: 0.6976\n",
      "Epoch 34/100\n",
      "297/297 [==============================] - 0s 178us/sample - loss: 0.7234 - mae: 0.7234 - val_loss: 0.6892 - val_mae: 0.6892\n",
      "Epoch 35/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.7706 - mae: 0.7706 - val_loss: 0.6386 - val_mae: 0.6386\n",
      "Epoch 36/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.7452 - mae: 0.7452 - val_loss: 0.6383 - val_mae: 0.6383\n",
      "Epoch 37/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.6991 - mae: 0.6991 - val_loss: 0.6590 - val_mae: 0.6590\n",
      "Epoch 38/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 0.7851 - mae: 0.7851 - val_loss: 0.6209 - val_mae: 0.6209\n",
      "Epoch 39/100\n",
      "297/297 [==============================] - 0s 175us/sample - loss: 0.7339 - mae: 0.7339 - val_loss: 0.6022 - val_mae: 0.6022\n",
      "Epoch 40/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.7296 - mae: 0.7296 - val_loss: 0.5602 - val_mae: 0.5602\n",
      "Epoch 41/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.7462 - mae: 0.7462 - val_loss: 0.5725 - val_mae: 0.5725\n",
      "Epoch 42/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.7176 - mae: 0.7176 - val_loss: 0.5329 - val_mae: 0.5329\n",
      "Epoch 43/100\n",
      "297/297 [==============================] - 0s 168us/sample - loss: 0.7063 - mae: 0.7063 - val_loss: 0.5242 - val_mae: 0.5242\n",
      "Epoch 44/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.7048 - mae: 0.7048 - val_loss: 0.5187 - val_mae: 0.5187\n",
      "Epoch 45/100\n",
      "297/297 [==============================] - 0s 172us/sample - loss: 0.6936 - mae: 0.6936 - val_loss: 0.5207 - val_mae: 0.5207\n",
      "Epoch 46/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 0.7230 - mae: 0.7230 - val_loss: 0.5227 - val_mae: 0.5227\n",
      "297/297 [==============================] - 0s 49us/sample - loss: 0.5227 - mae: 0.5227\n",
      "Val score is 0.5227455496788025\n",
      "DataFrame: cleveland imput_method :MLP model :KNN score:  0.52\n",
      "DataFrame: cleveland imput_method :MLP model :XGB score:  0.57\n",
      "DataFrame: cleveland imput_method :MLP model :MLP score:  0.54\n",
      "DataFrame: dermatology imput_method :LOCF model :KNN score:  0.82\n",
      "DataFrame: dermatology imput_method :LOCF model :XGB score:  0.95\n",
      "DataFrame: dermatology imput_method :LOCF model :MLP score:  0.95\n",
      "DataFrame: dermatology imput_method :mean_mode model :KNN score:  0.82\n",
      "DataFrame: dermatology imput_method :mean_mode model :XGB score:  0.95\n",
      "DataFrame: dermatology imput_method :mean_mode model :MLP score:  0.96\n",
      "DataFrame: dermatology imput_method :knn model :KNN score:  0.83\n",
      "DataFrame: dermatology imput_method :knn model :XGB score:  0.95\n",
      "DataFrame: dermatology imput_method :knn model :MLP score:  0.98\n",
      "[0]\tvalidation_0-mae:25.66351\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:18.90507\n",
      "[2]\tvalidation_0-mae:15.18072\n",
      "[3]\tvalidation_0-mae:13.29571\n",
      "[4]\tvalidation_0-mae:12.13745\n",
      "[5]\tvalidation_0-mae:11.24534\n",
      "[6]\tvalidation_0-mae:10.73597\n",
      "[7]\tvalidation_0-mae:10.41240\n",
      "[8]\tvalidation_0-mae:9.99080\n",
      "[9]\tvalidation_0-mae:9.49224\n",
      "[10]\tvalidation_0-mae:9.10065\n",
      "[11]\tvalidation_0-mae:8.70210\n",
      "[12]\tvalidation_0-mae:8.33026\n",
      "[13]\tvalidation_0-mae:8.05300\n",
      "[14]\tvalidation_0-mae:7.75713\n",
      "[15]\tvalidation_0-mae:7.39550\n",
      "[16]\tvalidation_0-mae:7.09288\n",
      "[17]\tvalidation_0-mae:6.87692\n",
      "[18]\tvalidation_0-mae:6.64609\n",
      "[19]\tvalidation_0-mae:6.49813\n",
      "[20]\tvalidation_0-mae:6.33202\n",
      "[21]\tvalidation_0-mae:6.13893\n",
      "[22]\tvalidation_0-mae:5.87466\n",
      "[23]\tvalidation_0-mae:5.69739\n",
      "[24]\tvalidation_0-mae:5.60077\n",
      "[25]\tvalidation_0-mae:5.48350\n",
      "[26]\tvalidation_0-mae:5.36342\n",
      "[27]\tvalidation_0-mae:5.16226\n",
      "[28]\tvalidation_0-mae:5.06095\n",
      "[29]\tvalidation_0-mae:4.95141\n",
      "[30]\tvalidation_0-mae:4.78788\n",
      "[31]\tvalidation_0-mae:4.69748\n",
      "[32]\tvalidation_0-mae:4.55213\n",
      "[33]\tvalidation_0-mae:4.40470\n",
      "[34]\tvalidation_0-mae:4.27727\n",
      "[35]\tvalidation_0-mae:4.15762\n",
      "[36]\tvalidation_0-mae:4.02790\n",
      "[37]\tvalidation_0-mae:3.92176\n",
      "[38]\tvalidation_0-mae:3.80284\n",
      "[39]\tvalidation_0-mae:3.74624\n",
      "[40]\tvalidation_0-mae:3.67887\n",
      "[41]\tvalidation_0-mae:3.60164\n",
      "[42]\tvalidation_0-mae:3.57437\n",
      "[43]\tvalidation_0-mae:3.50276\n",
      "[44]\tvalidation_0-mae:3.38085\n",
      "[45]\tvalidation_0-mae:3.30257\n",
      "[46]\tvalidation_0-mae:3.26239\n",
      "[47]\tvalidation_0-mae:3.23065\n",
      "[48]\tvalidation_0-mae:3.14443\n",
      "[49]\tvalidation_0-mae:3.06497\n",
      "[50]\tvalidation_0-mae:3.03145\n",
      "[51]\tvalidation_0-mae:2.95995\n",
      "[52]\tvalidation_0-mae:2.91393\n",
      "[53]\tvalidation_0-mae:2.84847\n",
      "[54]\tvalidation_0-mae:2.80884\n",
      "[55]\tvalidation_0-mae:2.78333\n",
      "[56]\tvalidation_0-mae:2.74848\n",
      "[57]\tvalidation_0-mae:2.71526\n",
      "[58]\tvalidation_0-mae:2.69533\n",
      "[59]\tvalidation_0-mae:2.65576\n",
      "[60]\tvalidation_0-mae:2.61078\n",
      "[61]\tvalidation_0-mae:2.56539\n",
      "[62]\tvalidation_0-mae:2.52127\n",
      "[63]\tvalidation_0-mae:2.49159\n",
      "[64]\tvalidation_0-mae:2.46779\n",
      "[65]\tvalidation_0-mae:2.43162\n",
      "[66]\tvalidation_0-mae:2.37070\n",
      "[67]\tvalidation_0-mae:2.34639\n",
      "[68]\tvalidation_0-mae:2.32815\n",
      "[69]\tvalidation_0-mae:2.31199\n",
      "[70]\tvalidation_0-mae:2.28853\n",
      "[71]\tvalidation_0-mae:2.26811\n",
      "[72]\tvalidation_0-mae:2.24519\n",
      "[73]\tvalidation_0-mae:2.22466\n",
      "[74]\tvalidation_0-mae:2.21552\n",
      "[75]\tvalidation_0-mae:2.18768\n",
      "[76]\tvalidation_0-mae:2.16056\n",
      "[77]\tvalidation_0-mae:2.14905\n",
      "[78]\tvalidation_0-mae:2.13653\n",
      "[79]\tvalidation_0-mae:2.12941\n",
      "[80]\tvalidation_0-mae:2.10284\n",
      "[81]\tvalidation_0-mae:2.09811\n",
      "[82]\tvalidation_0-mae:2.06786\n",
      "[83]\tvalidation_0-mae:2.04134\n",
      "[84]\tvalidation_0-mae:2.01458\n",
      "[85]\tvalidation_0-mae:2.00276\n",
      "[86]\tvalidation_0-mae:1.99460\n",
      "[87]\tvalidation_0-mae:1.98580\n",
      "[88]\tvalidation_0-mae:1.98085\n",
      "[89]\tvalidation_0-mae:1.96885\n",
      "[90]\tvalidation_0-mae:1.95910\n",
      "[91]\tvalidation_0-mae:1.94899\n",
      "[92]\tvalidation_0-mae:1.93576\n",
      "[93]\tvalidation_0-mae:1.93046\n",
      "[94]\tvalidation_0-mae:1.92900\n",
      "[95]\tvalidation_0-mae:1.92051\n",
      "[96]\tvalidation_0-mae:1.91112\n",
      "[97]\tvalidation_0-mae:1.89279\n",
      "[98]\tvalidation_0-mae:1.88246\n",
      "[99]\tvalidation_0-mae:1.86159\n",
      "[100]\tvalidation_0-mae:1.85565\n",
      "[101]\tvalidation_0-mae:1.85542\n",
      "[102]\tvalidation_0-mae:1.85566\n",
      "[103]\tvalidation_0-mae:1.84181\n",
      "[104]\tvalidation_0-mae:1.84205\n",
      "[105]\tvalidation_0-mae:1.84186\n",
      "Stopping. Best iteration:\n",
      "[103]\tvalidation_0-mae:1.84181\n",
      "\n",
      "DataFrame: dermatology imput_method :trees model :KNN score:  0.83\n",
      "DataFrame: dermatology imput_method :trees model :XGB score:  0.95\n",
      "DataFrame: dermatology imput_method :trees model :MLP score:  0.95\n",
      "Train on 358 samples, validate on 358 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - 1s 3ms/sample - loss: 36.2532 - mae: 36.2532 - val_loss: 36.5314 - val_mae: 36.5314\n",
      "Epoch 2/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 36.1277 - mae: 36.1277 - val_loss: 36.3149 - val_mae: 36.3149\n",
      "Epoch 3/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 35.9907 - mae: 35.9907 - val_loss: 36.1203 - val_mae: 36.1203\n",
      "Epoch 4/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 35.8298 - mae: 35.8298 - val_loss: 35.9079 - val_mae: 35.9079\n",
      "Epoch 5/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 35.6411 - mae: 35.6411 - val_loss: 35.6665 - val_mae: 35.6665\n",
      "Epoch 6/100\n",
      "358/358 [==============================] - 0s 150us/sample - loss: 35.4221 - mae: 35.4221 - val_loss: 35.3910 - val_mae: 35.3910\n",
      "Epoch 7/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 35.1707 - mae: 35.1707 - val_loss: 35.0899 - val_mae: 35.0899\n",
      "Epoch 8/100\n",
      "358/358 [==============================] - 0s 140us/sample - loss: 34.8856 - mae: 34.8856 - val_loss: 34.7519 - val_mae: 34.7519\n",
      "Epoch 9/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 34.5657 - mae: 34.5657 - val_loss: 34.3827 - val_mae: 34.3827\n",
      "Epoch 10/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 34.2101 - mae: 34.2101 - val_loss: 33.9869 - val_mae: 33.9869\n",
      "Epoch 11/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 33.8183 - mae: 33.8183 - val_loss: 33.5438 - val_mae: 33.5438\n",
      "Epoch 12/100\n",
      "358/358 [==============================] - 0s 140us/sample - loss: 33.3898 - mae: 33.3898 - val_loss: 33.0670 - val_mae: 33.0670\n",
      "Epoch 13/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 32.9241 - mae: 32.9241 - val_loss: 32.5608 - val_mae: 32.5608\n",
      "Epoch 14/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 32.4210 - mae: 32.4210 - val_loss: 32.0322 - val_mae: 32.0322\n",
      "Epoch 15/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 31.8804 - mae: 31.8804 - val_loss: 31.4527 - val_mae: 31.4527\n",
      "Epoch 16/100\n",
      "358/358 [==============================] - 0s 139us/sample - loss: 31.3065 - mae: 31.3064 - val_loss: 30.5787 - val_mae: 30.5787\n",
      "Epoch 17/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 30.7169 - mae: 30.7169 - val_loss: 29.7577 - val_mae: 29.7577\n",
      "Epoch 18/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 30.0593 - mae: 30.0593 - val_loss: 28.9099 - val_mae: 28.9099\n",
      "Epoch 19/100\n",
      "358/358 [==============================] - 0s 155us/sample - loss: 29.3649 - mae: 29.3649 - val_loss: 28.1472 - val_mae: 28.1472\n",
      "Epoch 20/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 28.6294 - mae: 28.6294 - val_loss: 27.4617 - val_mae: 27.4617\n",
      "Epoch 21/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 27.8630 - mae: 27.8630 - val_loss: 26.7572 - val_mae: 26.7572\n",
      "Epoch 22/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 27.0840 - mae: 27.0840 - val_loss: 25.9672 - val_mae: 25.9672\n",
      "Epoch 23/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 26.2362 - mae: 26.2362 - val_loss: 25.1557 - val_mae: 25.1557\n",
      "Epoch 24/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 25.3962 - mae: 25.3962 - val_loss: 24.3985 - val_mae: 24.3985\n",
      "Epoch 25/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 24.5010 - mae: 24.5010 - val_loss: 23.6270 - val_mae: 23.6270\n",
      "Epoch 26/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 23.5757 - mae: 23.5757 - val_loss: 22.7058 - val_mae: 22.7058\n",
      "Epoch 27/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 22.5972 - mae: 22.5972 - val_loss: 21.8845 - val_mae: 21.8845\n",
      "Epoch 28/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 21.6248 - mae: 21.6248 - val_loss: 20.9794 - val_mae: 20.9794\n",
      "Epoch 29/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 20.5948 - mae: 20.5948 - val_loss: 19.7768 - val_mae: 19.7768\n",
      "Epoch 30/100\n",
      "358/358 [==============================] - 0s 140us/sample - loss: 19.5519 - mae: 19.5519 - val_loss: 18.5284 - val_mae: 18.5284\n",
      "Epoch 31/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 18.5900 - mae: 18.5900 - val_loss: 17.4037 - val_mae: 17.4037\n",
      "Epoch 32/100\n",
      "358/358 [==============================] - 0s 143us/sample - loss: 17.6329 - mae: 17.6329 - val_loss: 16.3809 - val_mae: 16.3809\n",
      "Epoch 33/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 16.7213 - mae: 16.7213 - val_loss: 15.6359 - val_mae: 15.6359\n",
      "Epoch 34/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 15.8848 - mae: 15.8848 - val_loss: 14.4932 - val_mae: 14.4932\n",
      "Epoch 35/100\n",
      "358/358 [==============================] - 0s 141us/sample - loss: 14.8797 - mae: 14.8797 - val_loss: 13.7132 - val_mae: 13.7132\n",
      "Epoch 36/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 14.0952 - mae: 14.0952 - val_loss: 13.0217 - val_mae: 13.0217\n",
      "Epoch 37/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 13.2610 - mae: 13.2610 - val_loss: 12.5119 - val_mae: 12.5119\n",
      "Epoch 38/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 12.5050 - mae: 12.5050 - val_loss: 11.7954 - val_mae: 11.7954\n",
      "Epoch 39/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 12.0405 - mae: 12.0405 - val_loss: 11.0217 - val_mae: 11.0217\n",
      "Epoch 40/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 11.3253 - mae: 11.3253 - val_loss: 10.4153 - val_mae: 10.4153\n",
      "Epoch 41/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 10.6650 - mae: 10.6650 - val_loss: 9.9356 - val_mae: 9.9356\n",
      "Epoch 42/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 10.3435 - mae: 10.3435 - val_loss: 9.5950 - val_mae: 9.5950\n",
      "Epoch 43/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 10.0576 - mae: 10.0576 - val_loss: 9.2550 - val_mae: 9.2550\n",
      "Epoch 44/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 9.6192 - mae: 9.6192 - val_loss: 8.8921 - val_mae: 8.8921\n",
      "Epoch 45/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 9.2376 - mae: 9.2376 - val_loss: 8.6308 - val_mae: 8.6308\n",
      "Epoch 46/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 8.8473 - mae: 8.8473 - val_loss: 8.4026 - val_mae: 8.4026\n",
      "Epoch 47/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 8.7035 - mae: 8.7035 - val_loss: 8.1147 - val_mae: 8.1147\n",
      "Epoch 48/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 8.8452 - mae: 8.8452 - val_loss: 7.8382 - val_mae: 7.8382\n",
      "Epoch 49/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 8.2338 - mae: 8.2338 - val_loss: 7.6368 - val_mae: 7.6368\n",
      "Epoch 50/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 7.9778 - mae: 7.9778 - val_loss: 7.4176 - val_mae: 7.4176\n",
      "Epoch 51/100\n",
      "358/358 [==============================] - 0s 140us/sample - loss: 7.7255 - mae: 7.7255 - val_loss: 7.2924 - val_mae: 7.2924\n",
      "Epoch 52/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 7.6851 - mae: 7.6851 - val_loss: 7.1833 - val_mae: 7.1833\n",
      "Epoch 53/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 7.1975 - mae: 7.1975 - val_loss: 7.0348 - val_mae: 7.0348\n",
      "Epoch 54/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 7.4948 - mae: 7.4948 - val_loss: 6.8339 - val_mae: 6.8339\n",
      "Epoch 55/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 7.0375 - mae: 7.0375 - val_loss: 6.6416 - val_mae: 6.6416\n",
      "Epoch 56/100\n",
      "358/358 [==============================] - 0s 157us/sample - loss: 6.9015 - mae: 6.9015 - val_loss: 6.4485 - val_mae: 6.4485\n",
      "Epoch 57/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 6.9234 - mae: 6.9234 - val_loss: 6.2729 - val_mae: 6.2729\n",
      "Epoch 58/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 6.8978 - mae: 6.8978 - val_loss: 6.1140 - val_mae: 6.1140\n",
      "Epoch 59/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 6.6313 - mae: 6.6313 - val_loss: 6.0620 - val_mae: 6.0620\n",
      "Epoch 60/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 6.8417 - mae: 6.8417 - val_loss: 5.8836 - val_mae: 5.8836\n",
      "Epoch 61/100\n",
      "358/358 [==============================] - 0s 139us/sample - loss: 6.4934 - mae: 6.4934 - val_loss: 5.8897 - val_mae: 5.8897\n",
      "Epoch 62/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 6.3747 - mae: 6.3747 - val_loss: 5.5833 - val_mae: 5.5833\n",
      "Epoch 63/100\n",
      "358/358 [==============================] - 0s 140us/sample - loss: 6.4332 - mae: 6.4332 - val_loss: 5.4965 - val_mae: 5.4965\n",
      "Epoch 64/100\n",
      "358/358 [==============================] - 0s 143us/sample - loss: 6.1770 - mae: 6.1770 - val_loss: 5.6278 - val_mae: 5.6278\n",
      "Epoch 65/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 6.5230 - mae: 6.5230 - val_loss: 5.6640 - val_mae: 5.6640\n",
      "358/358 [==============================] - 0s 45us/sample - loss: 5.6640 - mae: 5.6640\n",
      "Val score is 5.663990020751953\n",
      "DataFrame: dermatology imput_method :MLP model :KNN score:  0.83\n",
      "DataFrame: dermatology imput_method :MLP model :XGB score:  0.95\n",
      "DataFrame: dermatology imput_method :MLP model :MLP score:  0.95\n",
      "DataFrame: hepatitis imput_method :LOCF model :KNN score:  0.76\n",
      "DataFrame: hepatitis imput_method :LOCF model :XGB score:  0.79\n",
      "DataFrame: hepatitis imput_method :LOCF model :MLP score:  0.83\n",
      "DataFrame: hepatitis imput_method :mean_mode model :KNN score:  0.76\n",
      "DataFrame: hepatitis imput_method :mean_mode model :XGB score:  0.79\n",
      "DataFrame: hepatitis imput_method :mean_mode model :MLP score:  0.82\n",
      "DataFrame: hepatitis imput_method :knn model :KNN score:  0.78\n",
      "DataFrame: hepatitis imput_method :knn model :XGB score:  0.79\n",
      "DataFrame: hepatitis imput_method :knn model :MLP score:  0.85\n",
      "[0]\tvalidation_0-mae:0.72849\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.52856\n",
      "[2]\tvalidation_0-mae:0.50485\n",
      "[3]\tvalidation_0-mae:0.50352\n",
      "[4]\tvalidation_0-mae:0.50244\n",
      "[5]\tvalidation_0-mae:0.50167\n",
      "[6]\tvalidation_0-mae:0.50125\n",
      "[7]\tvalidation_0-mae:0.50084\n",
      "[8]\tvalidation_0-mae:0.50052\n",
      "[9]\tvalidation_0-mae:0.50030\n",
      "[10]\tvalidation_0-mae:0.50010\n",
      "[11]\tvalidation_0-mae:0.49996\n",
      "[12]\tvalidation_0-mae:0.49992\n",
      "[13]\tvalidation_0-mae:0.49986\n",
      "[14]\tvalidation_0-mae:0.49996\n",
      "[15]\tvalidation_0-mae:0.50002\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-mae:0.49986\n",
      "\n",
      "[0]\tvalidation_0-mae:0.61465\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.44148\n",
      "[2]\tvalidation_0-mae:0.36200\n",
      "[3]\tvalidation_0-mae:0.38767\n",
      "[4]\tvalidation_0-mae:0.40437\n",
      "Stopping. Best iteration:\n",
      "[2]\tvalidation_0-mae:0.36200\n",
      "\n",
      "[0]\tvalidation_0-mae:0.79190\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.59665\n",
      "[2]\tvalidation_0-mae:0.51511\n",
      "[3]\tvalidation_0-mae:0.49154\n",
      "[4]\tvalidation_0-mae:0.47518\n",
      "[5]\tvalidation_0-mae:0.46415\n",
      "[6]\tvalidation_0-mae:0.45561\n",
      "[7]\tvalidation_0-mae:0.44796\n",
      "[8]\tvalidation_0-mae:0.44375\n",
      "[9]\tvalidation_0-mae:0.44018\n",
      "[10]\tvalidation_0-mae:0.43676\n",
      "[11]\tvalidation_0-mae:0.43529\n",
      "[12]\tvalidation_0-mae:0.43382\n",
      "[13]\tvalidation_0-mae:0.43250\n",
      "[14]\tvalidation_0-mae:0.43289\n",
      "[15]\tvalidation_0-mae:0.43375\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-mae:0.43250\n",
      "\n",
      "[0]\tvalidation_0-mae:0.92421\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.71865\n",
      "[2]\tvalidation_0-mae:0.61103\n",
      "[3]\tvalidation_0-mae:0.53076\n",
      "[4]\tvalidation_0-mae:0.47450\n",
      "[5]\tvalidation_0-mae:0.43578\n",
      "[6]\tvalidation_0-mae:0.40636\n",
      "[7]\tvalidation_0-mae:0.38609\n",
      "[8]\tvalidation_0-mae:0.37229\n",
      "[9]\tvalidation_0-mae:0.36289\n",
      "[10]\tvalidation_0-mae:0.35596\n",
      "[11]\tvalidation_0-mae:0.35124\n",
      "[12]\tvalidation_0-mae:0.34191\n",
      "[13]\tvalidation_0-mae:0.33283\n",
      "[14]\tvalidation_0-mae:0.33306\n",
      "[15]\tvalidation_0-mae:0.33408\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-mae:0.33283\n",
      "\n",
      "[0]\tvalidation_0-mae:0.95731\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.73694\n",
      "[2]\tvalidation_0-mae:0.61088\n",
      "[3]\tvalidation_0-mae:0.52020\n",
      "[4]\tvalidation_0-mae:0.45643\n",
      "[5]\tvalidation_0-mae:0.40964\n",
      "[6]\tvalidation_0-mae:0.37781\n",
      "[7]\tvalidation_0-mae:0.35315\n",
      "[8]\tvalidation_0-mae:0.33524\n",
      "[9]\tvalidation_0-mae:0.32053\n",
      "[10]\tvalidation_0-mae:0.31021\n",
      "[11]\tvalidation_0-mae:0.30320\n",
      "[12]\tvalidation_0-mae:0.29507\n",
      "[13]\tvalidation_0-mae:0.28876\n",
      "[14]\tvalidation_0-mae:0.28465\n",
      "[15]\tvalidation_0-mae:0.28259\n",
      "[16]\tvalidation_0-mae:0.28419\n",
      "[17]\tvalidation_0-mae:0.28418\n",
      "Stopping. Best iteration:\n",
      "[15]\tvalidation_0-mae:0.28259\n",
      "\n",
      "[0]\tvalidation_0-mae:0.77911\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.58080\n",
      "[2]\tvalidation_0-mae:0.55433\n",
      "[3]\tvalidation_0-mae:0.53466\n",
      "[4]\tvalidation_0-mae:0.52199\n",
      "[5]\tvalidation_0-mae:0.51218\n",
      "[6]\tvalidation_0-mae:0.50385\n",
      "[7]\tvalidation_0-mae:0.49916\n",
      "[8]\tvalidation_0-mae:0.49491\n",
      "[9]\tvalidation_0-mae:0.49255\n",
      "[10]\tvalidation_0-mae:0.49105\n",
      "[11]\tvalidation_0-mae:0.48847\n",
      "[12]\tvalidation_0-mae:0.48754\n",
      "[13]\tvalidation_0-mae:0.48795\n",
      "[14]\tvalidation_0-mae:0.48688\n",
      "[15]\tvalidation_0-mae:0.48597\n",
      "[16]\tvalidation_0-mae:0.48559\n",
      "[17]\tvalidation_0-mae:0.48619\n",
      "[18]\tvalidation_0-mae:0.48682\n",
      "Stopping. Best iteration:\n",
      "[16]\tvalidation_0-mae:0.48559\n",
      "\n",
      "[0]\tvalidation_0-mae:0.93630\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.72578\n",
      "[2]\tvalidation_0-mae:0.61084\n",
      "[3]\tvalidation_0-mae:0.53019\n",
      "[4]\tvalidation_0-mae:0.46849\n",
      "[5]\tvalidation_0-mae:0.42893\n",
      "[6]\tvalidation_0-mae:0.40127\n",
      "[7]\tvalidation_0-mae:0.37733\n",
      "[8]\tvalidation_0-mae:0.36136\n",
      "[9]\tvalidation_0-mae:0.34958\n",
      "[10]\tvalidation_0-mae:0.34082\n",
      "[11]\tvalidation_0-mae:0.33229\n",
      "[12]\tvalidation_0-mae:0.32685\n",
      "[13]\tvalidation_0-mae:0.32567\n",
      "[14]\tvalidation_0-mae:0.32435\n",
      "[15]\tvalidation_0-mae:0.32469\n",
      "[16]\tvalidation_0-mae:0.32338\n",
      "[17]\tvalidation_0-mae:0.31940\n",
      "[18]\tvalidation_0-mae:0.31755\n",
      "[19]\tvalidation_0-mae:0.31897\n",
      "[20]\tvalidation_0-mae:0.31655\n",
      "[21]\tvalidation_0-mae:0.31503\n",
      "[22]\tvalidation_0-mae:0.31917\n",
      "[23]\tvalidation_0-mae:0.32170\n",
      "Stopping. Best iteration:\n",
      "[21]\tvalidation_0-mae:0.31503\n",
      "\n",
      "[0]\tvalidation_0-mae:0.83329\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.64078\n",
      "[2]\tvalidation_0-mae:0.58462\n",
      "[3]\tvalidation_0-mae:0.54724\n",
      "[4]\tvalidation_0-mae:0.51934\n",
      "[5]\tvalidation_0-mae:0.49924\n",
      "[6]\tvalidation_0-mae:0.48479\n",
      "[7]\tvalidation_0-mae:0.47480\n",
      "[8]\tvalidation_0-mae:0.46605\n",
      "[9]\tvalidation_0-mae:0.46087\n",
      "[10]\tvalidation_0-mae:0.45826\n",
      "[11]\tvalidation_0-mae:0.45480\n",
      "[12]\tvalidation_0-mae:0.45298\n",
      "[13]\tvalidation_0-mae:0.45209\n",
      "[14]\tvalidation_0-mae:0.44920\n",
      "[15]\tvalidation_0-mae:0.45056\n",
      "[16]\tvalidation_0-mae:0.44881\n",
      "[17]\tvalidation_0-mae:0.44754\n",
      "[18]\tvalidation_0-mae:0.44640\n",
      "[19]\tvalidation_0-mae:0.44734\n",
      "[20]\tvalidation_0-mae:0.44825\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-mae:0.44640\n",
      "\n",
      "[0]\tvalidation_0-mae:0.98653\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.75370\n",
      "[2]\tvalidation_0-mae:0.60394\n",
      "[3]\tvalidation_0-mae:0.50032\n",
      "[4]\tvalidation_0-mae:0.42166\n",
      "[5]\tvalidation_0-mae:0.36799\n",
      "[6]\tvalidation_0-mae:0.32845\n",
      "[7]\tvalidation_0-mae:0.30126\n",
      "[8]\tvalidation_0-mae:0.28120\n",
      "[9]\tvalidation_0-mae:0.26963\n",
      "[10]\tvalidation_0-mae:0.26261\n",
      "[11]\tvalidation_0-mae:0.25360\n",
      "[12]\tvalidation_0-mae:0.24774\n",
      "[13]\tvalidation_0-mae:0.24419\n",
      "[14]\tvalidation_0-mae:0.24023\n",
      "[15]\tvalidation_0-mae:0.23896\n",
      "[16]\tvalidation_0-mae:0.23525\n",
      "[17]\tvalidation_0-mae:0.23781\n",
      "[18]\tvalidation_0-mae:0.23328\n",
      "[19]\tvalidation_0-mae:0.23527\n",
      "[20]\tvalidation_0-mae:0.23541\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-mae:0.23328\n",
      "\n",
      "[0]\tvalidation_0-mae:0.99575\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.75911\n",
      "[2]\tvalidation_0-mae:0.60497\n",
      "[3]\tvalidation_0-mae:0.49516\n",
      "[4]\tvalidation_0-mae:0.41509\n",
      "[5]\tvalidation_0-mae:0.35895\n",
      "[6]\tvalidation_0-mae:0.31916\n",
      "[7]\tvalidation_0-mae:0.29026\n",
      "[8]\tvalidation_0-mae:0.26726\n",
      "[9]\tvalidation_0-mae:0.24885\n",
      "[10]\tvalidation_0-mae:0.23861\n",
      "[11]\tvalidation_0-mae:0.22889\n",
      "[12]\tvalidation_0-mae:0.22242\n",
      "[13]\tvalidation_0-mae:0.21860\n",
      "[14]\tvalidation_0-mae:0.21247\n",
      "[15]\tvalidation_0-mae:0.21133\n",
      "[16]\tvalidation_0-mae:0.21098\n",
      "[17]\tvalidation_0-mae:0.20969\n",
      "[18]\tvalidation_0-mae:0.21018\n",
      "[19]\tvalidation_0-mae:0.21133\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-mae:0.20969\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:0.73650\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.63586\n",
      "[2]\tvalidation_0-mae:0.60369\n",
      "[3]\tvalidation_0-mae:0.59769\n",
      "[4]\tvalidation_0-mae:0.59146\n",
      "[5]\tvalidation_0-mae:0.58876\n",
      "[6]\tvalidation_0-mae:0.58287\n",
      "[7]\tvalidation_0-mae:0.57432\n",
      "[8]\tvalidation_0-mae:0.59229\n",
      "[9]\tvalidation_0-mae:0.59481\n",
      "Stopping. Best iteration:\n",
      "[7]\tvalidation_0-mae:0.57432\n",
      "\n",
      "[0]\tvalidation_0-mae:75.83102\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:57.30908\n",
      "[2]\tvalidation_0-mae:45.21658\n",
      "[3]\tvalidation_0-mae:38.57803\n",
      "[4]\tvalidation_0-mae:34.19435\n",
      "[5]\tvalidation_0-mae:30.51182\n",
      "[6]\tvalidation_0-mae:28.01520\n",
      "[7]\tvalidation_0-mae:26.14144\n",
      "[8]\tvalidation_0-mae:24.01025\n",
      "[9]\tvalidation_0-mae:21.99362\n",
      "[10]\tvalidation_0-mae:20.29127\n",
      "[11]\tvalidation_0-mae:18.60715\n",
      "[12]\tvalidation_0-mae:17.00975\n",
      "[13]\tvalidation_0-mae:16.00872\n",
      "[14]\tvalidation_0-mae:14.92299\n",
      "[15]\tvalidation_0-mae:13.81232\n",
      "[16]\tvalidation_0-mae:13.10910\n",
      "[17]\tvalidation_0-mae:12.41086\n",
      "[18]\tvalidation_0-mae:11.64304\n",
      "[19]\tvalidation_0-mae:11.19623\n",
      "[20]\tvalidation_0-mae:10.70430\n",
      "[21]\tvalidation_0-mae:10.01982\n",
      "[22]\tvalidation_0-mae:9.54977\n",
      "[23]\tvalidation_0-mae:8.83336\n",
      "[24]\tvalidation_0-mae:8.27197\n",
      "[25]\tvalidation_0-mae:7.98820\n",
      "[26]\tvalidation_0-mae:7.66626\n",
      "[27]\tvalidation_0-mae:7.16552\n",
      "[28]\tvalidation_0-mae:6.82334\n",
      "[29]\tvalidation_0-mae:6.38247\n",
      "[30]\tvalidation_0-mae:6.18620\n",
      "[31]\tvalidation_0-mae:5.79460\n",
      "[32]\tvalidation_0-mae:5.55058\n",
      "[33]\tvalidation_0-mae:5.24334\n",
      "[34]\tvalidation_0-mae:4.94941\n",
      "[35]\tvalidation_0-mae:4.77658\n",
      "[36]\tvalidation_0-mae:4.53847\n",
      "[37]\tvalidation_0-mae:4.38433\n",
      "[38]\tvalidation_0-mae:4.17539\n",
      "[39]\tvalidation_0-mae:4.04968\n",
      "[40]\tvalidation_0-mae:3.91201\n",
      "[41]\tvalidation_0-mae:3.83148\n",
      "[42]\tvalidation_0-mae:3.69249\n",
      "[43]\tvalidation_0-mae:3.58516\n",
      "[44]\tvalidation_0-mae:3.48960\n",
      "[45]\tvalidation_0-mae:3.35510\n",
      "[46]\tvalidation_0-mae:3.20972\n",
      "[47]\tvalidation_0-mae:3.05204\n",
      "[48]\tvalidation_0-mae:2.95894\n",
      "[49]\tvalidation_0-mae:2.92940\n",
      "[50]\tvalidation_0-mae:2.81423\n",
      "[51]\tvalidation_0-mae:2.72741\n",
      "[52]\tvalidation_0-mae:2.62495\n",
      "[53]\tvalidation_0-mae:2.51498\n",
      "[54]\tvalidation_0-mae:2.40549\n",
      "[55]\tvalidation_0-mae:2.31996\n",
      "[56]\tvalidation_0-mae:2.26541\n",
      "[57]\tvalidation_0-mae:2.19229\n",
      "[58]\tvalidation_0-mae:2.12007\n",
      "[59]\tvalidation_0-mae:2.06895\n",
      "[60]\tvalidation_0-mae:2.01254\n",
      "[61]\tvalidation_0-mae:1.95494\n",
      "[62]\tvalidation_0-mae:1.92426\n",
      "[63]\tvalidation_0-mae:1.89110\n",
      "[64]\tvalidation_0-mae:1.86722\n",
      "[65]\tvalidation_0-mae:1.82132\n",
      "[66]\tvalidation_0-mae:1.76989\n",
      "[67]\tvalidation_0-mae:1.72932\n",
      "[68]\tvalidation_0-mae:1.73560\n",
      "[69]\tvalidation_0-mae:1.72028\n",
      "[70]\tvalidation_0-mae:1.69524\n",
      "[71]\tvalidation_0-mae:1.65999\n",
      "[72]\tvalidation_0-mae:1.62468\n",
      "[73]\tvalidation_0-mae:1.61797\n",
      "[74]\tvalidation_0-mae:1.59889\n",
      "[75]\tvalidation_0-mae:1.58669\n",
      "[76]\tvalidation_0-mae:1.54020\n",
      "[77]\tvalidation_0-mae:1.50364\n",
      "[78]\tvalidation_0-mae:1.48404\n",
      "[79]\tvalidation_0-mae:1.47190\n",
      "[80]\tvalidation_0-mae:1.44377\n",
      "[81]\tvalidation_0-mae:1.42319\n",
      "[82]\tvalidation_0-mae:1.42589\n",
      "[83]\tvalidation_0-mae:1.42851\n",
      "Stopping. Best iteration:\n",
      "[81]\tvalidation_0-mae:1.42319\n",
      "\n",
      "[0]\tvalidation_0-mae:65.29009\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:52.59268\n",
      "[2]\tvalidation_0-mae:47.37643\n",
      "[3]\tvalidation_0-mae:43.02621\n",
      "[4]\tvalidation_0-mae:40.05509\n",
      "[5]\tvalidation_0-mae:36.61806\n",
      "[6]\tvalidation_0-mae:33.45132\n",
      "[7]\tvalidation_0-mae:30.63791\n",
      "[8]\tvalidation_0-mae:28.66530\n",
      "[9]\tvalidation_0-mae:26.95004\n",
      "[10]\tvalidation_0-mae:25.58833\n",
      "[11]\tvalidation_0-mae:24.33642\n",
      "[12]\tvalidation_0-mae:22.77794\n",
      "[13]\tvalidation_0-mae:21.23202\n",
      "[14]\tvalidation_0-mae:20.01996\n",
      "[15]\tvalidation_0-mae:18.90628\n",
      "[16]\tvalidation_0-mae:17.97472\n",
      "[17]\tvalidation_0-mae:16.88771\n",
      "[18]\tvalidation_0-mae:15.93831\n",
      "[19]\tvalidation_0-mae:15.05848\n",
      "[20]\tvalidation_0-mae:14.04619\n",
      "[21]\tvalidation_0-mae:13.06636\n",
      "[22]\tvalidation_0-mae:12.32679\n",
      "[23]\tvalidation_0-mae:11.75693\n",
      "[24]\tvalidation_0-mae:11.12600\n",
      "[25]\tvalidation_0-mae:10.78932\n",
      "[26]\tvalidation_0-mae:10.19900\n",
      "[27]\tvalidation_0-mae:9.79700\n",
      "[28]\tvalidation_0-mae:9.12257\n",
      "[29]\tvalidation_0-mae:8.64663\n",
      "[30]\tvalidation_0-mae:8.21318\n",
      "[31]\tvalidation_0-mae:7.93644\n",
      "[32]\tvalidation_0-mae:7.71588\n",
      "[33]\tvalidation_0-mae:7.43062\n",
      "[34]\tvalidation_0-mae:7.16718\n",
      "[35]\tvalidation_0-mae:7.00250\n",
      "[36]\tvalidation_0-mae:6.73271\n",
      "[37]\tvalidation_0-mae:6.54614\n",
      "[38]\tvalidation_0-mae:6.25293\n",
      "[39]\tvalidation_0-mae:5.95623\n",
      "[40]\tvalidation_0-mae:5.73268\n",
      "[41]\tvalidation_0-mae:5.46814\n",
      "[42]\tvalidation_0-mae:5.29820\n",
      "[43]\tvalidation_0-mae:5.19230\n",
      "[44]\tvalidation_0-mae:4.98989\n",
      "[45]\tvalidation_0-mae:4.89390\n",
      "[46]\tvalidation_0-mae:4.70836\n",
      "[47]\tvalidation_0-mae:4.62383\n",
      "[48]\tvalidation_0-mae:4.48492\n",
      "[49]\tvalidation_0-mae:4.30743\n",
      "[50]\tvalidation_0-mae:4.11897\n",
      "[51]\tvalidation_0-mae:3.98321\n",
      "[52]\tvalidation_0-mae:3.87240\n",
      "[53]\tvalidation_0-mae:3.76349\n",
      "[54]\tvalidation_0-mae:3.68376\n",
      "[55]\tvalidation_0-mae:3.60814\n",
      "[56]\tvalidation_0-mae:3.51556\n",
      "[57]\tvalidation_0-mae:3.37940\n",
      "[58]\tvalidation_0-mae:3.32080\n",
      "[59]\tvalidation_0-mae:3.23406\n",
      "[60]\tvalidation_0-mae:3.16366\n",
      "[61]\tvalidation_0-mae:3.12382\n",
      "[62]\tvalidation_0-mae:3.02177\n",
      "[63]\tvalidation_0-mae:2.92673\n",
      "[64]\tvalidation_0-mae:2.84323\n",
      "[65]\tvalidation_0-mae:2.75214\n",
      "[66]\tvalidation_0-mae:2.67271\n",
      "[67]\tvalidation_0-mae:2.61740\n",
      "[68]\tvalidation_0-mae:2.54597\n",
      "[69]\tvalidation_0-mae:2.46800\n",
      "[70]\tvalidation_0-mae:2.41141\n",
      "[71]\tvalidation_0-mae:2.37745\n",
      "[72]\tvalidation_0-mae:2.34760\n",
      "[73]\tvalidation_0-mae:2.30601\n",
      "[74]\tvalidation_0-mae:2.24746\n",
      "[75]\tvalidation_0-mae:2.20141\n",
      "[76]\tvalidation_0-mae:2.16402\n",
      "[77]\tvalidation_0-mae:2.10959\n",
      "[78]\tvalidation_0-mae:2.09059\n",
      "[79]\tvalidation_0-mae:2.04142\n",
      "[80]\tvalidation_0-mae:2.02055\n",
      "[81]\tvalidation_0-mae:1.99157\n",
      "[82]\tvalidation_0-mae:1.98491\n",
      "[83]\tvalidation_0-mae:1.95515\n",
      "[84]\tvalidation_0-mae:1.93729\n",
      "[85]\tvalidation_0-mae:1.92768\n",
      "[86]\tvalidation_0-mae:1.90598\n",
      "[87]\tvalidation_0-mae:1.89372\n",
      "[88]\tvalidation_0-mae:1.87270\n",
      "[89]\tvalidation_0-mae:1.84538\n",
      "[90]\tvalidation_0-mae:1.82998\n",
      "[91]\tvalidation_0-mae:1.78724\n",
      "[92]\tvalidation_0-mae:1.76404\n",
      "[93]\tvalidation_0-mae:1.72500\n",
      "[94]\tvalidation_0-mae:1.71014\n",
      "[95]\tvalidation_0-mae:1.70239\n",
      "[96]\tvalidation_0-mae:1.67387\n",
      "[97]\tvalidation_0-mae:1.66302\n",
      "[98]\tvalidation_0-mae:1.65206\n",
      "[99]\tvalidation_0-mae:1.64633\n",
      "[100]\tvalidation_0-mae:1.64563\n",
      "[101]\tvalidation_0-mae:1.63599\n",
      "[102]\tvalidation_0-mae:1.63733\n",
      "[103]\tvalidation_0-mae:1.61643\n",
      "[104]\tvalidation_0-mae:1.59445\n",
      "[105]\tvalidation_0-mae:1.59302\n",
      "[106]\tvalidation_0-mae:1.55560\n",
      "[107]\tvalidation_0-mae:1.53396\n",
      "[108]\tvalidation_0-mae:1.52278\n",
      "[109]\tvalidation_0-mae:1.52077\n",
      "[110]\tvalidation_0-mae:1.52076\n",
      "[111]\tvalidation_0-mae:1.49467\n",
      "[112]\tvalidation_0-mae:1.46043\n",
      "[113]\tvalidation_0-mae:1.44373\n",
      "[114]\tvalidation_0-mae:1.44267\n",
      "[115]\tvalidation_0-mae:1.42893\n",
      "[116]\tvalidation_0-mae:1.42558\n",
      "[117]\tvalidation_0-mae:1.42219\n",
      "[118]\tvalidation_0-mae:1.42079\n",
      "[119]\tvalidation_0-mae:1.41463\n",
      "[120]\tvalidation_0-mae:1.38552\n",
      "[121]\tvalidation_0-mae:1.38543\n",
      "[122]\tvalidation_0-mae:1.38000\n",
      "[123]\tvalidation_0-mae:1.35955\n",
      "[124]\tvalidation_0-mae:1.35188\n",
      "[125]\tvalidation_0-mae:1.32979\n",
      "[126]\tvalidation_0-mae:1.32975\n",
      "[127]\tvalidation_0-mae:1.32978\n",
      "[128]\tvalidation_0-mae:1.31994\n",
      "[129]\tvalidation_0-mae:1.31726\n",
      "[130]\tvalidation_0-mae:1.31722\n",
      "[131]\tvalidation_0-mae:1.31781\n",
      "[132]\tvalidation_0-mae:1.29506\n",
      "[133]\tvalidation_0-mae:1.29624\n",
      "[134]\tvalidation_0-mae:1.28022\n",
      "[135]\tvalidation_0-mae:1.28202\n",
      "[136]\tvalidation_0-mae:1.27326\n",
      "[137]\tvalidation_0-mae:1.26286\n",
      "[138]\tvalidation_0-mae:1.25657\n",
      "[139]\tvalidation_0-mae:1.25251\n",
      "[140]\tvalidation_0-mae:1.25228\n",
      "[141]\tvalidation_0-mae:1.25114\n",
      "[142]\tvalidation_0-mae:1.23100\n",
      "[143]\tvalidation_0-mae:1.22786\n",
      "[144]\tvalidation_0-mae:1.22805\n",
      "[145]\tvalidation_0-mae:1.20153\n",
      "[146]\tvalidation_0-mae:1.20151\n",
      "[147]\tvalidation_0-mae:1.20146\n",
      "[148]\tvalidation_0-mae:1.20152\n",
      "[149]\tvalidation_0-mae:1.20060\n",
      "[0]\tvalidation_0-mae:2.40238\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.73615\n",
      "[2]\tvalidation_0-mae:1.27521\n",
      "[3]\tvalidation_0-mae:0.95887\n",
      "[4]\tvalidation_0-mae:0.77329\n",
      "[5]\tvalidation_0-mae:0.64912\n",
      "[6]\tvalidation_0-mae:0.57618\n",
      "[7]\tvalidation_0-mae:0.53667\n",
      "[8]\tvalidation_0-mae:0.51094\n",
      "[9]\tvalidation_0-mae:0.49367\n",
      "[10]\tvalidation_0-mae:0.48268\n",
      "[11]\tvalidation_0-mae:0.47170\n",
      "[12]\tvalidation_0-mae:0.46558\n",
      "[13]\tvalidation_0-mae:0.46201\n",
      "[14]\tvalidation_0-mae:0.46034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15]\tvalidation_0-mae:0.45952\n",
      "[16]\tvalidation_0-mae:0.45703\n",
      "[17]\tvalidation_0-mae:0.45642\n",
      "[18]\tvalidation_0-mae:0.45665\n",
      "[19]\tvalidation_0-mae:0.45628\n",
      "[20]\tvalidation_0-mae:0.45544\n",
      "[21]\tvalidation_0-mae:0.45578\n",
      "[22]\tvalidation_0-mae:0.45531\n",
      "[23]\tvalidation_0-mae:0.45444\n",
      "[24]\tvalidation_0-mae:0.45572\n",
      "[25]\tvalidation_0-mae:0.45464\n",
      "Stopping. Best iteration:\n",
      "[23]\tvalidation_0-mae:0.45444\n",
      "\n",
      "[0]\tvalidation_0-mae:45.58359\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:34.23891\n",
      "[2]\tvalidation_0-mae:26.80280\n",
      "[3]\tvalidation_0-mae:21.40434\n",
      "[4]\tvalidation_0-mae:17.95964\n",
      "[5]\tvalidation_0-mae:15.83616\n",
      "[6]\tvalidation_0-mae:14.31218\n",
      "[7]\tvalidation_0-mae:13.14538\n",
      "[8]\tvalidation_0-mae:11.79153\n",
      "[9]\tvalidation_0-mae:10.58966\n",
      "[10]\tvalidation_0-mae:9.74212\n",
      "[11]\tvalidation_0-mae:8.95003\n",
      "[12]\tvalidation_0-mae:8.40409\n",
      "[13]\tvalidation_0-mae:7.88196\n",
      "[14]\tvalidation_0-mae:7.32925\n",
      "[15]\tvalidation_0-mae:6.82092\n",
      "[16]\tvalidation_0-mae:6.47928\n",
      "[17]\tvalidation_0-mae:6.09465\n",
      "[18]\tvalidation_0-mae:5.63100\n",
      "[19]\tvalidation_0-mae:5.28027\n",
      "[20]\tvalidation_0-mae:4.98080\n",
      "[21]\tvalidation_0-mae:4.59735\n",
      "[22]\tvalidation_0-mae:4.36651\n",
      "[23]\tvalidation_0-mae:4.06717\n",
      "[24]\tvalidation_0-mae:3.88214\n",
      "[25]\tvalidation_0-mae:3.73026\n",
      "[26]\tvalidation_0-mae:3.54493\n",
      "[27]\tvalidation_0-mae:3.37393\n",
      "[28]\tvalidation_0-mae:3.27743\n",
      "[29]\tvalidation_0-mae:3.13894\n",
      "[30]\tvalidation_0-mae:3.01556\n",
      "[31]\tvalidation_0-mae:2.86764\n",
      "[32]\tvalidation_0-mae:2.78310\n",
      "[33]\tvalidation_0-mae:2.69851\n",
      "[34]\tvalidation_0-mae:2.60802\n",
      "[35]\tvalidation_0-mae:2.49050\n",
      "[36]\tvalidation_0-mae:2.40483\n",
      "[37]\tvalidation_0-mae:2.34704\n",
      "[38]\tvalidation_0-mae:2.33996\n",
      "[39]\tvalidation_0-mae:2.29890\n",
      "[40]\tvalidation_0-mae:2.24252\n",
      "[41]\tvalidation_0-mae:2.21460\n",
      "[42]\tvalidation_0-mae:2.15504\n",
      "[43]\tvalidation_0-mae:2.08865\n",
      "[44]\tvalidation_0-mae:2.07366\n",
      "[45]\tvalidation_0-mae:2.01959\n",
      "[46]\tvalidation_0-mae:1.98338\n",
      "[47]\tvalidation_0-mae:1.89723\n",
      "[48]\tvalidation_0-mae:1.79092\n",
      "[49]\tvalidation_0-mae:1.77323\n",
      "[50]\tvalidation_0-mae:1.74840\n",
      "[51]\tvalidation_0-mae:1.74945\n",
      "[52]\tvalidation_0-mae:1.72246\n",
      "[53]\tvalidation_0-mae:1.70100\n",
      "[54]\tvalidation_0-mae:1.69490\n",
      "[55]\tvalidation_0-mae:1.66904\n",
      "[56]\tvalidation_0-mae:1.64390\n",
      "[57]\tvalidation_0-mae:1.63912\n",
      "[58]\tvalidation_0-mae:1.60865\n",
      "[59]\tvalidation_0-mae:1.60138\n",
      "[60]\tvalidation_0-mae:1.59647\n",
      "[61]\tvalidation_0-mae:1.59432\n",
      "[62]\tvalidation_0-mae:1.58376\n",
      "[63]\tvalidation_0-mae:1.58529\n",
      "[64]\tvalidation_0-mae:1.58565\n",
      "Stopping. Best iteration:\n",
      "[62]\tvalidation_0-mae:1.58376\n",
      "\n",
      "DataFrame: hepatitis imput_method :trees model :KNN score:  0.76\n",
      "DataFrame: hepatitis imput_method :trees model :XGB score:  0.79\n",
      "DataFrame: hepatitis imput_method :trees model :MLP score:  0.83\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 17ms/sample - loss: 1.8290 - mae: 1.8290 - val_loss: 0.9212 - val_mae: 0.9212\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.6003 - mae: 1.6003 - val_loss: 0.9189 - val_mae: 0.9189\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5318 - mae: 1.5318 - val_loss: 0.9156 - val_mae: 0.9156\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4900 - mae: 1.4900 - val_loss: 0.9119 - val_mae: 0.9119\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4406 - mae: 1.4406 - val_loss: 0.9033 - val_mae: 0.9033\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.4082 - mae: 1.4082 - val_loss: 0.8926 - val_mae: 0.8926\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3772 - mae: 1.3772 - val_loss: 0.8813 - val_mae: 0.8813\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3202 - mae: 1.3202 - val_loss: 0.8651 - val_mae: 0.8651\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2927 - mae: 1.2927 - val_loss: 0.8464 - val_mae: 0.8464\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.2830 - mae: 1.2830 - val_loss: 0.8268 - val_mae: 0.8268\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2166 - mae: 1.2166 - val_loss: 0.8078 - val_mae: 0.8078\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1868 - mae: 1.1868 - val_loss: 0.7879 - val_mae: 0.7879\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1526 - mae: 1.1526 - val_loss: 0.7700 - val_mae: 0.7700\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1125 - mae: 1.1125 - val_loss: 0.7503 - val_mae: 0.7503\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1045 - mae: 1.1045 - val_loss: 0.7310 - val_mae: 0.7310\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0687 - mae: 1.0687 - val_loss: 0.7079 - val_mae: 0.7079\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0204 - mae: 1.0204 - val_loss: 0.6882 - val_mae: 0.6882\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9977 - mae: 0.9977 - val_loss: 0.6689 - val_mae: 0.6689\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9642 - mae: 0.9642 - val_loss: 0.6549 - val_mae: 0.6549\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9476 - mae: 0.9476 - val_loss: 0.6397 - val_mae: 0.6397\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9026 - mae: 0.9026 - val_loss: 0.6222 - val_mae: 0.6222\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.8463 - mae: 0.8463 - val_loss: 0.6031 - val_mae: 0.6031\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8485 - mae: 0.8485 - val_loss: 0.5819 - val_mae: 0.5819\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7856 - mae: 0.7856 - val_loss: 0.5583 - val_mae: 0.5583\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7508 - mae: 0.7508 - val_loss: 0.5347 - val_mae: 0.5347\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7481 - mae: 0.7481 - val_loss: 0.5135 - val_mae: 0.5135\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7560 - mae: 0.7560 - val_loss: 0.4902 - val_mae: 0.4902\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6979 - mae: 0.6979 - val_loss: 0.4670 - val_mae: 0.4670\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6253 - mae: 0.6253 - val_loss: 0.4544 - val_mae: 0.4544\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.6275 - mae: 0.6275 - val_loss: 0.4434 - val_mae: 0.4434\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6087 - mae: 0.6087 - val_loss: 0.4256 - val_mae: 0.4256\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.5920 - mae: 0.592 - 0s 275us/sample - loss: 0.5773 - mae: 0.5773 - val_loss: 0.4137 - val_mae: 0.4137\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6017 - mae: 0.6017 - val_loss: 0.4049 - val_mae: 0.4049\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4905 - mae: 0.4905 - val_loss: 0.3975 - val_mae: 0.3975\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4825 - mae: 0.4825 - val_loss: 0.3885 - val_mae: 0.3885\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4902 - mae: 0.4902 - val_loss: 0.3841 - val_mae: 0.3841\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4751 - mae: 0.4751 - val_loss: 0.3698 - val_mae: 0.3698\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4742 - mae: 0.4742 - val_loss: 0.3495 - val_mae: 0.3495\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4507 - mae: 0.4507 - val_loss: 0.3278 - val_mae: 0.3278\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4165 - mae: 0.4165 - val_loss: 0.3255 - val_mae: 0.3255\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3441 - mae: 0.3441 - val_loss: 0.3222 - val_mae: 0.3222\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3548 - mae: 0.3548 - val_loss: 0.3072 - val_mae: 0.3072\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3265 - mae: 0.3265 - val_loss: 0.3038 - val_mae: 0.3038\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3090 - mae: 0.3090 - val_loss: 0.3081 - val_mae: 0.3081\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.2955 - mae: 0.2955 - val_loss: 0.3127 - val_mae: 0.3127\n",
      "80/80 [==============================] - 0s 110us/sample - loss: 0.3127 - mae: 0.3127\n",
      "Val score is 0.3126836121082306\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 1.7196 - mae: 1.7196 - val_loss: 0.9111 - val_mae: 0.9111\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 1.4875 - mae: 1.4875 - val_loss: 0.8889 - val_mae: 0.8889\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3907 - mae: 1.3907 - val_loss: 0.8732 - val_mae: 0.8732\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 272us/sample - loss: 1.2913 - mae: 1.2913 - val_loss: 0.8546 - val_mae: 0.8546\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2761 - mae: 1.2761 - val_loss: 0.8271 - val_mae: 0.8271\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2447 - mae: 1.2447 - val_loss: 0.7993 - val_mae: 0.7993\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 1.1743 - mae: 1.1743 - val_loss: 0.7740 - val_mae: 0.7740\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1257 - mae: 1.1257 - val_loss: 0.7507 - val_mae: 0.7507\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.1089 - mae: 1.1089 - val_loss: 0.7279 - val_mae: 0.7279\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0569 - mae: 1.0569 - val_loss: 0.7027 - val_mae: 0.7027\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0534 - mae: 1.0534 - val_loss: 0.6736 - val_mae: 0.6736\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.0090 - mae: 1.0090 - val_loss: 0.6440 - val_mae: 0.6440\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9906 - mae: 0.9906 - val_loss: 0.6174 - val_mae: 0.6174\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 0.9505 - mae: 0.9505 - val_loss: 0.5934 - val_mae: 0.5934\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9075 - mae: 0.9075 - val_loss: 0.5679 - val_mae: 0.5679\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8701 - mae: 0.8701 - val_loss: 0.5459 - val_mae: 0.5459\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8891 - mae: 0.8891 - val_loss: 0.5267 - val_mae: 0.5267\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.8058 - mae: 0.8058 - val_loss: 0.5108 - val_mae: 0.5108\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 283us/sample - loss: 0.8041 - mae: 0.8041 - val_loss: 0.4985 - val_mae: 0.4985\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8011 - mae: 0.8011 - val_loss: 0.4725 - val_mae: 0.4725\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7425 - mae: 0.7425 - val_loss: 0.4462 - val_mae: 0.4462\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7151 - mae: 0.7151 - val_loss: 0.4280 - val_mae: 0.4280\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7214 - mae: 0.7214 - val_loss: 0.4089 - val_mae: 0.4089\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6585 - mae: 0.6585 - val_loss: 0.3948 - val_mae: 0.3948\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6012 - mae: 0.6012 - val_loss: 0.3953 - val_mae: 0.3953\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5484 - mae: 0.5484 - val_loss: 0.4017 - val_mae: 0.4017\n",
      "80/80 [==============================] - 0s 93us/sample - loss: 0.4017 - mae: 0.4017\n",
      "Val score is 0.40165290236473083\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 17ms/sample - loss: 2.0586 - mae: 2.0586 - val_loss: 1.5626 - val_mae: 1.5626\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.8692 - mae: 1.8692 - val_loss: 1.5493 - val_mae: 1.5493\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.7522 - mae: 1.7522 - val_loss: 1.5326 - val_mae: 1.5326\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5749 - mae: 1.5749 - val_loss: 1.5107 - val_mae: 1.5107\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5234 - mae: 1.5234 - val_loss: 1.4867 - val_mae: 1.4867\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5040 - mae: 1.5040 - val_loss: 1.4592 - val_mae: 1.4592\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 1.4396 - mae: 1.4396 - val_loss: 1.4307 - val_mae: 1.4307\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.3994 - mae: 1.3994 - val_loss: 1.3978 - val_mae: 1.3978\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3773 - mae: 1.3773 - val_loss: 1.3658 - val_mae: 1.3658\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3133 - mae: 1.3133 - val_loss: 1.3278 - val_mae: 1.3278\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2816 - mae: 1.2816 - val_loss: 1.2887 - val_mae: 1.2887\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2512 - mae: 1.2512 - val_loss: 1.2498 - val_mae: 1.2498\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2283 - mae: 1.2283 - val_loss: 1.2123 - val_mae: 1.2123\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1989 - mae: 1.1989 - val_loss: 1.1737 - val_mae: 1.1737\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1444 - mae: 1.1444 - val_loss: 1.1423 - val_mae: 1.1423\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1078 - mae: 1.1078 - val_loss: 1.1070 - val_mae: 1.1070\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0700 - mae: 1.0700 - val_loss: 1.0704 - val_mae: 1.0704\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0335 - mae: 1.0335 - val_loss: 1.0333 - val_mae: 1.0333\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0188 - mae: 1.0188 - val_loss: 0.9951 - val_mae: 0.9951\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.9611 - mae: 0.9611 - val_loss: 0.9558 - val_mae: 0.9558\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.9276 - mae: 0.9276 - val_loss: 0.9140 - val_mae: 0.9140\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8863 - mae: 0.8863 - val_loss: 0.8802 - val_mae: 0.8802\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8778 - mae: 0.8778 - val_loss: 0.8502 - val_mae: 0.8502\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 0.8870 - mae: 0.8870 - val_loss: 0.8202 - val_mae: 0.8202\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 0.8293 - mae: 0.8293 - val_loss: 0.7814 - val_mae: 0.7814\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7986 - mae: 0.7986 - val_loss: 0.7354 - val_mae: 0.7354\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7765 - mae: 0.7765 - val_loss: 0.6942 - val_mae: 0.6942\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7178 - mae: 0.7177 - val_loss: 0.6578 - val_mae: 0.6578\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.6877 - mae: 0.6877 - val_loss: 0.6233 - val_mae: 0.6233\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6281 - mae: 0.6281 - val_loss: 0.5888 - val_mae: 0.5888\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5853 - mae: 0.5853 - val_loss: 0.5618 - val_mae: 0.5618\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5815 - mae: 0.5815 - val_loss: 0.5448 - val_mae: 0.5448\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.5438 - mae: 0.5438 - val_loss: 0.5240 - val_mae: 0.5240\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5110 - mae: 0.5110 - val_loss: 0.4962 - val_mae: 0.4962\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4689 - mae: 0.4689 - val_loss: 0.4742 - val_mae: 0.4742\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4430 - mae: 0.4430 - val_loss: 0.4608 - val_mae: 0.4608\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3963 - mae: 0.3963 - val_loss: 0.4417 - val_mae: 0.4417\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 279us/sample - loss: 0.4173 - mae: 0.4173 - val_loss: 0.4063 - val_mae: 0.4063\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4458 - mae: 0.4458 - val_loss: 0.3668 - val_mae: 0.3668\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4485 - mae: 0.4485 - val_loss: 0.3382 - val_mae: 0.3382\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3603 - mae: 0.3603 - val_loss: 0.3273 - val_mae: 0.3273\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4298 - mae: 0.4298 - val_loss: 0.3079 - val_mae: 0.3079\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3669 - mae: 0.3669 - val_loss: 0.2971 - val_mae: 0.2971\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.3015 - mae: 0.3015 - val_loss: 0.2824 - val_mae: 0.2824\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3493 - mae: 0.3493 - val_loss: 0.2669 - val_mae: 0.2669\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3314 - mae: 0.3314 - val_loss: 0.2702 - val_mae: 0.2702\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.2678 - mae: 0.2678 - val_loss: 0.2814 - val_mae: 0.2814\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.2814 - mae: 0.2814\n",
      "Val score is 0.2814372777938843\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 2.0080 - mae: 2.0080 - val_loss: 1.8326 - val_mae: 1.8326\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.9205 - mae: 1.9205 - val_loss: 1.7932 - val_mae: 1.7932\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.8233 - mae: 1.8233 - val_loss: 1.7571 - val_mae: 1.7571\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.7549 - mae: 1.7549 - val_loss: 1.7243 - val_mae: 1.7243\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7155 - mae: 1.7155 - val_loss: 1.6938 - val_mae: 1.6938\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6985 - mae: 1.6985 - val_loss: 1.6597 - val_mae: 1.6597\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6534 - mae: 1.6534 - val_loss: 1.6216 - val_mae: 1.6216\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6402 - mae: 1.6402 - val_loss: 1.5796 - val_mae: 1.5796\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5819 - mae: 1.5819 - val_loss: 1.5349 - val_mae: 1.5349\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5551 - mae: 1.5551 - val_loss: 1.4885 - val_mae: 1.4885\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5194 - mae: 1.5194 - val_loss: 1.4432 - val_mae: 1.4432\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4824 - mae: 1.4824 - val_loss: 1.3989 - val_mae: 1.3989\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4485 - mae: 1.4485 - val_loss: 1.3570 - val_mae: 1.3570\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4146 - mae: 1.4146 - val_loss: 1.3152 - val_mae: 1.3152\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3802 - mae: 1.3802 - val_loss: 1.2727 - val_mae: 1.2727\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3651 - mae: 1.3651 - val_loss: 1.2306 - val_mae: 1.2306\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3142 - mae: 1.3142 - val_loss: 1.1856 - val_mae: 1.1856\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2651 - mae: 1.2651 - val_loss: 1.1480 - val_mae: 1.1480\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.2521 - mae: 1.2521 - val_loss: 1.1074 - val_mae: 1.1074\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.2156 - mae: 1.2156 - val_loss: 1.0664 - val_mae: 1.0664\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1679 - mae: 1.1679 - val_loss: 1.0251 - val_mae: 1.0251\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1300 - mae: 1.1300 - val_loss: 0.9844 - val_mae: 0.9844\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.0872 - mae: 1.0872 - val_loss: 0.9438 - val_mae: 0.9438\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0576 - mae: 1.0576 - val_loss: 0.9037 - val_mae: 0.9037\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.0022 - mae: 1.0022 - val_loss: 0.8646 - val_mae: 0.8646\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9646 - mae: 0.9646 - val_loss: 0.8210 - val_mae: 0.8210\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9247 - mae: 0.9247 - val_loss: 0.7779 - val_mae: 0.7779\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9228 - mae: 0.9228 - val_loss: 0.7437 - val_mae: 0.7437\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.8346 - mae: 0.8346 - val_loss: 0.7086 - val_mae: 0.7086\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8107 - mae: 0.8107 - val_loss: 0.6609 - val_mae: 0.6609\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 450us/sample - loss: 0.7692 - mae: 0.7692 - val_loss: 0.6014 - val_mae: 0.6014\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7399 - mae: 0.7399 - val_loss: 0.5483 - val_mae: 0.5483\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7032 - mae: 0.7032 - val_loss: 0.5071 - val_mae: 0.5071\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6552 - mae: 0.6552 - val_loss: 0.4808 - val_mae: 0.4808\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6560 - mae: 0.6560 - val_loss: 0.4637 - val_mae: 0.4637\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6210 - mae: 0.6210 - val_loss: 0.4380 - val_mae: 0.4380\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5966 - mae: 0.5966 - val_loss: 0.4037 - val_mae: 0.4037\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 0.5502 - mae: 0.5502 - val_loss: 0.3655 - val_mae: 0.3655\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 0.5162 - mae: 0.5162 - val_loss: 0.3382 - val_mae: 0.3382\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4196 - mae: 0.4196 - val_loss: 0.3133 - val_mae: 0.3133\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 0.4542 - mae: 0.4542 - val_loss: 0.3090 - val_mae: 0.3090\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4316 - mae: 0.4316 - val_loss: 0.2941 - val_mae: 0.2941\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3927 - mae: 0.3927 - val_loss: 0.2754 - val_mae: 0.2754\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 0.4163 - mae: 0.4163 - val_loss: 0.2560 - val_mae: 0.2560\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3803 - mae: 0.3803 - val_loss: 0.2306 - val_mae: 0.2306\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4300 - mae: 0.4300 - val_loss: 0.2135 - val_mae: 0.2135\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4149 - mae: 0.4149 - val_loss: 0.2105 - val_mae: 0.2105\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3696 - mae: 0.3696 - val_loss: 0.1967 - val_mae: 0.1967\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3362 - mae: 0.3362 - val_loss: 0.2037 - val_mae: 0.2037\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3523 - mae: 0.3523 - val_loss: 0.2099 - val_mae: 0.2099\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.2099 - mae: 0.2099\n",
      "Val score is 0.20994937419891357\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 1.9175 - mae: 1.9175 - val_loss: 1.7464 - val_mae: 1.7464\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 1.8220 - mae: 1.8220 - val_loss: 1.7116 - val_mae: 1.7116\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7756 - mae: 1.7756 - val_loss: 1.6780 - val_mae: 1.6780\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7463 - mae: 1.7463 - val_loss: 1.6467 - val_mae: 1.6467\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7215 - mae: 1.7215 - val_loss: 1.6192 - val_mae: 1.6192\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6913 - mae: 1.6913 - val_loss: 1.5966 - val_mae: 1.5966\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 1.6697 - mae: 1.6697 - val_loss: 1.5764 - val_mae: 1.5764\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 1.6325 - mae: 1.6325 - val_loss: 1.5538 - val_mae: 1.5538\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.6109 - mae: 1.6109 - val_loss: 1.5319 - val_mae: 1.5319\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5740 - mae: 1.5740 - val_loss: 1.5092 - val_mae: 1.5092\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.5439 - mae: 1.5439 - val_loss: 1.4849 - val_mae: 1.4849\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5131 - mae: 1.5131 - val_loss: 1.4601 - val_mae: 1.4601\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4815 - mae: 1.4815 - val_loss: 1.4341 - val_mae: 1.4341\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.4524 - mae: 1.4524 - val_loss: 1.4075 - val_mae: 1.4075\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4163 - mae: 1.4163 - val_loss: 1.3780 - val_mae: 1.3780\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3853 - mae: 1.3853 - val_loss: 1.3549 - val_mae: 1.3549\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3572 - mae: 1.3572 - val_loss: 1.3278 - val_mae: 1.3278\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 1.3201 - mae: 1.3201 - val_loss: 1.2929 - val_mae: 1.2929\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2948 - mae: 1.2948 - val_loss: 1.2504 - val_mae: 1.2504\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 1.2567 - mae: 1.2567 - val_loss: 1.2086 - val_mae: 1.2086\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2084 - mae: 1.2084 - val_loss: 1.1636 - val_mae: 1.1636\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 1.1855 - mae: 1.1855 - val_loss: 1.1175 - val_mae: 1.1175\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1470 - mae: 1.1470 - val_loss: 1.0670 - val_mae: 1.0670\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0964 - mae: 1.0964 - val_loss: 1.0117 - val_mae: 1.0117\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0548 - mae: 1.0548 - val_loss: 0.9622 - val_mae: 0.9622\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.0081 - mae: 1.0081 - val_loss: 0.9162 - val_mae: 0.9162\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0143 - mae: 1.0143 - val_loss: 0.8700 - val_mae: 0.8700\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9485 - mae: 0.9485 - val_loss: 0.8307 - val_mae: 0.8307\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9025 - mae: 0.9025 - val_loss: 0.7898 - val_mae: 0.7898\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9019 - mae: 0.9019 - val_loss: 0.7509 - val_mae: 0.7509\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.8001 - mae: 0.8001 - val_loss: 0.7111 - val_mae: 0.7111\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7963 - mae: 0.7963 - val_loss: 0.6692 - val_mae: 0.6692\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7248 - mae: 0.7248 - val_loss: 0.6301 - val_mae: 0.6301\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6686 - mae: 0.6686 - val_loss: 0.5927 - val_mae: 0.5927\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7033 - mae: 0.7033 - val_loss: 0.5621 - val_mae: 0.5621\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6631 - mae: 0.6631 - val_loss: 0.5208 - val_mae: 0.5208\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5905 - mae: 0.5905 - val_loss: 0.4808 - val_mae: 0.4808\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5643 - mae: 0.5643 - val_loss: 0.4404 - val_mae: 0.4404\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5025 - mae: 0.5025 - val_loss: 0.4119 - val_mae: 0.4119\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5257 - mae: 0.5257 - val_loss: 0.3896 - val_mae: 0.3896\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4685 - mae: 0.4685 - val_loss: 0.3576 - val_mae: 0.3576\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4291 - mae: 0.4291 - val_loss: 0.3210 - val_mae: 0.3210\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4432 - mae: 0.4432 - val_loss: 0.2877 - val_mae: 0.2877\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4590 - mae: 0.4590 - val_loss: 0.2626 - val_mae: 0.2626\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4009 - mae: 0.4009 - val_loss: 0.2393 - val_mae: 0.2393\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.3090 - mae: 0.3090 - val_loss: 0.2218 - val_mae: 0.2218\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3401 - mae: 0.3401 - val_loss: 0.2272 - val_mae: 0.2272\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.3491 - mae: 0.3491 - val_loss: 0.2425 - val_mae: 0.2425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 100us/sample - loss: 0.2425 - mae: 0.2425\n",
      "Val score is 0.24248361587524414\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 17ms/sample - loss: 1.9410 - mae: 1.9410 - val_loss: 1.4493 - val_mae: 1.4493\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 1.7312 - mae: 1.7312 - val_loss: 1.4219 - val_mae: 1.4219\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 1.6377 - mae: 1.6377 - val_loss: 1.3989 - val_mae: 1.3989\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5238 - mae: 1.5238 - val_loss: 1.3738 - val_mae: 1.3738\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4503 - mae: 1.4503 - val_loss: 1.3451 - val_mae: 1.3451\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3939 - mae: 1.3939 - val_loss: 1.3186 - val_mae: 1.3186\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3581 - mae: 1.3581 - val_loss: 1.2934 - val_mae: 1.2934\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.3228 - mae: 1.3228 - val_loss: 1.2657 - val_mae: 1.2657\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2923 - mae: 1.2923 - val_loss: 1.2377 - val_mae: 1.2377\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 1.2668 - mae: 1.2668 - val_loss: 1.2079 - val_mae: 1.2079\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 1.2433 - mae: 1.2433 - val_loss: 1.1756 - val_mae: 1.1756\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.2048 - mae: 1.2048 - val_loss: 1.1435 - val_mae: 1.1435\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.1761 - mae: 1.1761 - val_loss: 1.1146 - val_mae: 1.1146\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1497 - mae: 1.1497 - val_loss: 1.0843 - val_mae: 1.0843\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1374 - mae: 1.1374 - val_loss: 1.0583 - val_mae: 1.0583\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 278us/sample - loss: 1.1132 - mae: 1.1132 - val_loss: 1.0412 - val_mae: 1.0412\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0487 - mae: 1.0487 - val_loss: 1.0188 - val_mae: 1.0188\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 1.0445 - mae: 1.0445 - val_loss: 0.9965 - val_mae: 0.9965\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0232 - mae: 1.0232 - val_loss: 0.9735 - val_mae: 0.9735\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9752 - mae: 0.9752 - val_loss: 0.9500 - val_mae: 0.9500\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 0.9501 - mae: 0.9501 - val_loss: 0.9224 - val_mae: 0.9224\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8983 - mae: 0.8983 - val_loss: 0.8914 - val_mae: 0.8914\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8833 - mae: 0.8833 - val_loss: 0.8576 - val_mae: 0.8576\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8564 - mae: 0.8564 - val_loss: 0.8220 - val_mae: 0.8220\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8466 - mae: 0.8466 - val_loss: 0.7907 - val_mae: 0.7907\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7809 - mae: 0.7809 - val_loss: 0.7609 - val_mae: 0.7609\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7446 - mae: 0.7446 - val_loss: 0.7315 - val_mae: 0.7315\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.7161 - mae: 0.7161 - val_loss: 0.7020 - val_mae: 0.7020\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7084 - mae: 0.7084 - val_loss: 0.6748 - val_mae: 0.6748\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6259 - mae: 0.6259 - val_loss: 0.6511 - val_mae: 0.6511\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.6243 - mae: 0.6243 - val_loss: 0.6193 - val_mae: 0.6193\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5731 - mae: 0.5731 - val_loss: 0.5879 - val_mae: 0.5879\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5891 - mae: 0.5891 - val_loss: 0.5644 - val_mae: 0.5644\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5095 - mae: 0.5095 - val_loss: 0.5441 - val_mae: 0.5441\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4868 - mae: 0.4868 - val_loss: 0.5208 - val_mae: 0.5208\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5158 - mae: 0.5158 - val_loss: 0.4963 - val_mae: 0.4963\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4732 - mae: 0.4732 - val_loss: 0.4763 - val_mae: 0.4763\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4078 - mae: 0.4078 - val_loss: 0.4522 - val_mae: 0.4522\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4175 - mae: 0.4175 - val_loss: 0.4247 - val_mae: 0.4247\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4094 - mae: 0.4094 - val_loss: 0.4068 - val_mae: 0.4068\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3850 - mae: 0.3850 - val_loss: 0.3942 - val_mae: 0.3942\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3370 - mae: 0.3370 - val_loss: 0.3973 - val_mae: 0.3973\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.2991 - mae: 0.2991 - val_loss: 0.3961 - val_mae: 0.3961\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.3961 - mae: 0.3961\n",
      "Val score is 0.39607876539230347\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 2.1528 - mae: 2.1528 - val_loss: 1.4901 - val_mae: 1.4901\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.8777 - mae: 1.8777 - val_loss: 1.4719 - val_mae: 1.4719\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.8762 - mae: 1.8762 - val_loss: 1.4433 - val_mae: 1.4433\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.7607 - mae: 1.7607 - val_loss: 1.4125 - val_mae: 1.4125\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.7460 - mae: 1.7460 - val_loss: 1.3790 - val_mae: 1.3790\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.6879 - mae: 1.6879 - val_loss: 1.3438 - val_mae: 1.3438\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 1.6118 - mae: 1.6118 - val_loss: 1.3039 - val_mae: 1.3039\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 1.5940 - mae: 1.5940 - val_loss: 1.2646 - val_mae: 1.2646\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5566 - mae: 1.5566 - val_loss: 1.2244 - val_mae: 1.2244\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.5101 - mae: 1.5101 - val_loss: 1.1830 - val_mae: 1.1830\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4769 - mae: 1.4769 - val_loss: 1.1416 - val_mae: 1.1416\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4451 - mae: 1.4451 - val_loss: 1.1023 - val_mae: 1.1023\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4049 - mae: 1.4049 - val_loss: 1.0602 - val_mae: 1.0602\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.3769 - mae: 1.3769 - val_loss: 1.0197 - val_mae: 1.0197\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 258us/sample - loss: 1.3389 - mae: 1.3389 - val_loss: 0.9807 - val_mae: 0.9807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3065 - mae: 1.3065 - val_loss: 0.9453 - val_mae: 0.9453\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 1.2803 - mae: 1.2803 - val_loss: 0.9197 - val_mae: 0.9197\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2308 - mae: 1.2308 - val_loss: 0.8902 - val_mae: 0.8902\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1886 - mae: 1.1886 - val_loss: 0.8606 - val_mae: 0.8606\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.1826 - mae: 1.1826 - val_loss: 0.8302 - val_mae: 0.8302\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1264 - mae: 1.1264 - val_loss: 0.8009 - val_mae: 0.8009\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1191 - mae: 1.1191 - val_loss: 0.7702 - val_mae: 0.7702\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 1.0668 - mae: 1.0668 - val_loss: 0.7337 - val_mae: 0.7337\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0094 - mae: 1.0094 - val_loss: 0.6935 - val_mae: 0.6935\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9809 - mae: 0.9809 - val_loss: 0.6501 - val_mae: 0.6501\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9322 - mae: 0.9322 - val_loss: 0.6052 - val_mae: 0.6052\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9148 - mae: 0.9148 - val_loss: 0.5665 - val_mae: 0.5665\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8544 - mae: 0.8544 - val_loss: 0.5338 - val_mae: 0.5338\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8397 - mae: 0.8397 - val_loss: 0.5062 - val_mae: 0.5062\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7990 - mae: 0.7990 - val_loss: 0.4823 - val_mae: 0.4823\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7359 - mae: 0.7359 - val_loss: 0.4586 - val_mae: 0.4586\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7533 - mae: 0.7533 - val_loss: 0.4317 - val_mae: 0.4317\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 0.6618 - mae: 0.6618 - val_loss: 0.4004 - val_mae: 0.4004\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6410 - mae: 0.6410 - val_loss: 0.3664 - val_mae: 0.3664\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 0.6440 - mae: 0.6440 - val_loss: 0.3438 - val_mae: 0.3438\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5787 - mae: 0.5787 - val_loss: 0.3217 - val_mae: 0.3217\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5502 - mae: 0.5502 - val_loss: 0.3024 - val_mae: 0.3024\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5438 - mae: 0.5438 - val_loss: 0.2956 - val_mae: 0.2956\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5088 - mae: 0.5088 - val_loss: 0.2998 - val_mae: 0.2998\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4579 - mae: 0.4579 - val_loss: 0.3019 - val_mae: 0.3019\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 0.3019 - mae: 0.3019\n",
      "Val score is 0.3019044101238251\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 1.7613 - mae: 1.7613 - val_loss: 2.0452 - val_mae: 2.0452\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6967 - mae: 1.6967 - val_loss: 1.9817 - val_mae: 1.9817\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 257us/sample - loss: 1.6467 - mae: 1.6467 - val_loss: 1.9159 - val_mae: 1.9159\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.5985 - mae: 1.5985 - val_loss: 1.8490 - val_mae: 1.8490\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 264us/sample - loss: 1.5691 - mae: 1.5691 - val_loss: 1.7883 - val_mae: 1.7883\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5340 - mae: 1.5340 - val_loss: 1.7340 - val_mae: 1.7340\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5079 - mae: 1.5079 - val_loss: 1.6795 - val_mae: 1.6795\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4874 - mae: 1.4874 - val_loss: 1.6287 - val_mae: 1.6287\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4451 - mae: 1.4451 - val_loss: 1.5791 - val_mae: 1.5791\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4181 - mae: 1.4181 - val_loss: 1.5274 - val_mae: 1.5274\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.3906 - mae: 1.3906 - val_loss: 1.4735 - val_mae: 1.4735\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 1.3652 - mae: 1.3652 - val_loss: 1.4201 - val_mae: 1.4201\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3212 - mae: 1.3212 - val_loss: 1.3661 - val_mae: 1.3661\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3033 - mae: 1.3033 - val_loss: 1.3129 - val_mae: 1.3129\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2615 - mae: 1.2615 - val_loss: 1.2558 - val_mae: 1.2558\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.2405 - mae: 1.2405 - val_loss: 1.1957 - val_mae: 1.1957\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1900 - mae: 1.1900 - val_loss: 1.1459 - val_mae: 1.1459\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1620 - mae: 1.1620 - val_loss: 1.0977 - val_mae: 1.0977\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1346 - mae: 1.1346 - val_loss: 1.0435 - val_mae: 1.0435\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 279us/sample - loss: 1.0893 - mae: 1.0893 - val_loss: 0.9896 - val_mae: 0.9896\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.0512 - mae: 1.0512 - val_loss: 0.9384 - val_mae: 0.9384\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0382 - mae: 1.0382 - val_loss: 0.8869 - val_mae: 0.8869\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9903 - mae: 0.9903 - val_loss: 0.8355 - val_mae: 0.8355\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9388 - mae: 0.9388 - val_loss: 0.7901 - val_mae: 0.7901\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 272us/sample - loss: 0.9317 - mae: 0.9317 - val_loss: 0.7413 - val_mae: 0.7413\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.8791 - mae: 0.8791 - val_loss: 0.6915 - val_mae: 0.6915\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.8601 - mae: 0.8601 - val_loss: 0.6410 - val_mae: 0.6410\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 0.7806 - mae: 0.7806 - val_loss: 0.5994 - val_mae: 0.5994\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7692 - mae: 0.7692 - val_loss: 0.5517 - val_mae: 0.5517\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7243 - mae: 0.7243 - val_loss: 0.5212 - val_mae: 0.5212\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6843 - mae: 0.6843 - val_loss: 0.4913 - val_mae: 0.4913\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6627 - mae: 0.6627 - val_loss: 0.4568 - val_mae: 0.4568\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.6229 - mae: 0.6229 - val_loss: 0.4365 - val_mae: 0.4365\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5923 - mae: 0.5923 - val_loss: 0.4232 - val_mae: 0.4232\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5411 - mae: 0.5411 - val_loss: 0.3901 - val_mae: 0.3901\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5083 - mae: 0.5083 - val_loss: 0.3574 - val_mae: 0.3574\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4931 - mae: 0.4931 - val_loss: 0.3239 - val_mae: 0.3239\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4653 - mae: 0.4653 - val_loss: 0.3083 - val_mae: 0.3083\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4117 - mae: 0.4117 - val_loss: 0.2780 - val_mae: 0.2780\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3983 - mae: 0.3983 - val_loss: 0.2694 - val_mae: 0.2694\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3856 - mae: 0.3856 - val_loss: 0.2848 - val_mae: 0.2848\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3529 - mae: 0.3529 - val_loss: 0.2969 - val_mae: 0.2969\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.2969 - mae: 0.2969\n",
      "Val score is 0.29690060019493103\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 17ms/sample - loss: 1.9258 - mae: 1.9258 - val_loss: 1.6558 - val_mae: 1.6558\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.8374 - mae: 1.8374 - val_loss: 1.6258 - val_mae: 1.6258\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 1.8139 - mae: 1.8139 - val_loss: 1.6010 - val_mae: 1.6010\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.7648 - mae: 1.7648 - val_loss: 1.5768 - val_mae: 1.5768\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7402 - mae: 1.7402 - val_loss: 1.5528 - val_mae: 1.5528\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7117 - mae: 1.7117 - val_loss: 1.5301 - val_mae: 1.5301\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.6843 - mae: 1.6843 - val_loss: 1.5046 - val_mae: 1.5046\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6678 - mae: 1.6678 - val_loss: 1.4717 - val_mae: 1.4717\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.6258 - mae: 1.6258 - val_loss: 1.4364 - val_mae: 1.4364\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5971 - mae: 1.5971 - val_loss: 1.4041 - val_mae: 1.4041\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.5687 - mae: 1.5687 - val_loss: 1.3737 - val_mae: 1.3737\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.5417 - mae: 1.5417 - val_loss: 1.3479 - val_mae: 1.3479\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 1.5210 - mae: 1.5210 - val_loss: 1.3254 - val_mae: 1.3254\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4799 - mae: 1.4799 - val_loss: 1.3012 - val_mae: 1.3012\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 273us/sample - loss: 1.4539 - mae: 1.4539 - val_loss: 1.2774 - val_mae: 1.2774\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 1.4213 - mae: 1.4213 - val_loss: 1.2532 - val_mae: 1.2532\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3843 - mae: 1.3843 - val_loss: 1.2206 - val_mae: 1.2206\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3587 - mae: 1.3587 - val_loss: 1.1821 - val_mae: 1.1821\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 1.3424 - mae: 1.3424 - val_loss: 1.1449 - val_mae: 1.1449\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2939 - mae: 1.2939 - val_loss: 1.1098 - val_mae: 1.1098\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.2472 - mae: 1.2472 - val_loss: 1.0755 - val_mae: 1.0755\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 1.2353 - mae: 1.2353 - val_loss: 1.0399 - val_mae: 1.0399\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 1.1756 - mae: 1.1756 - val_loss: 1.0043 - val_mae: 1.0043\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1338 - mae: 1.1338 - val_loss: 0.9659 - val_mae: 0.9659\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.0926 - mae: 1.0926 - val_loss: 0.9262 - val_mae: 0.9262\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.0527 - mae: 1.0527 - val_loss: 0.8835 - val_mae: 0.8835\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.0118 - mae: 1.0118 - val_loss: 0.8401 - val_mae: 0.8401\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 0.9604 - mae: 0.9604 - val_loss: 0.7946 - val_mae: 0.7946\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9197 - mae: 0.9197 - val_loss: 0.7488 - val_mae: 0.7488\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 266us/sample - loss: 0.8840 - mae: 0.8840 - val_loss: 0.6998 - val_mae: 0.6998\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.8517 - mae: 0.8517 - val_loss: 0.6460 - val_mae: 0.6460\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7884 - mae: 0.7884 - val_loss: 0.5914 - val_mae: 0.5914\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7371 - mae: 0.7371 - val_loss: 0.5428 - val_mae: 0.5428\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6860 - mae: 0.6860 - val_loss: 0.4961 - val_mae: 0.4961\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6755 - mae: 0.6755 - val_loss: 0.4480 - val_mae: 0.4480\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6072 - mae: 0.6072 - val_loss: 0.4113 - val_mae: 0.4113\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5675 - mae: 0.5675 - val_loss: 0.3813 - val_mae: 0.3813\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5740 - mae: 0.5740 - val_loss: 0.3448 - val_mae: 0.3448\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5324 - mae: 0.5324 - val_loss: 0.3206 - val_mae: 0.3206\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4764 - mae: 0.4764 - val_loss: 0.2898 - val_mae: 0.2898\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4545 - mae: 0.4545 - val_loss: 0.2514 - val_mae: 0.2514\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4513 - mae: 0.4513 - val_loss: 0.2336 - val_mae: 0.2336\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 0.3988 - mae: 0.3988 - val_loss: 0.2558 - val_mae: 0.2558\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4187 - mae: 0.4187 - val_loss: 0.2852 - val_mae: 0.2852\n",
      "80/80 [==============================] - 0s 98us/sample - loss: 0.2852 - mae: 0.2852\n",
      "Val score is 0.285169392824173\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 1.9544 - mae: 1.9544 - val_loss: 1.6529 - val_mae: 1.6529\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.9005 - mae: 1.9005 - val_loss: 1.6294 - val_mae: 1.6294\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.8236 - mae: 1.8236 - val_loss: 1.6074 - val_mae: 1.6074\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7954 - mae: 1.7954 - val_loss: 1.5850 - val_mae: 1.5850\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.7756 - mae: 1.7756 - val_loss: 1.5603 - val_mae: 1.5603\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7522 - mae: 1.7522 - val_loss: 1.5378 - val_mae: 1.5378\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7248 - mae: 1.7248 - val_loss: 1.5190 - val_mae: 1.5190\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6967 - mae: 1.6967 - val_loss: 1.4985 - val_mae: 1.4985\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6763 - mae: 1.6763 - val_loss: 1.4764 - val_mae: 1.4764\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.6462 - mae: 1.6462 - val_loss: 1.4533 - val_mae: 1.4533\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 266us/sample - loss: 1.6200 - mae: 1.6200 - val_loss: 1.4318 - val_mae: 1.4318\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.6012 - mae: 1.6012 - val_loss: 1.4098 - val_mae: 1.4098\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5708 - mae: 1.5708 - val_loss: 1.3871 - val_mae: 1.3871\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 1.5391 - mae: 1.5391 - val_loss: 1.3638 - val_mae: 1.3638\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.5110 - mae: 1.5110 - val_loss: 1.3391 - val_mae: 1.3391\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4925 - mae: 1.4925 - val_loss: 1.3123 - val_mae: 1.3123\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4598 - mae: 1.4598 - val_loss: 1.2808 - val_mae: 1.2808\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.4210 - mae: 1.4210 - val_loss: 1.2487 - val_mae: 1.2487\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4135 - mae: 1.4135 - val_loss: 1.2176 - val_mae: 1.2176\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 1.3634 - mae: 1.3634 - val_loss: 1.1869 - val_mae: 1.1869\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3468 - mae: 1.3468 - val_loss: 1.1629 - val_mae: 1.1629\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.3075 - mae: 1.3075 - val_loss: 1.1388 - val_mae: 1.1388\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2558 - mae: 1.2558 - val_loss: 1.1073 - val_mae: 1.1073\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 1.2197 - mae: 1.2197 - val_loss: 1.0738 - val_mae: 1.0738\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1938 - mae: 1.1938 - val_loss: 1.0354 - val_mae: 1.0354\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1599 - mae: 1.1599 - val_loss: 0.9919 - val_mae: 0.9919\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.1146 - mae: 1.1146 - val_loss: 0.9504 - val_mae: 0.9504\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0800 - mae: 1.0800 - val_loss: 0.9085 - val_mae: 0.9085\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0329 - mae: 1.0329 - val_loss: 0.8664 - val_mae: 0.8664\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.9867 - mae: 0.9867 - val_loss: 0.8220 - val_mae: 0.8220\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 0.9597 - mae: 0.9597 - val_loss: 0.7787 - val_mae: 0.7787\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9518 - mae: 0.9518 - val_loss: 0.7410 - val_mae: 0.7410\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8632 - mae: 0.8632 - val_loss: 0.7063 - val_mae: 0.7063\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8656 - mae: 0.8656 - val_loss: 0.6705 - val_mae: 0.6705\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8026 - mae: 0.8026 - val_loss: 0.6377 - val_mae: 0.6377\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.7891 - mae: 0.7891 - val_loss: 0.6042 - val_mae: 0.6042\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7337 - mae: 0.7337 - val_loss: 0.5651 - val_mae: 0.5651\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6568 - mae: 0.6568 - val_loss: 0.5334 - val_mae: 0.5334\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6508 - mae: 0.6508 - val_loss: 0.4951 - val_mae: 0.4951\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5954 - mae: 0.5954 - val_loss: 0.4534 - val_mae: 0.4534\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 0.5488 - mae: 0.5488 - val_loss: 0.4087 - val_mae: 0.4087\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 0.5326 - mae: 0.5326 - val_loss: 0.3759 - val_mae: 0.3759\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4824 - mae: 0.4824 - val_loss: 0.3378 - val_mae: 0.3378\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4867 - mae: 0.4867 - val_loss: 0.3077 - val_mae: 0.3077\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4966 - mae: 0.4966 - val_loss: 0.2865 - val_mae: 0.2865\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4331 - mae: 0.4331 - val_loss: 0.2606 - val_mae: 0.2606\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3981 - mae: 0.3981 - val_loss: 0.2521 - val_mae: 0.2521\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4075 - mae: 0.4075 - val_loss: 0.2350 - val_mae: 0.2350\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4218 - mae: 0.4218 - val_loss: 0.2197 - val_mae: 0.2197\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3326 - mae: 0.3326 - val_loss: 0.2144 - val_mae: 0.2144\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3155 - mae: 0.3155 - val_loss: 0.2010 - val_mae: 0.2010\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.3588 - mae: 0.3588 - val_loss: 0.1907 - val_mae: 0.1907\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3654 - mae: 0.3654 - val_loss: 0.2059 - val_mae: 0.2059\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3081 - mae: 0.3081 - val_loss: 0.1933 - val_mae: 0.1933\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 0.1933 - mae: 0.1933\n",
      "Val score is 0.19325068593025208\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 1.5490 - mae: 1.5490 - val_loss: 1.2329 - val_mae: 1.2329\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4398 - mae: 1.4398 - val_loss: 1.1771 - val_mae: 1.1771\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3307 - mae: 1.3307 - val_loss: 1.1289 - val_mae: 1.1289\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2995 - mae: 1.2995 - val_loss: 1.0763 - val_mae: 1.0763\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 274us/sample - loss: 1.1910 - mae: 1.1910 - val_loss: 1.0216 - val_mae: 1.0216\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.1997 - mae: 1.1997 - val_loss: 0.9719 - val_mae: 0.9719\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 1.1454 - mae: 1.1454 - val_loss: 0.9286 - val_mae: 0.9286\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 1.0930 - mae: 1.0930 - val_loss: 0.8855 - val_mae: 0.8855\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0610 - mae: 1.0610 - val_loss: 0.8451 - val_mae: 0.8451\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.9843 - mae: 0.9843 - val_loss: 0.8069 - val_mae: 0.8069\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.9209 - mae: 0.9209 - val_loss: 0.7699 - val_mae: 0.7699\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 412us/sample - loss: 0.9493 - mae: 0.9493 - val_loss: 0.7393 - val_mae: 0.7393\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 0.8646 - mae: 0.8646 - val_loss: 0.7108 - val_mae: 0.7108\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 0.8435 - mae: 0.8435 - val_loss: 0.6808 - val_mae: 0.6808\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.8326 - mae: 0.8326 - val_loss: 0.6552 - val_mae: 0.6552\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 375us/sample - loss: 0.8371 - mae: 0.8371 - val_loss: 0.6342 - val_mae: 0.6342\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 362us/sample - loss: 0.7589 - mae: 0.7589 - val_loss: 0.6126 - val_mae: 0.6126\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 387us/sample - loss: 0.7327 - mae: 0.7327 - val_loss: 0.5957 - val_mae: 0.5957\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 350us/sample - loss: 0.6941 - mae: 0.6941 - val_loss: 0.5746 - val_mae: 0.5746\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6967 - mae: 0.6967 - val_loss: 0.5552 - val_mae: 0.5552\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6533 - mae: 0.6533 - val_loss: 0.5334 - val_mae: 0.5334\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6482 - mae: 0.6482 - val_loss: 0.5062 - val_mae: 0.5062\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6030 - mae: 0.6030 - val_loss: 0.4866 - val_mae: 0.4866\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 279us/sample - loss: 0.5709 - mae: 0.5709 - val_loss: 0.4728 - val_mae: 0.4728\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5574 - mae: 0.5574 - val_loss: 0.4686 - val_mae: 0.4686\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5731 - mae: 0.5731 - val_loss: 0.4554 - val_mae: 0.4554\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.5313 - mae: 0.5313 - val_loss: 0.4398 - val_mae: 0.4398\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5185 - mae: 0.5185 - val_loss: 0.4198 - val_mae: 0.4198\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5235 - mae: 0.5235 - val_loss: 0.4063 - val_mae: 0.4063\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4622 - mae: 0.4622 - val_loss: 0.3999 - val_mae: 0.3999\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4682 - mae: 0.4682 - val_loss: 0.3955 - val_mae: 0.3955\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4383 - mae: 0.4383 - val_loss: 0.3880 - val_mae: 0.3880\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4296 - mae: 0.4296 - val_loss: 0.3772 - val_mae: 0.3772\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3880 - mae: 0.3880 - val_loss: 0.3677 - val_mae: 0.3677\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.3893 - mae: 0.3893 - val_loss: 0.3622 - val_mae: 0.3622\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3757 - mae: 0.3757 - val_loss: 0.3650 - val_mae: 0.3650\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 266us/sample - loss: 0.3968 - mae: 0.3968 - val_loss: 0.3689 - val_mae: 0.3689\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.3689 - mae: 0.3689\n",
      "Val score is 0.36890774965286255\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 17ms/sample - loss: 102.9047 - mae: 102.9047 - val_loss: 102.7566 - val_mae: 102.7566\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 102.8752 - mae: 102.8752 - val_loss: 102.7276 - val_mae: 102.7276\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 102.8448 - mae: 102.8448 - val_loss: 102.6997 - val_mae: 102.6997\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 102.8134 - mae: 102.8134 - val_loss: 102.6691 - val_mae: 102.6691\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 284us/sample - loss: 102.7810 - mae: 102.7810 - val_loss: 102.6366 - val_mae: 102.6366\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.7474 - mae: 102.7474 - val_loss: 102.6040 - val_mae: 102.6040\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 102.7126 - mae: 102.7126 - val_loss: 102.5710 - val_mae: 102.5710\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.6765 - mae: 102.6765 - val_loss: 102.5346 - val_mae: 102.5346\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.6390 - mae: 102.6391 - val_loss: 102.4990 - val_mae: 102.4990\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 102.6001 - mae: 102.6001 - val_loss: 102.4618 - val_mae: 102.4618\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 102.5597 - mae: 102.5597 - val_loss: 102.4245 - val_mae: 102.4245\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 102.5176 - mae: 102.5176 - val_loss: 102.3870 - val_mae: 102.3870\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 102.4740 - mae: 102.4740 - val_loss: 102.3474 - val_mae: 102.3474\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.4286 - mae: 102.4286 - val_loss: 102.3055 - val_mae: 102.3055\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 102.3815 - mae: 102.3815 - val_loss: 102.2641 - val_mae: 102.2641\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.3326 - mae: 102.3326 - val_loss: 102.2189 - val_mae: 102.2189\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.2819 - mae: 102.2819 - val_loss: 102.1726 - val_mae: 102.1726\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.2293 - mae: 102.2293 - val_loss: 102.1228 - val_mae: 102.1228\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 102.1748 - mae: 102.1748 - val_loss: 102.0716 - val_mae: 102.0716\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.1184 - mae: 102.1184 - val_loss: 102.0181 - val_mae: 102.0181\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 102.0600 - mae: 102.0600 - val_loss: 101.9644 - val_mae: 101.9644\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 101.9997 - mae: 101.9997 - val_loss: 101.9078 - val_mae: 101.9078\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 101.9373 - mae: 101.9373 - val_loss: 101.8492 - val_mae: 101.8492\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 101.8729 - mae: 101.8729 - val_loss: 101.7907 - val_mae: 101.7907\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.8064 - mae: 101.8064 - val_loss: 101.7291 - val_mae: 101.7291\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.7379 - mae: 101.7379 - val_loss: 101.6664 - val_mae: 101.6664\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 101.6673 - mae: 101.6673 - val_loss: 101.6012 - val_mae: 101.6012\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.5945 - mae: 101.5945 - val_loss: 101.5337 - val_mae: 101.5337\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 101.5197 - mae: 101.5197 - val_loss: 101.4653 - val_mae: 101.4653\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.4426 - mae: 101.4426 - val_loss: 101.3900 - val_mae: 101.3900\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.3635 - mae: 101.3634 - val_loss: 101.3132 - val_mae: 101.3132\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 262us/sample - loss: 101.2821 - mae: 101.2821 - val_loss: 101.2354 - val_mae: 101.2354\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 101.1986 - mae: 101.1986 - val_loss: 101.1566 - val_mae: 101.1566\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.1129 - mae: 101.1129 - val_loss: 101.0740 - val_mae: 101.0740\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 101.0249 - mae: 101.0249 - val_loss: 100.9896 - val_mae: 100.9896\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 276us/sample - loss: 100.9348 - mae: 100.9348 - val_loss: 100.9006 - val_mae: 100.9006\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 100.8424 - mae: 100.8424 - val_loss: 100.8126 - val_mae: 100.8126\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 100.7478 - mae: 100.7478 - val_loss: 100.7217 - val_mae: 100.7217\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 100.6509 - mae: 100.6509 - val_loss: 100.6276 - val_mae: 100.6276\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 100.5518 - mae: 100.5518 - val_loss: 100.5306 - val_mae: 100.5306\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 100.4505 - mae: 100.4505 - val_loss: 100.4317 - val_mae: 100.4317\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 100.3468 - mae: 100.3468 - val_loss: 100.3301 - val_mae: 100.3301\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 100.2409 - mae: 100.2409 - val_loss: 100.2295 - val_mae: 100.2295\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 100.1327 - mae: 100.1328 - val_loss: 100.1197 - val_mae: 100.1197\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 100.0223 - mae: 100.0223 - val_loss: 100.0123 - val_mae: 100.0123\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 99.9095 - mae: 99.9095 - val_loss: 99.8996 - val_mae: 99.8996\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 99.7945 - mae: 99.7945 - val_loss: 99.7865 - val_mae: 99.7866\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 99.6771 - mae: 99.6771 - val_loss: 99.6726 - val_mae: 99.6726\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 99.5575 - mae: 99.5575 - val_loss: 99.5575 - val_mae: 99.5575\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 99.4355 - mae: 99.4355 - val_loss: 99.4350 - val_mae: 99.4350\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 99.3112 - mae: 99.3112 - val_loss: 99.3155 - val_mae: 99.3155\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 99.1846 - mae: 99.1846 - val_loss: 99.1906 - val_mae: 99.1906\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 99.0557 - mae: 99.0557 - val_loss: 99.0649 - val_mae: 99.0649\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 98.9244 - mae: 98.9244 - val_loss: 98.9382 - val_mae: 98.9382\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 98.7909 - mae: 98.7909 - val_loss: 98.8083 - val_mae: 98.8083\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 98.6550 - mae: 98.6550 - val_loss: 98.6731 - val_mae: 98.6731\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 98.5167 - mae: 98.5167 - val_loss: 98.5419 - val_mae: 98.5419\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 98.3762 - mae: 98.3762 - val_loss: 98.4049 - val_mae: 98.4049\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 98.2333 - mae: 98.2333 - val_loss: 98.2677 - val_mae: 98.2677\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 98.0880 - mae: 98.0880 - val_loss: 98.1241 - val_mae: 98.1241\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 97.9404 - mae: 97.9404 - val_loss: 97.9737 - val_mae: 97.9737\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 97.7905 - mae: 97.7905 - val_loss: 97.8248 - val_mae: 97.8248\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 292us/sample - loss: 97.6382 - mae: 97.6382 - val_loss: 97.6720 - val_mae: 97.6720\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 97.4836 - mae: 97.4836 - val_loss: 97.5151 - val_mae: 97.5151\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 97.3267 - mae: 97.3266 - val_loss: 97.3661 - val_mae: 97.3661\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 97.1673 - mae: 97.1673 - val_loss: 97.2053 - val_mae: 97.2053\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 97.0057 - mae: 97.0057 - val_loss: 97.0472 - val_mae: 97.0472\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 96.8417 - mae: 96.8417 - val_loss: 96.8877 - val_mae: 96.8877\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 96.6753 - mae: 96.6753 - val_loss: 96.7218 - val_mae: 96.7218\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 96.5066 - mae: 96.5066 - val_loss: 96.5490 - val_mae: 96.5490\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 96.3355 - mae: 96.3355 - val_loss: 96.3818 - val_mae: 96.3818\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 96.1621 - mae: 96.1621 - val_loss: 96.2059 - val_mae: 96.2059\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 95.9863 - mae: 95.9863 - val_loss: 96.0289 - val_mae: 96.0289\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 95.8082 - mae: 95.8082 - val_loss: 95.8517 - val_mae: 95.8517\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 95.6277 - mae: 95.6277 - val_loss: 95.6679 - val_mae: 95.6679\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 95.4449 - mae: 95.4449 - val_loss: 95.4839 - val_mae: 95.4839\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 95.2597 - mae: 95.2597 - val_loss: 95.3037 - val_mae: 95.3037\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 95.0722 - mae: 95.0722 - val_loss: 95.1120 - val_mae: 95.1120\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 94.8823 - mae: 94.8823 - val_loss: 94.9153 - val_mae: 94.9153\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 94.6901 - mae: 94.6901 - val_loss: 94.7199 - val_mae: 94.7199\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 94.4955 - mae: 94.4955 - val_loss: 94.5187 - val_mae: 94.5187\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 94.2985 - mae: 94.2985 - val_loss: 94.3124 - val_mae: 94.3124\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 94.0992 - mae: 94.0992 - val_loss: 94.1094 - val_mae: 94.1094\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 93.8976 - mae: 93.8976 - val_loss: 93.9074 - val_mae: 93.9074\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 93.6936 - mae: 93.6936 - val_loss: 93.6987 - val_mae: 93.6987\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 93.4872 - mae: 93.4872 - val_loss: 93.4820 - val_mae: 93.4820\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 93.2785 - mae: 93.2785 - val_loss: 93.2687 - val_mae: 93.2687\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 93.0675 - mae: 93.0675 - val_loss: 93.0564 - val_mae: 93.0564\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 92.8541 - mae: 92.8541 - val_loss: 92.8367 - val_mae: 92.8367\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 92.6383 - mae: 92.6383 - val_loss: 92.6193 - val_mae: 92.6192\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 92.4203 - mae: 92.4202 - val_loss: 92.4003 - val_mae: 92.4003\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 92.1998 - mae: 92.1998 - val_loss: 92.1817 - val_mae: 92.1817\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 91.9771 - mae: 91.9771 - val_loss: 91.9525 - val_mae: 91.9525\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 91.7519 - mae: 91.7519 - val_loss: 91.7230 - val_mae: 91.7230\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 91.5245 - mae: 91.5245 - val_loss: 91.4823 - val_mae: 91.4823\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 91.2947 - mae: 91.2947 - val_loss: 91.2555 - val_mae: 91.2555\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 91.0625 - mae: 91.0625 - val_loss: 91.0277 - val_mae: 91.0277\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 90.8281 - mae: 90.8281 - val_loss: 90.7927 - val_mae: 90.7927\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 90.5913 - mae: 90.5912 - val_loss: 90.5545 - val_mae: 90.5545\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 90.3521 - mae: 90.3521 - val_loss: 90.3002 - val_mae: 90.3002\n",
      "80/80 [==============================] - 0s 112us/sample - loss: 90.3002 - mae: 90.3002\n",
      "Val score is 90.30017852783203\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 82.0178 - mae: 82.0178 - val_loss: 81.5252 - val_mae: 81.5252\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 81.9904 - mae: 81.9904 - val_loss: 81.5053 - val_mae: 81.5053\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.9622 - mae: 81.9622 - val_loss: 81.4870 - val_mae: 81.4870\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 81.9330 - mae: 81.9330 - val_loss: 81.4667 - val_mae: 81.4667\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.9028 - mae: 81.9027 - val_loss: 81.4462 - val_mae: 81.4462\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.8713 - mae: 81.8713 - val_loss: 81.4244 - val_mae: 81.4244\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.8387 - mae: 81.8387 - val_loss: 81.4007 - val_mae: 81.4007\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 81.8048 - mae: 81.8048 - val_loss: 81.3763 - val_mae: 81.3763\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 81.7695 - mae: 81.7695 - val_loss: 81.3504 - val_mae: 81.3504\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 81.7327 - mae: 81.7327 - val_loss: 81.3214 - val_mae: 81.3214\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 81.6944 - mae: 81.6944 - val_loss: 81.2929 - val_mae: 81.2929\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 292us/sample - loss: 81.6545 - mae: 81.6545 - val_loss: 81.2620 - val_mae: 81.2620\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 81.6130 - mae: 81.6130 - val_loss: 81.2282 - val_mae: 81.2282\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 81.5698 - mae: 81.5698 - val_loss: 81.1951 - val_mae: 81.1951\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.5249 - mae: 81.5249 - val_loss: 81.1560 - val_mae: 81.1560\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 81.4782 - mae: 81.4782 - val_loss: 81.1187 - val_mae: 81.1187\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 81.4296 - mae: 81.4296 - val_loss: 81.0777 - val_mae: 81.0777\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.3792 - mae: 81.3792 - val_loss: 81.0351 - val_mae: 81.0351\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 81.3269 - mae: 81.3269 - val_loss: 80.9924 - val_mae: 80.9924\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 81.2726 - mae: 81.2726 - val_loss: 80.9462 - val_mae: 80.9462\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 81.2164 - mae: 81.2164 - val_loss: 80.8980 - val_mae: 80.8980\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 81.1582 - mae: 81.1582 - val_loss: 80.8483 - val_mae: 80.8483\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 81.0980 - mae: 81.0980 - val_loss: 80.7985 - val_mae: 80.7985\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 81.0358 - mae: 81.0358 - val_loss: 80.7467 - val_mae: 80.7467\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 80.9715 - mae: 80.9715 - val_loss: 80.6950 - val_mae: 80.6950\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.9051 - mae: 80.9051 - val_loss: 80.6388 - val_mae: 80.6388\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.8366 - mae: 80.8366 - val_loss: 80.5815 - val_mae: 80.5815\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 80.7661 - mae: 80.7661 - val_loss: 80.5224 - val_mae: 80.5224\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.6933 - mae: 80.6933 - val_loss: 80.4577 - val_mae: 80.4577\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.6185 - mae: 80.6185 - val_loss: 80.3947 - val_mae: 80.3947\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 80.5414 - mae: 80.5414 - val_loss: 80.3262 - val_mae: 80.3262\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.4622 - mae: 80.4622 - val_loss: 80.2526 - val_mae: 80.2526\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.3808 - mae: 80.3808 - val_loss: 80.1792 - val_mae: 80.1792\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 80.2972 - mae: 80.2972 - val_loss: 80.1059 - val_mae: 80.1059\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 80.2114 - mae: 80.2114 - val_loss: 80.0288 - val_mae: 80.0289\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 80.1234 - mae: 80.1234 - val_loss: 79.9520 - val_mae: 79.9520\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 80.0332 - mae: 80.0332 - val_loss: 79.8728 - val_mae: 79.8728\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 79.9407 - mae: 79.9407 - val_loss: 79.7932 - val_mae: 79.7932\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 70.5965 - mae: 70.596 - 0s 262us/sample - loss: 79.8459 - mae: 79.8459 - val_loss: 79.7038 - val_mae: 79.7038\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 79.7489 - mae: 79.7489 - val_loss: 79.6176 - val_mae: 79.6176\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 79.6496 - mae: 79.6496 - val_loss: 79.5283 - val_mae: 79.5283\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 79.5481 - mae: 79.5481 - val_loss: 79.4337 - val_mae: 79.4337\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 277us/sample - loss: 79.4443 - mae: 79.4443 - val_loss: 79.3359 - val_mae: 79.3359\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 79.3382 - mae: 79.3382 - val_loss: 79.2379 - val_mae: 79.2379\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 79.2298 - mae: 79.2298 - val_loss: 79.1357 - val_mae: 79.1357\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 79.1191 - mae: 79.1191 - val_loss: 79.0314 - val_mae: 79.0314\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 275us/sample - loss: 79.0061 - mae: 79.0061 - val_loss: 78.9284 - val_mae: 78.9284\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 78.8907 - mae: 78.8907 - val_loss: 78.8210 - val_mae: 78.8210\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 78.7731 - mae: 78.7731 - val_loss: 78.7108 - val_mae: 78.7108\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 78.6532 - mae: 78.6532 - val_loss: 78.5990 - val_mae: 78.5990\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 78.5309 - mae: 78.5309 - val_loss: 78.4855 - val_mae: 78.4855\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 78.4063 - mae: 78.4063 - val_loss: 78.3679 - val_mae: 78.3679\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 78.2794 - mae: 78.2794 - val_loss: 78.2535 - val_mae: 78.2535\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 78.1501 - mae: 78.1501 - val_loss: 78.1287 - val_mae: 78.1287\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 78.0185 - mae: 78.0185 - val_loss: 78.0027 - val_mae: 78.0027\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 77.8846 - mae: 77.8846 - val_loss: 77.8732 - val_mae: 77.8732\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 77.7483 - mae: 77.7483 - val_loss: 77.7444 - val_mae: 77.7444\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 77.6097 - mae: 77.6097 - val_loss: 77.6106 - val_mae: 77.6106\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 77.4688 - mae: 77.4688 - val_loss: 77.4731 - val_mae: 77.4731\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 77.3255 - mae: 77.3255 - val_loss: 77.3335 - val_mae: 77.3335\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 77.1798 - mae: 77.1798 - val_loss: 77.1894 - val_mae: 77.1894\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 77.0318 - mae: 77.0318 - val_loss: 77.0470 - val_mae: 77.0470\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 76.8814 - mae: 76.8814 - val_loss: 76.9016 - val_mae: 76.9016\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 76.7287 - mae: 76.7287 - val_loss: 76.7502 - val_mae: 76.7502\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 76.5736 - mae: 76.5736 - val_loss: 76.6003 - val_mae: 76.6003\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 296us/sample - loss: 76.4162 - mae: 76.4162 - val_loss: 76.4474 - val_mae: 76.4474\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 76.2564 - mae: 76.2564 - val_loss: 76.2982 - val_mae: 76.2982\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 76.0942 - mae: 76.0942 - val_loss: 76.1387 - val_mae: 76.1387\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 75.9297 - mae: 75.9297 - val_loss: 75.9750 - val_mae: 75.9750\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 75.7628 - mae: 75.7628 - val_loss: 75.8109 - val_mae: 75.8109\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 75.5936 - mae: 75.5936 - val_loss: 75.6433 - val_mae: 75.6433\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 75.4220 - mae: 75.4220 - val_loss: 75.4760 - val_mae: 75.4760\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 75.2480 - mae: 75.2480 - val_loss: 75.2991 - val_mae: 75.2991\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 75.0717 - mae: 75.0717 - val_loss: 75.1202 - val_mae: 75.1202\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 74.8930 - mae: 74.8930 - val_loss: 74.9444 - val_mae: 74.9444\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 74.7119 - mae: 74.7119 - val_loss: 74.7643 - val_mae: 74.7643\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 74.5285 - mae: 74.5285 - val_loss: 74.5872 - val_mae: 74.5872\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 74.3427 - mae: 74.3427 - val_loss: 74.4040 - val_mae: 74.4040\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 74.1546 - mae: 74.1546 - val_loss: 74.2228 - val_mae: 74.2228\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 73.9641 - mae: 73.9641 - val_loss: 74.0351 - val_mae: 74.0350\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 73.7712 - mae: 73.7712 - val_loss: 73.8445 - val_mae: 73.8445\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 73.5760 - mae: 73.5760 - val_loss: 73.6504 - val_mae: 73.6504\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 73.3784 - mae: 73.3784 - val_loss: 73.4580 - val_mae: 73.4580\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 73.1784 - mae: 73.1784 - val_loss: 73.2577 - val_mae: 73.2577\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 72.9761 - mae: 72.9761 - val_loss: 73.0579 - val_mae: 73.0579\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 72.7714 - mae: 72.7714 - val_loss: 72.8498 - val_mae: 72.8498\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 72.5644 - mae: 72.5644 - val_loss: 72.6399 - val_mae: 72.6399\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 72.3550 - mae: 72.3550 - val_loss: 72.4215 - val_mae: 72.4215\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 72.1432 - mae: 72.1432 - val_loss: 72.2136 - val_mae: 72.2136\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 71.9627 - mae: 71.9627 - val_loss: 71.9950 - val_mae: 71.9950\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 71.7375 - mae: 71.7375 - val_loss: 71.7541 - val_mae: 71.7541\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 71.4994 - mae: 71.4994 - val_loss: 71.4762 - val_mae: 71.4762\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 71.2811 - mae: 71.2811 - val_loss: 71.2036 - val_mae: 71.2036\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 71.0596 - mae: 71.0596 - val_loss: 70.9331 - val_mae: 70.9331\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 70.8352 - mae: 70.8352 - val_loss: 70.6587 - val_mae: 70.6587\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 70.6080 - mae: 70.6080 - val_loss: 70.4036 - val_mae: 70.4036\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 70.3781 - mae: 70.3781 - val_loss: 70.1504 - val_mae: 70.1504\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 70.1457 - mae: 70.1457 - val_loss: 69.9083 - val_mae: 69.9083\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 69.9108 - mae: 69.9108 - val_loss: 69.6644 - val_mae: 69.6644\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 69.6734 - mae: 69.6734 - val_loss: 69.4259 - val_mae: 69.4259\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 69.4259 - mae: 69.4259\n",
      "Val score is 69.42591094970703\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 3.8355 - mae: 3.8355 - val_loss: 4.2574 - val_mae: 4.2574\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.8042 - mae: 3.8042 - val_loss: 4.2160 - val_mae: 4.2160\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 3.7720 - mae: 3.7720 - val_loss: 4.1755 - val_mae: 4.1755\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.7388 - mae: 3.7388 - val_loss: 4.1338 - val_mae: 4.1338\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.7090 - mae: 3.7090 - val_loss: 4.0834 - val_mae: 4.0834\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.6706 - mae: 3.6706 - val_loss: 4.0287 - val_mae: 4.0287\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.6363 - mae: 3.6363 - val_loss: 3.9763 - val_mae: 3.9763\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 265us/sample - loss: 3.6063 - mae: 3.6063 - val_loss: 3.9059 - val_mae: 3.9059\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.5655 - mae: 3.5655 - val_loss: 3.8455 - val_mae: 3.8455\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.5284 - mae: 3.5284 - val_loss: 3.7840 - val_mae: 3.7840\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.4900 - mae: 3.4900 - val_loss: 3.7247 - val_mae: 3.7247\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.4502 - mae: 3.4502 - val_loss: 3.6676 - val_mae: 3.6676\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.4088 - mae: 3.4088 - val_loss: 3.6102 - val_mae: 3.6102\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.3659 - mae: 3.3659 - val_loss: 3.5530 - val_mae: 3.5530\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.3214 - mae: 3.3214 - val_loss: 3.4946 - val_mae: 3.4946\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.2751 - mae: 3.2751 - val_loss: 3.4369 - val_mae: 3.4369\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.2271 - mae: 3.2271 - val_loss: 3.3759 - val_mae: 3.3759\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.1838 - mae: 3.1838 - val_loss: 3.3092 - val_mae: 3.3092\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 251us/sample - loss: 3.1269 - mae: 3.1269 - val_loss: 3.2328 - val_mae: 3.2328\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 3.0762 - mae: 3.0762 - val_loss: 3.1474 - val_mae: 3.1474\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 3.0217 - mae: 3.0217 - val_loss: 3.0647 - val_mae: 3.0647\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 2.9774 - mae: 2.9774 - val_loss: 2.9954 - val_mae: 2.9954\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 2.9129 - mae: 2.9129 - val_loss: 2.9274 - val_mae: 2.9274\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 2.8514 - mae: 2.8514 - val_loss: 2.8526 - val_mae: 2.8526\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.7895 - mae: 2.7895 - val_loss: 2.7723 - val_mae: 2.7723\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.7315 - mae: 2.7315 - val_loss: 2.6899 - val_mae: 2.6899\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 2.6669 - mae: 2.6669 - val_loss: 2.6151 - val_mae: 2.6151\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.5991 - mae: 2.5991 - val_loss: 2.5458 - val_mae: 2.5458\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 2.5438 - mae: 2.5438 - val_loss: 2.4744 - val_mae: 2.4744\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 2.4687 - mae: 2.4687 - val_loss: 2.3960 - val_mae: 2.3960\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.3901 - mae: 2.3901 - val_loss: 2.3156 - val_mae: 2.3156\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.3400 - mae: 2.3400 - val_loss: 2.2313 - val_mae: 2.2313\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.2405 - mae: 2.2405 - val_loss: 2.1467 - val_mae: 2.1467\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 2.1760 - mae: 2.1760 - val_loss: 2.0627 - val_mae: 2.0627\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 2.1010 - mae: 2.1010 - val_loss: 1.9509 - val_mae: 1.9509\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 2.0085 - mae: 2.0085 - val_loss: 1.8558 - val_mae: 1.8558\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 1.9356 - mae: 1.9356 - val_loss: 1.7585 - val_mae: 1.7585\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.8531 - mae: 1.8531 - val_loss: 1.6515 - val_mae: 1.6515\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.7564 - mae: 1.7564 - val_loss: 1.5382 - val_mae: 1.5382\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6663 - mae: 1.6663 - val_loss: 1.4292 - val_mae: 1.4292\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.6333 - mae: 1.6333 - val_loss: 1.3387 - val_mae: 1.3387\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.5281 - mae: 1.5281 - val_loss: 1.2346 - val_mae: 1.2346\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.4380 - mae: 1.4380 - val_loss: 1.1316 - val_mae: 1.1316\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.3401 - mae: 1.3401 - val_loss: 1.0378 - val_mae: 1.0378\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.2934 - mae: 1.2934 - val_loss: 0.9470 - val_mae: 0.9470\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.1942 - mae: 1.1942 - val_loss: 0.8322 - val_mae: 0.8322\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 1.0867 - mae: 1.0867 - val_loss: 0.7418 - val_mae: 0.7418\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0624 - mae: 1.0624 - val_loss: 0.6604 - val_mae: 0.6604\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 1.0209 - mae: 1.0209 - val_loss: 0.6053 - val_mae: 0.6053\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.9069 - mae: 0.9069 - val_loss: 0.5674 - val_mae: 0.5674\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.8747 - mae: 0.8747 - val_loss: 0.5380 - val_mae: 0.5380\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8354 - mae: 0.8354 - val_loss: 0.4641 - val_mae: 0.4641\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6508 - mae: 0.6508 - val_loss: 0.4768 - val_mae: 0.4768\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.7501 - mae: 0.7501 - val_loss: 0.4281 - val_mae: 0.4281\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6555 - mae: 0.6555 - val_loss: 0.3767 - val_mae: 0.3767\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5759 - mae: 0.5759 - val_loss: 0.3999 - val_mae: 0.3999\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5405 - mae: 0.5405 - val_loss: 0.3729 - val_mae: 0.3729\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.6033 - mae: 0.6033 - val_loss: 0.3538 - val_mae: 0.3538\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5448 - mae: 0.5448 - val_loss: 0.3409 - val_mae: 0.3409\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5477 - mae: 0.5477 - val_loss: 0.3340 - val_mae: 0.3340\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5450 - mae: 0.5450 - val_loss: 0.3326 - val_mae: 0.3326\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4858 - mae: 0.4858 - val_loss: 0.3207 - val_mae: 0.3207\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5503 - mae: 0.5503 - val_loss: 0.3197 - val_mae: 0.3197\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5079 - mae: 0.5079 - val_loss: 0.3083 - val_mae: 0.3083\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.5172 - mae: 0.5172 - val_loss: 0.2993 - val_mae: 0.2993\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 266us/sample - loss: 0.4503 - mae: 0.4503 - val_loss: 0.3037 - val_mae: 0.3037\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4414 - mae: 0.4414 - val_loss: 0.3284 - val_mae: 0.3284\n",
      "80/80 [==============================] - 0s 87us/sample - loss: 0.3284 - mae: 0.3284\n",
      "Val score is 0.3284103572368622\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 18ms/sample - loss: 62.5046 - mae: 62.5046 - val_loss: 62.3749 - val_mae: 62.3749\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 62.4746 - mae: 62.4746 - val_loss: 62.3449 - val_mae: 62.3449\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 62.4437 - mae: 62.4437 - val_loss: 62.3148 - val_mae: 62.3148\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 62.4119 - mae: 62.4119 - val_loss: 62.2831 - val_mae: 62.2831\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 325us/sample - loss: 62.3790 - mae: 62.3790 - val_loss: 62.2508 - val_mae: 62.2508\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 62.3449 - mae: 62.3449 - val_loss: 62.2179 - val_mae: 62.2179\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 62.3097 - mae: 62.3097 - val_loss: 62.1833 - val_mae: 62.1833\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 62.2731 - mae: 62.2731 - val_loss: 62.1469 - val_mae: 62.1469\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 62.2352 - mae: 62.2352 - val_loss: 62.1098 - val_mae: 62.1098\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 62.1958 - mae: 62.1958 - val_loss: 62.0707 - val_mae: 62.0707\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 62.1549 - mae: 62.1549 - val_loss: 62.0295 - val_mae: 62.0295\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 62.1124 - mae: 62.1124 - val_loss: 61.9881 - val_mae: 61.9881\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 62.0683 - mae: 62.0683 - val_loss: 61.9437 - val_mae: 61.9437\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 62.0226 - mae: 62.0226 - val_loss: 61.8973 - val_mae: 61.8973\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 61.9751 - mae: 61.9751 - val_loss: 61.8531 - val_mae: 61.8531\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 61.9258 - mae: 61.9258 - val_loss: 61.8054 - val_mae: 61.8055\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 61.8748 - mae: 61.8748 - val_loss: 61.7574 - val_mae: 61.7574\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 61.8219 - mae: 61.8218 - val_loss: 61.7058 - val_mae: 61.7058\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 337us/sample - loss: 61.7671 - mae: 61.7671 - val_loss: 61.6524 - val_mae: 61.6524\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 312us/sample - loss: 61.7104 - mae: 61.7104 - val_loss: 61.5980 - val_mae: 61.5980\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 61.6517 - mae: 61.6517 - val_loss: 61.5426 - val_mae: 61.5426\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 61.5911 - mae: 61.5911 - val_loss: 61.4839 - val_mae: 61.4839\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 61.5520 - mae: 61.5520 - val_loss: 61.3914 - val_mae: 61.3914\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 61.4645 - mae: 61.4645 - val_loss: 61.3097 - val_mae: 61.3097\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 61.3983 - mae: 61.3983 - val_loss: 61.2293 - val_mae: 61.2293\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 61.3299 - mae: 61.3299 - val_loss: 61.1495 - val_mae: 61.1495\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 61.2594 - mae: 61.2594 - val_loss: 61.0740 - val_mae: 61.0740\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 58.2877 - mae: 58.287 - 0s 275us/sample - loss: 61.1868 - mae: 61.1868 - val_loss: 60.9969 - val_mae: 60.9969\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 61.1121 - mae: 61.1121 - val_loss: 60.9209 - val_mae: 60.9209\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 61.0352 - mae: 61.0352 - val_loss: 60.8409 - val_mae: 60.8409\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 260us/sample - loss: 60.9562 - mae: 60.9562 - val_loss: 60.7657 - val_mae: 60.7657\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 60.8750 - mae: 60.8750 - val_loss: 60.6869 - val_mae: 60.6869\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 60.7916 - mae: 60.7916 - val_loss: 60.6039 - val_mae: 60.6039\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 60.7061 - mae: 60.7061 - val_loss: 60.5207 - val_mae: 60.5207\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 60.6183 - mae: 60.6183 - val_loss: 60.4390 - val_mae: 60.4390\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 247us/sample - loss: 60.5284 - mae: 60.5284 - val_loss: 60.3522 - val_mae: 60.3522\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 60.4362 - mae: 60.4362 - val_loss: 60.2672 - val_mae: 60.2672\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 60.3418 - mae: 60.3418 - val_loss: 60.1753 - val_mae: 60.1753\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 60.2452 - mae: 60.2452 - val_loss: 60.0809 - val_mae: 60.0809\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 60.1464 - mae: 60.1464 - val_loss: 59.9830 - val_mae: 59.9830\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 60.0453 - mae: 60.0453 - val_loss: 59.8854 - val_mae: 59.8854\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.9420 - mae: 59.9420 - val_loss: 59.7862 - val_mae: 59.7862\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.8364 - mae: 59.8364 - val_loss: 59.6812 - val_mae: 59.6812\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.7285 - mae: 59.7285 - val_loss: 59.5729 - val_mae: 59.5729\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.6183 - mae: 59.6183 - val_loss: 59.4645 - val_mae: 59.4645\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.5059 - mae: 59.5059 - val_loss: 59.3534 - val_mae: 59.3534\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 59.3912 - mae: 59.3912 - val_loss: 59.2419 - val_mae: 59.2419\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.2742 - mae: 59.2742 - val_loss: 59.1319 - val_mae: 59.1319\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 59.1548 - mae: 59.1548 - val_loss: 59.0207 - val_mae: 59.0207\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 59.0332 - mae: 59.0332 - val_loss: 58.8990 - val_mae: 58.8990\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 58.9093 - mae: 58.9093 - val_loss: 58.7796 - val_mae: 58.7796\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 58.7831 - mae: 58.7831 - val_loss: 58.6560 - val_mae: 58.6560\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 58.6546 - mae: 58.6546 - val_loss: 58.5268 - val_mae: 58.5268\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 58.5237 - mae: 58.5237 - val_loss: 58.4025 - val_mae: 58.4025\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 300us/sample - loss: 58.3905 - mae: 58.3905 - val_loss: 58.2676 - val_mae: 58.2676\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 58.2550 - mae: 58.2550 - val_loss: 58.1323 - val_mae: 58.1323\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 58.1172 - mae: 58.1172 - val_loss: 57.9988 - val_mae: 57.9988\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 57.9770 - mae: 57.9770 - val_loss: 57.8691 - val_mae: 57.8691\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 57.8346 - mae: 57.8346 - val_loss: 57.7243 - val_mae: 57.7243\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 57.6897 - mae: 57.6897 - val_loss: 57.5809 - val_mae: 57.5809\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 57.5426 - mae: 57.5426 - val_loss: 57.4332 - val_mae: 57.4332\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 57.3931 - mae: 57.3931 - val_loss: 57.2814 - val_mae: 57.2814\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 57.2412 - mae: 57.2412 - val_loss: 57.1340 - val_mae: 57.1340\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 57.0870 - mae: 57.0870 - val_loss: 56.9755 - val_mae: 56.9755\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 56.9305 - mae: 56.9305 - val_loss: 56.8187 - val_mae: 56.8187\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 56.7716 - mae: 56.7716 - val_loss: 56.6619 - val_mae: 56.6619\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 56.6104 - mae: 56.6104 - val_loss: 56.5056 - val_mae: 56.5056\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 56.4469 - mae: 56.4469 - val_loss: 56.3495 - val_mae: 56.3495\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 56.2810 - mae: 56.2810 - val_loss: 56.1756 - val_mae: 56.1756\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 56.1127 - mae: 56.1127 - val_loss: 56.0143 - val_mae: 56.0143\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 55.9421 - mae: 55.9421 - val_loss: 55.8440 - val_mae: 55.8440\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 55.7691 - mae: 55.7691 - val_loss: 55.6732 - val_mae: 55.6732\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 55.5938 - mae: 55.5938 - val_loss: 55.5006 - val_mae: 55.5006\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 55.4161 - mae: 55.4161 - val_loss: 55.3207 - val_mae: 55.3207\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 55.2361 - mae: 55.2361 - val_loss: 55.1430 - val_mae: 55.1430\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 55.0538 - mae: 55.0538 - val_loss: 54.9608 - val_mae: 54.9608\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 54.8690 - mae: 54.8690 - val_loss: 54.7794 - val_mae: 54.7794\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 54.6820 - mae: 54.6820 - val_loss: 54.5966 - val_mae: 54.5966\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 54.4925 - mae: 54.4925 - val_loss: 54.4125 - val_mae: 54.4125\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 54.3008 - mae: 54.3008 - val_loss: 54.2137 - val_mae: 54.2137\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 54.1066 - mae: 54.1066 - val_loss: 54.0278 - val_mae: 54.0278\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 53.9102 - mae: 53.9101 - val_loss: 53.8297 - val_mae: 53.8297\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 53.7113 - mae: 53.7113 - val_loss: 53.6291 - val_mae: 53.6291\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 53.5101 - mae: 53.5101 - val_loss: 53.4267 - val_mae: 53.4267\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 53.3066 - mae: 53.3066 - val_loss: 53.2245 - val_mae: 53.2245\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 53.1007 - mae: 53.1007 - val_loss: 53.0190 - val_mae: 53.0190\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 52.8925 - mae: 52.8925 - val_loss: 52.8193 - val_mae: 52.8193\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 52.6819 - mae: 52.6819 - val_loss: 52.6090 - val_mae: 52.6090\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 52.4785 - mae: 52.4785 - val_loss: 52.2761 - val_mae: 52.2761\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 52.2546 - mae: 52.2546 - val_loss: 51.7835 - val_mae: 51.7835\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 52.0382 - mae: 52.0382 - val_loss: 51.3770 - val_mae: 51.3770\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 51.8192 - mae: 51.8192 - val_loss: 51.0224 - val_mae: 51.0224\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 51.5975 - mae: 51.5975 - val_loss: 50.7079 - val_mae: 50.7079\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 51.3734 - mae: 51.3734 - val_loss: 50.4359 - val_mae: 50.4359\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 51.1467 - mae: 51.1467 - val_loss: 50.1829 - val_mae: 50.1829\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 50.9177 - mae: 50.9177 - val_loss: 49.9482 - val_mae: 49.9482\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 50.7096 - mae: 50.7096 - val_loss: 49.6765 - val_mae: 49.6765\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 50.4545 - mae: 50.4545 - val_loss: 49.4212 - val_mae: 49.4212\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 50.2195 - mae: 50.2195 - val_loss: 49.1529 - val_mae: 49.1529\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 49.9817 - mae: 49.9817 - val_loss: 48.8954 - val_mae: 48.8954\n",
      "80/80 [==============================] - 0s 91us/sample - loss: 48.8954 - mae: 48.8954\n",
      "Val score is 48.89543914794922\n",
      "DataFrame: hepatitis imput_method :MLP model :KNN score:  0.79\n",
      "DataFrame: hepatitis imput_method :MLP model :XGB score:  0.79\n",
      "DataFrame: hepatitis imput_method :MLP model :MLP score:  0.79\n",
      "DataFrame: housevotes imput_method :LOCF model :KNN score:  0.92\n",
      "DataFrame: housevotes imput_method :LOCF model :XGB score:  0.95\n",
      "DataFrame: housevotes imput_method :LOCF model :MLP score:  0.95\n",
      "DataFrame: housevotes imput_method :mean_mode model :KNN score:  0.92\n",
      "DataFrame: housevotes imput_method :mean_mode model :XGB score:  0.96\n",
      "DataFrame: housevotes imput_method :mean_mode model :MLP score:  0.96\n",
      "DataFrame: housevotes imput_method :knn model :KNN score:  0.93\n",
      "DataFrame: housevotes imput_method :knn model :XGB score:  0.96\n",
      "DataFrame: housevotes imput_method :knn model :MLP score:  0.96\n",
      "[0]\tvalidation_0-logloss:0.64467\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.61824\n",
      "[2]\tvalidation_0-logloss:0.59927\n",
      "[3]\tvalidation_0-logloss:0.58642\n",
      "[4]\tvalidation_0-logloss:0.57642\n",
      "[5]\tvalidation_0-logloss:0.57004\n",
      "[6]\tvalidation_0-logloss:0.56995\n",
      "[7]\tvalidation_0-logloss:0.56993\n",
      "[8]\tvalidation_0-logloss:0.56986\n",
      "[9]\tvalidation_0-logloss:0.56986\n",
      "[10]\tvalidation_0-logloss:0.56981\n",
      "[11]\tvalidation_0-logloss:0.56320\n",
      "[12]\tvalidation_0-logloss:0.56317\n",
      "[13]\tvalidation_0-logloss:0.56316\n",
      "[14]\tvalidation_0-logloss:0.56316\n",
      "[15]\tvalidation_0-logloss:0.56317\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-logloss:0.56316\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.68375\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-logloss:0.68375\n",
      "[2]\tvalidation_0-logloss:0.68375\n",
      "Stopping. Best iteration:\n",
      "[0]\tvalidation_0-logloss:0.68375\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.56783\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.48324\n",
      "[2]\tvalidation_0-logloss:0.43386\n",
      "[3]\tvalidation_0-logloss:0.39516\n",
      "[4]\tvalidation_0-logloss:0.36913\n",
      "[5]\tvalidation_0-logloss:0.35031\n",
      "[6]\tvalidation_0-logloss:0.33400\n",
      "[7]\tvalidation_0-logloss:0.32330\n",
      "[8]\tvalidation_0-logloss:0.31451\n",
      "[9]\tvalidation_0-logloss:0.30752\n",
      "[10]\tvalidation_0-logloss:0.30745\n",
      "[11]\tvalidation_0-logloss:0.30745\n",
      "[12]\tvalidation_0-logloss:0.30741\n",
      "[13]\tvalidation_0-logloss:0.30741\n",
      "[14]\tvalidation_0-logloss:0.30744\n",
      "Stopping. Best iteration:\n",
      "[12]\tvalidation_0-logloss:0.30741\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.50531\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.39432\n",
      "[2]\tvalidation_0-logloss:0.32302\n",
      "[3]\tvalidation_0-logloss:0.27359\n",
      "[4]\tvalidation_0-logloss:0.23858\n",
      "[5]\tvalidation_0-logloss:0.21278\n",
      "[6]\tvalidation_0-logloss:0.19408\n",
      "[7]\tvalidation_0-logloss:0.17836\n",
      "[8]\tvalidation_0-logloss:0.16713\n",
      "[9]\tvalidation_0-logloss:0.15825\n",
      "[10]\tvalidation_0-logloss:0.15091\n",
      "[11]\tvalidation_0-logloss:0.15089\n",
      "[12]\tvalidation_0-logloss:0.15089\n",
      "[13]\tvalidation_0-logloss:0.15087\n",
      "[14]\tvalidation_0-logloss:0.15085\n",
      "[15]\tvalidation_0-logloss:0.15085\n",
      "[16]\tvalidation_0-logloss:0.15083\n",
      "[17]\tvalidation_0-logloss:0.15083\n",
      "[18]\tvalidation_0-logloss:0.15084\n",
      "[19]\tvalidation_0-logloss:0.15084\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-logloss:0.15083\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.52294\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.41992\n",
      "[2]\tvalidation_0-logloss:0.35797\n",
      "[3]\tvalidation_0-logloss:0.30525\n",
      "[4]\tvalidation_0-logloss:0.26617\n",
      "[5]\tvalidation_0-logloss:0.24267\n",
      "[6]\tvalidation_0-logloss:0.22285\n",
      "[7]\tvalidation_0-logloss:0.20833\n",
      "[8]\tvalidation_0-logloss:0.19497\n",
      "[9]\tvalidation_0-logloss:0.18371\n",
      "[10]\tvalidation_0-logloss:0.17413\n",
      "[11]\tvalidation_0-logloss:0.16647\n",
      "[12]\tvalidation_0-logloss:0.16646\n",
      "[13]\tvalidation_0-logloss:0.15952\n",
      "[14]\tvalidation_0-logloss:0.15951\n",
      "[15]\tvalidation_0-logloss:0.15951\n",
      "[16]\tvalidation_0-logloss:0.15950\n",
      "[17]\tvalidation_0-logloss:0.15951\n",
      "[18]\tvalidation_0-logloss:0.15950\n",
      "Stopping. Best iteration:\n",
      "[16]\tvalidation_0-logloss:0.15950\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.59079\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.52418\n",
      "[2]\tvalidation_0-logloss:0.48715\n",
      "[3]\tvalidation_0-logloss:0.45909\n",
      "[4]\tvalidation_0-logloss:0.42914\n",
      "[5]\tvalidation_0-logloss:0.41536\n",
      "[6]\tvalidation_0-logloss:0.40425\n",
      "[7]\tvalidation_0-logloss:0.40301\n",
      "[8]\tvalidation_0-logloss:0.39455\n",
      "[9]\tvalidation_0-logloss:0.39437\n",
      "[10]\tvalidation_0-logloss:0.39416\n",
      "[11]\tvalidation_0-logloss:0.38713\n",
      "[12]\tvalidation_0-logloss:0.38048\n",
      "[13]\tvalidation_0-logloss:0.38055\n",
      "[14]\tvalidation_0-logloss:0.38042\n",
      "[15]\tvalidation_0-logloss:0.37467\n",
      "[16]\tvalidation_0-logloss:0.37472\n",
      "[17]\tvalidation_0-logloss:0.37465\n",
      "[18]\tvalidation_0-logloss:0.37458\n",
      "[19]\tvalidation_0-logloss:0.37463\n",
      "[20]\tvalidation_0-logloss:0.37459\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-logloss:0.37458\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.57926\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.50604\n",
      "[2]\tvalidation_0-logloss:0.45840\n",
      "[3]\tvalidation_0-logloss:0.42079\n",
      "[4]\tvalidation_0-logloss:0.39956\n",
      "[5]\tvalidation_0-logloss:0.38444\n",
      "[6]\tvalidation_0-logloss:0.37446\n",
      "[7]\tvalidation_0-logloss:0.36415\n",
      "[8]\tvalidation_0-logloss:0.36380\n",
      "[9]\tvalidation_0-logloss:0.36368\n",
      "[10]\tvalidation_0-logloss:0.36367\n",
      "[11]\tvalidation_0-logloss:0.35667\n",
      "[12]\tvalidation_0-logloss:0.35026\n",
      "[13]\tvalidation_0-logloss:0.35026\n",
      "[14]\tvalidation_0-logloss:0.35026\n",
      "[15]\tvalidation_0-logloss:0.35026\n",
      "Stopping. Best iteration:\n",
      "[13]\tvalidation_0-logloss:0.35026\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.53549\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.44517\n",
      "[2]\tvalidation_0-logloss:0.38033\n",
      "[3]\tvalidation_0-logloss:0.33022\n",
      "[4]\tvalidation_0-logloss:0.30316\n",
      "[5]\tvalidation_0-logloss:0.27964\n",
      "[6]\tvalidation_0-logloss:0.25973\n",
      "[7]\tvalidation_0-logloss:0.24732\n",
      "[8]\tvalidation_0-logloss:0.23562\n",
      "[9]\tvalidation_0-logloss:0.22547\n",
      "[10]\tvalidation_0-logloss:0.21721\n",
      "[11]\tvalidation_0-logloss:0.21712\n",
      "[12]\tvalidation_0-logloss:0.21710\n",
      "[13]\tvalidation_0-logloss:0.21707\n",
      "[14]\tvalidation_0-logloss:0.21706\n",
      "[15]\tvalidation_0-logloss:0.21705\n",
      "[16]\tvalidation_0-logloss:0.21705\n",
      "[17]\tvalidation_0-logloss:0.21704\n",
      "[18]\tvalidation_0-logloss:0.21705\n",
      "[19]\tvalidation_0-logloss:0.21706\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-logloss:0.21704\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.55704\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.47740\n",
      "[2]\tvalidation_0-logloss:0.42025\n",
      "[3]\tvalidation_0-logloss:0.38496\n",
      "[4]\tvalidation_0-logloss:0.35968\n",
      "[5]\tvalidation_0-logloss:0.33978\n",
      "[6]\tvalidation_0-logloss:0.32559\n",
      "[7]\tvalidation_0-logloss:0.31479\n",
      "[8]\tvalidation_0-logloss:0.30659\n",
      "[9]\tvalidation_0-logloss:0.30656\n",
      "[10]\tvalidation_0-logloss:0.30654\n",
      "[11]\tvalidation_0-logloss:0.30654\n",
      "[12]\tvalidation_0-logloss:0.29947\n",
      "[13]\tvalidation_0-logloss:0.29943\n",
      "[14]\tvalidation_0-logloss:0.29943\n",
      "[15]\tvalidation_0-logloss:0.29942\n",
      "[16]\tvalidation_0-logloss:0.29942\n",
      "[17]\tvalidation_0-logloss:0.29942\n",
      "[18]\tvalidation_0-logloss:0.29945\n",
      "Stopping. Best iteration:\n",
      "[16]\tvalidation_0-logloss:0.29942\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.69316\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.69312\n",
      "[2]\tvalidation_0-logloss:0.69313\n",
      "[3]\tvalidation_0-logloss:0.69310\n",
      "[4]\tvalidation_0-logloss:0.69312\n",
      "[5]\tvalidation_0-logloss:0.69317\n",
      "Stopping. Best iteration:\n",
      "[3]\tvalidation_0-logloss:0.69310\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.63260\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.60026\n",
      "[2]\tvalidation_0-logloss:0.58356\n",
      "[3]\tvalidation_0-logloss:0.57329\n",
      "[4]\tvalidation_0-logloss:0.57145\n",
      "[5]\tvalidation_0-logloss:0.57028\n",
      "[6]\tvalidation_0-logloss:0.56342\n",
      "[7]\tvalidation_0-logloss:0.56287\n",
      "[8]\tvalidation_0-logloss:0.55599\n",
      "[9]\tvalidation_0-logloss:0.55579\n",
      "[10]\tvalidation_0-logloss:0.55564\n",
      "[11]\tvalidation_0-logloss:0.55564\n",
      "[12]\tvalidation_0-logloss:0.55568\n",
      "[13]\tvalidation_0-logloss:0.55555\n",
      "[14]\tvalidation_0-logloss:0.55555\n",
      "[15]\tvalidation_0-logloss:0.55555\n",
      "[16]\tvalidation_0-logloss:0.55555\n",
      "[17]\tvalidation_0-logloss:0.55550\n",
      "[18]\tvalidation_0-logloss:0.55550\n",
      "[19]\tvalidation_0-logloss:0.55551\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-logloss:0.55550\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.57966\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.50863\n",
      "[2]\tvalidation_0-logloss:0.46224\n",
      "[3]\tvalidation_0-logloss:0.43055\n",
      "[4]\tvalidation_0-logloss:0.40471\n",
      "[5]\tvalidation_0-logloss:0.38526\n",
      "[6]\tvalidation_0-logloss:0.37047\n",
      "[7]\tvalidation_0-logloss:0.35803\n",
      "[8]\tvalidation_0-logloss:0.34762\n",
      "[9]\tvalidation_0-logloss:0.33928\n",
      "[10]\tvalidation_0-logloss:0.33906\n",
      "[11]\tvalidation_0-logloss:0.33229\n",
      "[12]\tvalidation_0-logloss:0.33228\n",
      "[13]\tvalidation_0-logloss:0.33215\n",
      "[14]\tvalidation_0-logloss:0.33212\n",
      "[15]\tvalidation_0-logloss:0.33211\n",
      "[16]\tvalidation_0-logloss:0.33211\n",
      "[17]\tvalidation_0-logloss:0.33210\n",
      "[18]\tvalidation_0-logloss:0.33211\n",
      "[19]\tvalidation_0-logloss:0.33210\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-logloss:0.33210\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.59819\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.54308\n",
      "[2]\tvalidation_0-logloss:0.50844\n",
      "[3]\tvalidation_0-logloss:0.48274\n",
      "[4]\tvalidation_0-logloss:0.46390\n",
      "[5]\tvalidation_0-logloss:0.44726\n",
      "[6]\tvalidation_0-logloss:0.43527\n",
      "[7]\tvalidation_0-logloss:0.42613\n",
      "[8]\tvalidation_0-logloss:0.41864\n",
      "[9]\tvalidation_0-logloss:0.41031\n",
      "[10]\tvalidation_0-logloss:0.41021\n",
      "[11]\tvalidation_0-logloss:0.41032\n",
      "[12]\tvalidation_0-logloss:0.41025\n",
      "Stopping. Best iteration:\n",
      "[10]\tvalidation_0-logloss:0.41021\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.58085\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.50568\n",
      "[2]\tvalidation_0-logloss:0.45091\n",
      "[3]\tvalidation_0-logloss:0.41149\n",
      "[4]\tvalidation_0-logloss:0.38991\n",
      "[5]\tvalidation_0-logloss:0.37163\n",
      "[6]\tvalidation_0-logloss:0.35864\n",
      "[7]\tvalidation_0-logloss:0.34608\n",
      "[8]\tvalidation_0-logloss:0.33711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\tvalidation_0-logloss:0.33661\n",
      "[10]\tvalidation_0-logloss:0.33656\n",
      "[11]\tvalidation_0-logloss:0.32939\n",
      "[12]\tvalidation_0-logloss:0.32928\n",
      "[13]\tvalidation_0-logloss:0.32922\n",
      "[14]\tvalidation_0-logloss:0.32926\n",
      "[15]\tvalidation_0-logloss:0.32921\n",
      "[16]\tvalidation_0-logloss:0.32920\n",
      "[17]\tvalidation_0-logloss:0.32919\n",
      "[18]\tvalidation_0-logloss:0.32919\n",
      "[19]\tvalidation_0-logloss:0.32262\n",
      "[20]\tvalidation_0-logloss:0.32260\n",
      "[21]\tvalidation_0-logloss:0.32258\n",
      "[22]\tvalidation_0-logloss:0.32257\n",
      "[23]\tvalidation_0-logloss:0.32257\n",
      "[24]\tvalidation_0-logloss:0.32257\n",
      "[25]\tvalidation_0-logloss:0.32259\n",
      "Stopping. Best iteration:\n",
      "[23]\tvalidation_0-logloss:0.32257\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.61953\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.57304\n",
      "[2]\tvalidation_0-logloss:0.53999\n",
      "[3]\tvalidation_0-logloss:0.51776\n",
      "[4]\tvalidation_0-logloss:0.50106\n",
      "[5]\tvalidation_0-logloss:0.48605\n",
      "[6]\tvalidation_0-logloss:0.47613\n",
      "[7]\tvalidation_0-logloss:0.46782\n",
      "[8]\tvalidation_0-logloss:0.46776\n",
      "[9]\tvalidation_0-logloss:0.46764\n",
      "[10]\tvalidation_0-logloss:0.46758\n",
      "[11]\tvalidation_0-logloss:0.46755\n",
      "[12]\tvalidation_0-logloss:0.46754\n",
      "[13]\tvalidation_0-logloss:0.46749\n",
      "[14]\tvalidation_0-logloss:0.46751\n",
      "[15]\tvalidation_0-logloss:0.46749\n",
      "[16]\tvalidation_0-logloss:0.46750\n",
      "[17]\tvalidation_0-logloss:0.46749\n",
      "Stopping. Best iteration:\n",
      "[15]\tvalidation_0-logloss:0.46749\n",
      "\n",
      "[0]\tvalidation_0-logloss:0.57988\n",
      "Will train until validation_0-logloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-logloss:0.51138\n",
      "[2]\tvalidation_0-logloss:0.46236\n",
      "[3]\tvalidation_0-logloss:0.43861\n",
      "[4]\tvalidation_0-logloss:0.41223\n",
      "[5]\tvalidation_0-logloss:0.39175\n",
      "[6]\tvalidation_0-logloss:0.38599\n",
      "[7]\tvalidation_0-logloss:0.37089\n",
      "[8]\tvalidation_0-logloss:0.36829\n",
      "[9]\tvalidation_0-logloss:0.35697\n",
      "[10]\tvalidation_0-logloss:0.35588\n",
      "[11]\tvalidation_0-logloss:0.35537\n",
      "[12]\tvalidation_0-logloss:0.35491\n",
      "[13]\tvalidation_0-logloss:0.34600\n",
      "[14]\tvalidation_0-logloss:0.34529\n",
      "[15]\tvalidation_0-logloss:0.34510\n",
      "[16]\tvalidation_0-logloss:0.34497\n",
      "[17]\tvalidation_0-logloss:0.34499\n",
      "[18]\tvalidation_0-logloss:0.34488\n",
      "[19]\tvalidation_0-logloss:0.34488\n",
      "[20]\tvalidation_0-logloss:0.34487\n",
      "[21]\tvalidation_0-logloss:0.34485\n",
      "[22]\tvalidation_0-logloss:0.34484\n",
      "[23]\tvalidation_0-logloss:0.34487\n",
      "[24]\tvalidation_0-logloss:0.34485\n",
      "Stopping. Best iteration:\n",
      "[22]\tvalidation_0-logloss:0.34484\n",
      "\n",
      "DataFrame: housevotes imput_method :trees model :KNN score:  0.93\n",
      "DataFrame: housevotes imput_method :trees model :XGB score:  0.96\n",
      "DataFrame: housevotes imput_method :trees model :MLP score:  0.95\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.6946 - accuracy: 0.5647 - val_loss: 0.5892 - val_accuracy: 0.6466\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.5928 - accuracy: 0.6595 - val_loss: 0.5765 - val_accuracy: 0.6509\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.5354 - accuracy: 0.7112 - val_loss: 0.5736 - val_accuracy: 0.6681\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 171us/sample - loss: 0.5222 - accuracy: 0.7155 - val_loss: 0.5685 - val_accuracy: 0.6853\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.5010 - accuracy: 0.7543 - val_loss: 0.5560 - val_accuracy: 0.7026\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.4924 - accuracy: 0.7500 - val_loss: 0.5496 - val_accuracy: 0.7198\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 173us/sample - loss: 0.4471 - accuracy: 0.7802 - val_loss: 0.5385 - val_accuracy: 0.7414\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.4523 - accuracy: 0.7888 - val_loss: 0.5277 - val_accuracy: 0.7457\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 207us/sample - loss: 0.4432 - accuracy: 0.7888 - val_loss: 0.5197 - val_accuracy: 0.7543\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 211us/sample - loss: 0.4203 - accuracy: 0.8147 - val_loss: 0.5108 - val_accuracy: 0.7716\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 207us/sample - loss: 0.4120 - accuracy: 0.8448 - val_loss: 0.5025 - val_accuracy: 0.7716\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 190us/sample - loss: 0.4188 - accuracy: 0.8147 - val_loss: 0.4933 - val_accuracy: 0.8060\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 194us/sample - loss: 0.3898 - accuracy: 0.8319 - val_loss: 0.4832 - val_accuracy: 0.8103\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 194us/sample - loss: 0.3824 - accuracy: 0.8362 - val_loss: 0.4761 - val_accuracy: 0.8060\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.3676 - accuracy: 0.8491 - val_loss: 0.4671 - val_accuracy: 0.8060\n",
      "232/232 [==============================] - 0s 54us/sample - loss: 0.4671 - accuracy: 0.8060\n",
      "Val score is 0.806034505367279\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 6ms/sample - loss: 0.6954 - accuracy: 0.5776 - val_loss: 0.6622 - val_accuracy: 0.6379\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.6110 - accuracy: 0.6853 - val_loss: 0.6435 - val_accuracy: 0.6336\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 175us/sample - loss: 0.5651 - accuracy: 0.7112 - val_loss: 0.6351 - val_accuracy: 0.6595\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.5446 - accuracy: 0.7241 - val_loss: 0.6298 - val_accuracy: 0.6595\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.5398 - accuracy: 0.7026 - val_loss: 0.6224 - val_accuracy: 0.6681\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 168us/sample - loss: 0.5156 - accuracy: 0.7241 - val_loss: 0.6115 - val_accuracy: 0.6940\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 168us/sample - loss: 0.4999 - accuracy: 0.7112 - val_loss: 0.6012 - val_accuracy: 0.7198\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.4839 - accuracy: 0.7414 - val_loss: 0.5960 - val_accuracy: 0.7241\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 168us/sample - loss: 0.5139 - accuracy: 0.7155 - val_loss: 0.5891 - val_accuracy: 0.7241\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4773 - accuracy: 0.7241 - val_loss: 0.5768 - val_accuracy: 0.7328\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4680 - accuracy: 0.7241 - val_loss: 0.5678 - val_accuracy: 0.7284\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4478 - accuracy: 0.7672 - val_loss: 0.5584 - val_accuracy: 0.7414\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.4409 - accuracy: 0.7716 - val_loss: 0.5506 - val_accuracy: 0.7586\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 174us/sample - loss: 0.4578 - accuracy: 0.7759 - val_loss: 0.5410 - val_accuracy: 0.7586\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4532 - accuracy: 0.7414 - val_loss: 0.5310 - val_accuracy: 0.7672\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.4257 - accuracy: 0.7802 - val_loss: 0.5197 - val_accuracy: 0.7672\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.4587 - accuracy: 0.7371 - val_loss: 0.5129 - val_accuracy: 0.7802\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.4000 - accuracy: 0.8190 - val_loss: 0.5059 - val_accuracy: 0.7974\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4404 - accuracy: 0.7586 - val_loss: 0.4978 - val_accuracy: 0.8017\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.3960 - accuracy: 0.8017 - val_loss: 0.4858 - val_accuracy: 0.7931\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 0s 198us/sample - loss: 0.4074 - accuracy: 0.8060 - val_loss: 0.4754 - val_accuracy: 0.7931\n",
      "232/232 [==============================] - 0s 56us/sample - loss: 0.4754 - accuracy: 0.7931\n",
      "Val score is 0.7931034564971924\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 2s 7ms/sample - loss: 1.0233 - accuracy: 0.4138 - val_loss: 0.6319 - val_accuracy: 0.7198\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 182us/sample - loss: 0.5564 - accuracy: 0.7457 - val_loss: 0.5287 - val_accuracy: 0.8534\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.3972 - accuracy: 0.8491 - val_loss: 0.4699 - val_accuracy: 0.8578\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 168us/sample - loss: 0.3375 - accuracy: 0.8707 - val_loss: 0.4368 - val_accuracy: 0.8750\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.3277 - accuracy: 0.8793 - val_loss: 0.4168 - val_accuracy: 0.8750\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.3062 - accuracy: 0.8793 - val_loss: 0.4028 - val_accuracy: 0.8879\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.2888 - accuracy: 0.8664 - val_loss: 0.3881 - val_accuracy: 0.8922\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.3172 - accuracy: 0.8836 - val_loss: 0.3733 - val_accuracy: 0.8966\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.2569 - accuracy: 0.9095 - val_loss: 0.3649 - val_accuracy: 0.8966\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 173us/sample - loss: 0.2646 - accuracy: 0.9009 - val_loss: 0.3557 - val_accuracy: 0.8966\n",
      "232/232 [==============================] - 0s 66us/sample - loss: 0.3557 - accuracy: 0.8966\n",
      "Val score is 0.8965517282485962\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.4363 - accuracy: 0.8276 - val_loss: 0.4600 - val_accuracy: 0.9052\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.2105 - accuracy: 0.9267 - val_loss: 0.3744 - val_accuracy: 0.9181\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.1600 - accuracy: 0.9526 - val_loss: 0.3202 - val_accuracy: 0.9310\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 159us/sample - loss: 0.1130 - accuracy: 0.9483 - val_loss: 0.2871 - val_accuracy: 0.9310\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 164us/sample - loss: 0.1164 - accuracy: 0.9698 - val_loss: 0.2617 - val_accuracy: 0.9483\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.1118 - accuracy: 0.9569 - val_loss: 0.2401 - val_accuracy: 0.9526\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 168us/sample - loss: 0.0923 - accuracy: 0.9784 - val_loss: 0.2218 - val_accuracy: 0.9612\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.0786 - accuracy: 0.9741 - val_loss: 0.2043 - val_accuracy: 0.9784\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 164us/sample - loss: 0.0802 - accuracy: 0.9741 - val_loss: 0.1869 - val_accuracy: 0.9784\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 169us/sample - loss: 0.0622 - accuracy: 0.9828 - val_loss: 0.1710 - val_accuracy: 0.9828\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 164us/sample - loss: 0.0567 - accuracy: 0.9784 - val_loss: 0.1573 - val_accuracy: 0.9828\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 168us/sample - loss: 0.0548 - accuracy: 0.9871 - val_loss: 0.1450 - val_accuracy: 0.9914\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.0661 - accuracy: 0.9871 - val_loss: 0.1332 - val_accuracy: 0.9914\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 171us/sample - loss: 0.0715 - accuracy: 0.9784 - val_loss: 0.1220 - val_accuracy: 0.9914\n",
      "232/232 [==============================] - 0s 58us/sample - loss: 0.1220 - accuracy: 0.9914\n",
      "Val score is 0.9913793206214905\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 2s 7ms/sample - loss: 0.7738 - accuracy: 0.5345 - val_loss: 0.6267 - val_accuracy: 0.6034\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 174us/sample - loss: 0.3534 - accuracy: 0.8879 - val_loss: 0.5061 - val_accuracy: 0.8060\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 180us/sample - loss: 0.2270 - accuracy: 0.9310 - val_loss: 0.4262 - val_accuracy: 0.8793\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.1758 - accuracy: 0.9353 - val_loss: 0.3756 - val_accuracy: 0.9009\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.1348 - accuracy: 0.9569 - val_loss: 0.3384 - val_accuracy: 0.9138\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.1163 - accuracy: 0.9526 - val_loss: 0.3069 - val_accuracy: 0.9181\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.1118 - accuracy: 0.9655 - val_loss: 0.2799 - val_accuracy: 0.9224\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 164us/sample - loss: 0.0910 - accuracy: 0.9784 - val_loss: 0.2521 - val_accuracy: 0.9353\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 174us/sample - loss: 0.0751 - accuracy: 0.9871 - val_loss: 0.2314 - val_accuracy: 0.9440\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.1157 - accuracy: 0.9526 - val_loss: 0.2098 - val_accuracy: 0.9483\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.0772 - accuracy: 0.9741 - val_loss: 0.1895 - val_accuracy: 0.9698\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.0591 - accuracy: 0.9914 - val_loss: 0.1749 - val_accuracy: 0.9698\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 133us/sample - loss: 0.0545 - accuracy: 0.9957 - val_loss: 0.1579 - val_accuracy: 0.9784\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.0523 - accuracy: 0.9957 - val_loss: 0.1402 - val_accuracy: 0.9914\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.0616 - accuracy: 0.9871 - val_loss: 0.1236 - val_accuracy: 0.9914\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.0779 - accuracy: 0.9828 - val_loss: 0.1100 - val_accuracy: 0.9914\n",
      "232/232 [==============================] - 0s 46us/sample - loss: 0.1100 - accuracy: 0.9914\n",
      "Val score is 0.9913793206214905\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.8049 - accuracy: 0.4871 - val_loss: 0.5926 - val_accuracy: 0.6983\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.5007 - accuracy: 0.7586 - val_loss: 0.5352 - val_accuracy: 0.7931\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 140us/sample - loss: 0.4533 - accuracy: 0.8017 - val_loss: 0.5030 - val_accuracy: 0.8319\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.3883 - accuracy: 0.8319 - val_loss: 0.4876 - val_accuracy: 0.8448\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.3844 - accuracy: 0.8578 - val_loss: 0.4773 - val_accuracy: 0.8319\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.3504 - accuracy: 0.8836 - val_loss: 0.4616 - val_accuracy: 0.8405\n",
      "232/232 [==============================] - 0s 47us/sample - loss: 0.4616 - accuracy: 0.8405\n",
      "Val score is 0.8405172228813171\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.5858 - accuracy: 0.7672 - val_loss: 0.5004 - val_accuracy: 0.8491\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.4225 - accuracy: 0.8405 - val_loss: 0.4624 - val_accuracy: 0.8534\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.3810 - accuracy: 0.8534 - val_loss: 0.4392 - val_accuracy: 0.8578\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 144us/sample - loss: 0.3446 - accuracy: 0.8707 - val_loss: 0.4263 - val_accuracy: 0.8578\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3295 - accuracy: 0.8750 - val_loss: 0.4147 - val_accuracy: 0.8664\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 144us/sample - loss: 0.3099 - accuracy: 0.8707 - val_loss: 0.4042 - val_accuracy: 0.8621\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 155us/sample - loss: 0.2787 - accuracy: 0.8922 - val_loss: 0.3922 - val_accuracy: 0.8621\n",
      "232/232 [==============================] - 0s 45us/sample - loss: 0.3922 - accuracy: 0.8621\n",
      "Val score is 0.8620689511299133\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.5971 - accuracy: 0.7284 - val_loss: 0.5441 - val_accuracy: 0.7931\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.3124 - accuracy: 0.8664 - val_loss: 0.4359 - val_accuracy: 0.8922\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.2648 - accuracy: 0.8793 - val_loss: 0.3759 - val_accuracy: 0.9138\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.1894 - accuracy: 0.9267 - val_loss: 0.3358 - val_accuracy: 0.9181\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.2052 - accuracy: 0.9095 - val_loss: 0.3096 - val_accuracy: 0.9181\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.1915 - accuracy: 0.9095 - val_loss: 0.2927 - val_accuracy: 0.9310\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.1791 - accuracy: 0.9310 - val_loss: 0.2789 - val_accuracy: 0.9353\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1706 - accuracy: 0.9440 - val_loss: 0.2652 - val_accuracy: 0.9397\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1688 - accuracy: 0.9397 - val_loss: 0.2498 - val_accuracy: 0.9440\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.1315 - accuracy: 0.9483 - val_loss: 0.2377 - val_accuracy: 0.9526\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.87 - 0s 142us/sample - loss: 0.1451 - accuracy: 0.9353 - val_loss: 0.2242 - val_accuracy: 0.9612\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 190us/sample - loss: 0.1297 - accuracy: 0.9569 - val_loss: 0.2101 - val_accuracy: 0.9612\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 160us/sample - loss: 0.1261 - accuracy: 0.9569 - val_loss: 0.1950 - val_accuracy: 0.9655\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1169 - accuracy: 0.9698 - val_loss: 0.1853 - val_accuracy: 0.9655\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.1466 - accuracy: 0.9353 - val_loss: 0.1742 - val_accuracy: 0.9655\n",
      "232/232 [==============================] - 0s 52us/sample - loss: 0.1742 - accuracy: 0.9655\n",
      "Val score is 0.9655172228813171\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.6056 - accuracy: 0.6767 - val_loss: 0.5431 - val_accuracy: 0.8448\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.3608 - accuracy: 0.8879 - val_loss: 0.4581 - val_accuracy: 0.8793\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.2821 - accuracy: 0.8922 - val_loss: 0.4050 - val_accuracy: 0.8879\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 132us/sample - loss: 0.2201 - accuracy: 0.9052 - val_loss: 0.3705 - val_accuracy: 0.8879\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2828 - accuracy: 0.8879 - val_loss: 0.3514 - val_accuracy: 0.9095\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.1932 - accuracy: 0.9224 - val_loss: 0.3417 - val_accuracy: 0.9224\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 131us/sample - loss: 0.1849 - accuracy: 0.9397 - val_loss: 0.3263 - val_accuracy: 0.9310\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 132us/sample - loss: 0.1519 - accuracy: 0.9569 - val_loss: 0.3084 - val_accuracy: 0.9353\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.1699 - accuracy: 0.9526 - val_loss: 0.2898 - val_accuracy: 0.9397\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1517 - accuracy: 0.9440 - val_loss: 0.2728 - val_accuracy: 0.9397\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1471 - accuracy: 0.9483 - val_loss: 0.2555 - val_accuracy: 0.9397\n",
      "232/232 [==============================] - 0s 45us/sample - loss: 0.2555 - accuracy: 0.9397\n",
      "Val score is 0.9396551847457886\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.8722 - accuracy: 0.4828 - val_loss: 0.6977 - val_accuracy: 0.5388\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.7648 - accuracy: 0.5043 - val_loss: 0.6862 - val_accuracy: 0.5603\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.6901 - accuracy: 0.5603 - val_loss: 0.6759 - val_accuracy: 0.5776\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 131us/sample - loss: 0.6557 - accuracy: 0.6078 - val_loss: 0.6668 - val_accuracy: 0.5905\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 133us/sample - loss: 0.6252 - accuracy: 0.6810 - val_loss: 0.6567 - val_accuracy: 0.6164\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.6203 - accuracy: 0.6336 - val_loss: 0.6507 - val_accuracy: 0.6293\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5860 - accuracy: 0.6681 - val_loss: 0.6446 - val_accuracy: 0.6379\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5750 - accuracy: 0.6810 - val_loss: 0.6349 - val_accuracy: 0.6638\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5476 - accuracy: 0.7543 - val_loss: 0.6249 - val_accuracy: 0.6681\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5726 - accuracy: 0.7026 - val_loss: 0.6160 - val_accuracy: 0.6853\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.5413 - accuracy: 0.6983 - val_loss: 0.6075 - val_accuracy: 0.7069\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.5388 - accuracy: 0.7414 - val_loss: 0.5993 - val_accuracy: 0.7198\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5235 - accuracy: 0.7241 - val_loss: 0.5917 - val_accuracy: 0.7198\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5086 - accuracy: 0.7198 - val_loss: 0.5827 - val_accuracy: 0.7284\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.5093 - accuracy: 0.7457 - val_loss: 0.5737 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.4950 - accuracy: 0.7284 - val_loss: 0.5640 - val_accuracy: 0.7672\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.4946 - accuracy: 0.7328 - val_loss: 0.5548 - val_accuracy: 0.7672\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.4876 - accuracy: 0.7672 - val_loss: 0.5472 - val_accuracy: 0.7629\n",
      "232/232 [==============================] - 0s 43us/sample - loss: 0.5472 - accuracy: 0.7629\n",
      "Val score is 0.7629310488700867\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.7969 - accuracy: 0.5991 - val_loss: 0.6178 - val_accuracy: 0.6595\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.6780 - accuracy: 0.6422 - val_loss: 0.6111 - val_accuracy: 0.6595\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.6098 - accuracy: 0.6724 - val_loss: 0.6008 - val_accuracy: 0.6595\n",
      "232/232 [==============================] - 0s 43us/sample - loss: 0.6008 - accuracy: 0.6595\n",
      "Val score is 0.6594827771186829\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.6466 - accuracy: 0.6466 - val_loss: 0.6029 - val_accuracy: 0.7543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.3374 - accuracy: 0.8836 - val_loss: 0.4979 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.2782 - accuracy: 0.8750 - val_loss: 0.4381 - val_accuracy: 0.8793\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2473 - accuracy: 0.9009 - val_loss: 0.4021 - val_accuracy: 0.8922\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 127us/sample - loss: 0.2643 - accuracy: 0.8922 - val_loss: 0.3824 - val_accuracy: 0.9009\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2060 - accuracy: 0.9181 - val_loss: 0.3683 - val_accuracy: 0.9095\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2095 - accuracy: 0.9224 - val_loss: 0.3532 - val_accuracy: 0.9138\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 144us/sample - loss: 0.1991 - accuracy: 0.9052 - val_loss: 0.3400 - val_accuracy: 0.9181\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 155us/sample - loss: 0.1771 - accuracy: 0.9310 - val_loss: 0.3277 - val_accuracy: 0.9224\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 159us/sample - loss: 0.1843 - accuracy: 0.9310 - val_loss: 0.3120 - val_accuracy: 0.9224\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1773 - accuracy: 0.9310 - val_loss: 0.2958 - val_accuracy: 0.9267\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1902 - accuracy: 0.9224 - val_loss: 0.2797 - val_accuracy: 0.9310\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.1545 - accuracy: 0.9397 - val_loss: 0.2678 - val_accuracy: 0.9310\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1634 - accuracy: 0.9440 - val_loss: 0.2563 - val_accuracy: 0.9397\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 155us/sample - loss: 0.1513 - accuracy: 0.9483 - val_loss: 0.2463 - val_accuracy: 0.9397\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.1601 - accuracy: 0.9569 - val_loss: 0.2382 - val_accuracy: 0.9483\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1393 - accuracy: 0.9612 - val_loss: 0.2271 - val_accuracy: 0.9483\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1635 - accuracy: 0.9440 - val_loss: 0.2131 - val_accuracy: 0.9569\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.1259 - accuracy: 0.9569 - val_loss: 0.2019 - val_accuracy: 0.9569\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.1448 - accuracy: 0.9612 - val_loss: 0.1917 - val_accuracy: 0.9569\n",
      "232/232 [==============================] - 0s 43us/sample - loss: 0.1917 - accuracy: 0.9569\n",
      "Val score is 0.9568965435028076\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.6868 - accuracy: 0.6250 - val_loss: 0.5495 - val_accuracy: 0.8233\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.4477 - accuracy: 0.7974 - val_loss: 0.4896 - val_accuracy: 0.8405\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.3928 - accuracy: 0.8448 - val_loss: 0.4625 - val_accuracy: 0.8362\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3731 - accuracy: 0.8534 - val_loss: 0.4470 - val_accuracy: 0.8405\n",
      "232/232 [==============================] - 0s 43us/sample - loss: 0.4470 - accuracy: 0.8405\n",
      "Val score is 0.8405172228813171\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.6262 - accuracy: 0.6336 - val_loss: 0.5031 - val_accuracy: 0.7586\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.4324 - accuracy: 0.7888 - val_loss: 0.4552 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 139us/sample - loss: 0.3672 - accuracy: 0.8448 - val_loss: 0.4288 - val_accuracy: 0.8879\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 127us/sample - loss: 0.3211 - accuracy: 0.8621 - val_loss: 0.4158 - val_accuracy: 0.8966\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.3033 - accuracy: 0.8750 - val_loss: 0.4113 - val_accuracy: 0.8966\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2809 - accuracy: 0.9009 - val_loss: 0.4028 - val_accuracy: 0.9009\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.2852 - accuracy: 0.8966 - val_loss: 0.3894 - val_accuracy: 0.8966\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.2769 - accuracy: 0.9095 - val_loss: 0.3802 - val_accuracy: 0.8966\n",
      "232/232 [==============================] - 0s 46us/sample - loss: 0.3802 - accuracy: 0.8966\n",
      "Val score is 0.8965517282485962\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.7123 - accuracy: 0.6207 - val_loss: 0.5452 - val_accuracy: 0.7629\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.5461 - accuracy: 0.7586 - val_loss: 0.5099 - val_accuracy: 0.7759\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.5427 - accuracy: 0.7543 - val_loss: 0.4996 - val_accuracy: 0.7802\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.4799 - accuracy: 0.8017 - val_loss: 0.4959 - val_accuracy: 0.7845\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.4566 - accuracy: 0.8017 - val_loss: 0.4972 - val_accuracy: 0.7974\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 133us/sample - loss: 0.4260 - accuracy: 0.8233 - val_loss: 0.4888 - val_accuracy: 0.8060\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 151us/sample - loss: 0.4006 - accuracy: 0.8276 - val_loss: 0.4792 - val_accuracy: 0.8103\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.3903 - accuracy: 0.8362 - val_loss: 0.4637 - val_accuracy: 0.8060\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.3710 - accuracy: 0.8362 - val_loss: 0.4481 - val_accuracy: 0.8103\n",
      "232/232 [==============================] - 0s 43us/sample - loss: 0.4481 - accuracy: 0.8103\n",
      "Val score is 0.8103448152542114\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.9806 - accuracy: 0.3578 - val_loss: 0.5531 - val_accuracy: 0.8147\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.6555 - accuracy: 0.6724 - val_loss: 0.5141 - val_accuracy: 0.7974\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.6130 - accuracy: 0.7241 - val_loss: 0.5029 - val_accuracy: 0.7845\n",
      "232/232 [==============================] - 0s 42us/sample - loss: 0.5029 - accuracy: 0.7845\n",
      "Val score is 0.7844827771186829\n",
      "DataFrame: housevotes imput_method :MLP model :KNN score:  0.93\n",
      "DataFrame: housevotes imput_method :MLP model :XGB score:  0.94\n",
      "DataFrame: housevotes imput_method :MLP model :MLP score:  0.95\n",
      "DataFrame: mammographic imput_method :LOCF model :KNN score:  0.8\n",
      "DataFrame: mammographic imput_method :LOCF model :XGB score:  0.83\n",
      "DataFrame: mammographic imput_method :LOCF model :MLP score:  0.81\n",
      "DataFrame: mammographic imput_method :mean_mode model :KNN score:  0.8\n",
      "DataFrame: mammographic imput_method :mean_mode model :XGB score:  0.83\n",
      "DataFrame: mammographic imput_method :mean_mode model :MLP score:  0.8\n",
      "DataFrame: mammographic imput_method :knn model :KNN score:  0.8\n",
      "DataFrame: mammographic imput_method :knn model :XGB score:  0.84\n",
      "DataFrame: mammographic imput_method :knn model :MLP score:  0.81\n",
      "[0]\tvalidation_0-mae:2.68928\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.92167\n",
      "[2]\tvalidation_0-mae:1.39425\n",
      "[3]\tvalidation_0-mae:1.03141\n",
      "[4]\tvalidation_0-mae:0.78286\n",
      "[5]\tvalidation_0-mae:0.62597\n",
      "[6]\tvalidation_0-mae:0.54183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\tvalidation_0-mae:0.47711\n",
      "[8]\tvalidation_0-mae:0.43843\n",
      "[9]\tvalidation_0-mae:0.41507\n",
      "[10]\tvalidation_0-mae:0.39960\n",
      "[11]\tvalidation_0-mae:0.39182\n",
      "[12]\tvalidation_0-mae:0.38629\n",
      "[13]\tvalidation_0-mae:0.38093\n",
      "[14]\tvalidation_0-mae:0.37805\n",
      "[15]\tvalidation_0-mae:0.37577\n",
      "[16]\tvalidation_0-mae:0.37404\n",
      "[17]\tvalidation_0-mae:0.37275\n",
      "[18]\tvalidation_0-mae:0.37275\n",
      "[19]\tvalidation_0-mae:0.37325\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-mae:0.37275\n",
      "\n",
      "[0]\tvalidation_0-mae:38.85208\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:27.60288\n",
      "[2]\tvalidation_0-mae:20.21725\n",
      "[3]\tvalidation_0-mae:15.83523\n",
      "[4]\tvalidation_0-mae:13.25244\n",
      "[5]\tvalidation_0-mae:11.82756\n",
      "[6]\tvalidation_0-mae:11.03671\n",
      "[7]\tvalidation_0-mae:10.60102\n",
      "[8]\tvalidation_0-mae:10.33149\n",
      "[9]\tvalidation_0-mae:10.16592\n",
      "[10]\tvalidation_0-mae:10.05732\n",
      "[11]\tvalidation_0-mae:9.97061\n",
      "[12]\tvalidation_0-mae:9.90872\n",
      "[13]\tvalidation_0-mae:9.86044\n",
      "[14]\tvalidation_0-mae:9.80772\n",
      "[15]\tvalidation_0-mae:9.75073\n",
      "[16]\tvalidation_0-mae:9.72061\n",
      "[17]\tvalidation_0-mae:9.70288\n",
      "[18]\tvalidation_0-mae:9.68863\n",
      "[19]\tvalidation_0-mae:9.66485\n",
      "[20]\tvalidation_0-mae:9.65337\n",
      "[21]\tvalidation_0-mae:9.64417\n",
      "[22]\tvalidation_0-mae:9.60965\n",
      "[23]\tvalidation_0-mae:9.58826\n",
      "[24]\tvalidation_0-mae:9.55276\n",
      "[25]\tvalidation_0-mae:9.53890\n",
      "[26]\tvalidation_0-mae:9.51538\n",
      "[27]\tvalidation_0-mae:9.49987\n",
      "[28]\tvalidation_0-mae:9.48774\n",
      "[29]\tvalidation_0-mae:9.48249\n",
      "[30]\tvalidation_0-mae:9.45504\n",
      "[31]\tvalidation_0-mae:9.43927\n",
      "[32]\tvalidation_0-mae:9.42542\n",
      "[33]\tvalidation_0-mae:9.41822\n",
      "[34]\tvalidation_0-mae:9.40739\n",
      "[35]\tvalidation_0-mae:9.39556\n",
      "[36]\tvalidation_0-mae:9.38020\n",
      "[37]\tvalidation_0-mae:9.37423\n",
      "[38]\tvalidation_0-mae:9.36454\n",
      "[39]\tvalidation_0-mae:9.35403\n",
      "[40]\tvalidation_0-mae:9.34546\n",
      "[41]\tvalidation_0-mae:9.32993\n",
      "[42]\tvalidation_0-mae:9.32557\n",
      "[43]\tvalidation_0-mae:9.31813\n",
      "[44]\tvalidation_0-mae:9.30915\n",
      "[45]\tvalidation_0-mae:9.30201\n",
      "[46]\tvalidation_0-mae:9.29411\n",
      "[47]\tvalidation_0-mae:9.28819\n",
      "[48]\tvalidation_0-mae:9.27622\n",
      "[49]\tvalidation_0-mae:9.26863\n",
      "[50]\tvalidation_0-mae:9.25815\n",
      "[51]\tvalidation_0-mae:9.24889\n",
      "[52]\tvalidation_0-mae:9.24080\n",
      "[53]\tvalidation_0-mae:9.23446\n",
      "[54]\tvalidation_0-mae:9.23143\n",
      "[55]\tvalidation_0-mae:9.23102\n",
      "[56]\tvalidation_0-mae:9.22746\n",
      "[57]\tvalidation_0-mae:9.21596\n",
      "[58]\tvalidation_0-mae:9.21671\n",
      "[59]\tvalidation_0-mae:9.21190\n",
      "[60]\tvalidation_0-mae:9.21611\n",
      "[61]\tvalidation_0-mae:9.20715\n",
      "[62]\tvalidation_0-mae:9.19723\n",
      "[63]\tvalidation_0-mae:9.19252\n",
      "[64]\tvalidation_0-mae:9.18942\n",
      "[65]\tvalidation_0-mae:9.18726\n",
      "[66]\tvalidation_0-mae:9.18022\n",
      "[67]\tvalidation_0-mae:9.18204\n",
      "[68]\tvalidation_0-mae:9.17435\n",
      "[69]\tvalidation_0-mae:9.17125\n",
      "[70]\tvalidation_0-mae:9.16138\n",
      "[71]\tvalidation_0-mae:9.15824\n",
      "[72]\tvalidation_0-mae:9.15111\n",
      "[73]\tvalidation_0-mae:9.14701\n",
      "[74]\tvalidation_0-mae:9.14247\n",
      "[75]\tvalidation_0-mae:9.14436\n",
      "[76]\tvalidation_0-mae:9.14088\n",
      "[77]\tvalidation_0-mae:9.12957\n",
      "[78]\tvalidation_0-mae:9.12459\n",
      "[79]\tvalidation_0-mae:9.12730\n",
      "[80]\tvalidation_0-mae:9.12225\n",
      "[81]\tvalidation_0-mae:9.12264\n",
      "[82]\tvalidation_0-mae:9.11770\n",
      "[83]\tvalidation_0-mae:9.10928\n",
      "[84]\tvalidation_0-mae:9.10817\n",
      "[85]\tvalidation_0-mae:9.10548\n",
      "[86]\tvalidation_0-mae:9.10740\n",
      "[87]\tvalidation_0-mae:9.09827\n",
      "[88]\tvalidation_0-mae:9.09781\n",
      "[89]\tvalidation_0-mae:9.09304\n",
      "[90]\tvalidation_0-mae:9.08363\n",
      "[91]\tvalidation_0-mae:9.08117\n",
      "[92]\tvalidation_0-mae:9.07765\n",
      "[93]\tvalidation_0-mae:9.07785\n",
      "[94]\tvalidation_0-mae:9.07655\n",
      "[95]\tvalidation_0-mae:9.07330\n",
      "[96]\tvalidation_0-mae:9.06929\n",
      "[97]\tvalidation_0-mae:9.06715\n",
      "[98]\tvalidation_0-mae:9.06838\n",
      "[99]\tvalidation_0-mae:9.06568\n",
      "[100]\tvalidation_0-mae:9.05828\n",
      "[101]\tvalidation_0-mae:9.05988\n",
      "[102]\tvalidation_0-mae:9.06013\n",
      "Stopping. Best iteration:\n",
      "[100]\tvalidation_0-mae:9.05828\n",
      "\n",
      "[0]\tvalidation_0-mae:1.58955\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.18897\n",
      "[2]\tvalidation_0-mae:1.00566\n",
      "[3]\tvalidation_0-mae:0.87685\n",
      "[4]\tvalidation_0-mae:0.78668\n",
      "[5]\tvalidation_0-mae:0.73251\n",
      "[6]\tvalidation_0-mae:0.69691\n",
      "[7]\tvalidation_0-mae:0.68125\n",
      "[8]\tvalidation_0-mae:0.66222\n",
      "[9]\tvalidation_0-mae:0.65489\n",
      "[10]\tvalidation_0-mae:0.65071\n",
      "[11]\tvalidation_0-mae:0.64563\n",
      "[12]\tvalidation_0-mae:0.64373\n",
      "[13]\tvalidation_0-mae:0.64178\n",
      "[14]\tvalidation_0-mae:0.64060\n",
      "[15]\tvalidation_0-mae:0.63948\n",
      "[16]\tvalidation_0-mae:0.63947\n",
      "[17]\tvalidation_0-mae:0.63956\n",
      "[18]\tvalidation_0-mae:0.63893\n",
      "[19]\tvalidation_0-mae:0.63821\n",
      "[20]\tvalidation_0-mae:0.63813\n",
      "[21]\tvalidation_0-mae:0.63863\n",
      "[22]\tvalidation_0-mae:0.63817\n",
      "Stopping. Best iteration:\n",
      "[20]\tvalidation_0-mae:0.63813\n",
      "\n",
      "[0]\tvalidation_0-mae:1.65914\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.23213\n",
      "[2]\tvalidation_0-mae:0.98121\n",
      "[3]\tvalidation_0-mae:0.85655\n",
      "[4]\tvalidation_0-mae:0.79335\n",
      "[5]\tvalidation_0-mae:0.75858\n",
      "[6]\tvalidation_0-mae:0.73743\n",
      "[7]\tvalidation_0-mae:0.72152\n",
      "[8]\tvalidation_0-mae:0.70074\n",
      "[9]\tvalidation_0-mae:0.70098\n",
      "[10]\tvalidation_0-mae:0.70107\n",
      "Stopping. Best iteration:\n",
      "[8]\tvalidation_0-mae:0.70074\n",
      "\n",
      "[0]\tvalidation_0-mae:1.70388\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.21906\n",
      "[2]\tvalidation_0-mae:0.88932\n",
      "[3]\tvalidation_0-mae:0.68208\n",
      "[4]\tvalidation_0-mae:0.53708\n",
      "[5]\tvalidation_0-mae:0.43514\n",
      "[6]\tvalidation_0-mae:0.36359\n",
      "[7]\tvalidation_0-mae:0.31275\n",
      "[8]\tvalidation_0-mae:0.27589\n",
      "[9]\tvalidation_0-mae:0.25184\n",
      "[10]\tvalidation_0-mae:0.23343\n",
      "[11]\tvalidation_0-mae:0.22079\n",
      "[12]\tvalidation_0-mae:0.21214\n",
      "[13]\tvalidation_0-mae:0.20673\n",
      "[14]\tvalidation_0-mae:0.20350\n",
      "[15]\tvalidation_0-mae:0.19961\n",
      "[16]\tvalidation_0-mae:0.19659\n",
      "[17]\tvalidation_0-mae:0.19459\n",
      "[18]\tvalidation_0-mae:0.19083\n",
      "[19]\tvalidation_0-mae:0.18839\n",
      "[20]\tvalidation_0-mae:0.19016\n",
      "[21]\tvalidation_0-mae:0.19160\n",
      "Stopping. Best iteration:\n",
      "[19]\tvalidation_0-mae:0.18839\n",
      "\n",
      "DataFrame: mammographic imput_method :trees model :KNN score:  0.8\n",
      "DataFrame: mammographic imput_method :trees model :XGB score:  0.84\n",
      "DataFrame: mammographic imput_method :trees model :MLP score:  0.82\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 2s 2ms/sample - loss: 4.2084 - mae: 4.2084 - val_loss: 4.1993 - val_mae: 4.1993\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 129us/sample - loss: 3.8994 - mae: 3.8994 - val_loss: 3.9132 - val_mae: 3.9132\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 3.4965 - mae: 3.4965 - val_loss: 3.3555 - val_mae: 3.3555\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 134us/sample - loss: 2.9677 - mae: 2.9677 - val_loss: 2.7645 - val_mae: 2.7645\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 2.3231 - mae: 2.3231 - val_loss: 1.9434 - val_mae: 1.9434\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 131us/sample - loss: 1.5774 - mae: 1.5774 - val_loss: 1.1148 - val_mae: 1.1148\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.9243 - mae: 0.9243 - val_loss: 0.5293 - val_mae: 0.5293\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.6059 - mae: 0.6059 - val_loss: 0.4556 - val_mae: 0.4556\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 134us/sample - loss: 0.4960 - mae: 0.4960 - val_loss: 0.4051 - val_mae: 0.4051\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 143us/sample - loss: 0.4433 - mae: 0.4433 - val_loss: 0.4044 - val_mae: 0.4044\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 131us/sample - loss: 0.4529 - mae: 0.4529 - val_loss: 0.3497 - val_mae: 0.3497\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.4569 - mae: 0.4569 - val_loss: 0.4057 - val_mae: 0.4057\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 135us/sample - loss: 0.4270 - mae: 0.4270 - val_loss: 0.3761 - val_mae: 0.3761\n",
      "830/830 [==============================] - 0s 51us/sample - loss: 0.3761 - mae: 0.3761\n",
      "Val score is 0.37612223625183105\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 1s 1ms/sample - loss: 55.6646 - mae: 55.6646 - val_loss: 55.5173 - val_mae: 55.5173\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 131us/sample - loss: 55.3561 - mae: 55.3561 - val_loss: 55.1336 - val_mae: 55.1336\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 54.9199 - mae: 54.9199 - val_loss: 54.6144 - val_mae: 54.6144\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 54.3316 - mae: 54.3316 - val_loss: 53.9458 - val_mae: 53.9458\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 53.5787 - mae: 53.5787 - val_loss: 53.1242 - val_mae: 53.1242\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 129us/sample - loss: 52.6545 - mae: 52.6544 - val_loss: 52.1469 - val_mae: 52.1469\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 51.5552 - mae: 51.5552 - val_loss: 50.9863 - val_mae: 50.9863\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 50.2789 - mae: 50.2789 - val_loss: 49.6159 - val_mae: 49.6159\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 48.8246 - mae: 48.8246 - val_loss: 48.1308 - val_mae: 48.1308\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 47.1923 - mae: 47.1923 - val_loss: 46.4307 - val_mae: 46.4307\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 45.3821 - mae: 45.3821 - val_loss: 44.6304 - val_mae: 44.6304\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 43.3949 - mae: 43.3949 - val_loss: 42.5523 - val_mae: 42.5523\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 41.2323 - mae: 41.2323 - val_loss: 40.4911 - val_mae: 40.4911\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 38.9185 - mae: 38.9185 - val_loss: 37.4092 - val_mae: 37.4092\n",
      "Epoch 15/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 36.3982 - mae: 36.3982 - val_loss: 33.3868 - val_mae: 33.3868\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 33.7250 - mae: 33.7250 - val_loss: 29.7927 - val_mae: 29.7927\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 30.9001 - mae: 30.9001 - val_loss: 26.9058 - val_mae: 26.9058\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 27.9819 - mae: 27.9819 - val_loss: 25.2068 - val_mae: 25.2068\n",
      "Epoch 19/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 25.0161 - mae: 25.0161 - val_loss: 22.8907 - val_mae: 22.8907\n",
      "Epoch 20/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 22.0419 - mae: 22.0418 - val_loss: 20.0270 - val_mae: 20.0270\n",
      "Epoch 21/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 19.3431 - mae: 19.3431 - val_loss: 15.9774 - val_mae: 15.9774\n",
      "Epoch 22/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 16.8968 - mae: 16.8968 - val_loss: 15.3418 - val_mae: 15.3418\n",
      "Epoch 23/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 14.8824 - mae: 14.8824 - val_loss: 13.3846 - val_mae: 13.3846\n",
      "Epoch 24/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 13.2123 - mae: 13.2123 - val_loss: 12.0704 - val_mae: 12.0704\n",
      "Epoch 25/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 12.1546 - mae: 12.1546 - val_loss: 11.2222 - val_mae: 11.2222\n",
      "Epoch 26/100\n",
      "830/830 [==============================] - 0s 136us/sample - loss: 11.4386 - mae: 11.4386 - val_loss: 10.6364 - val_mae: 10.6364\n",
      "Epoch 27/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 10.8273 - mae: 10.8273 - val_loss: 10.3230 - val_mae: 10.3230\n",
      "Epoch 28/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 10.4273 - mae: 10.4273 - val_loss: 10.1721 - val_mae: 10.1721\n",
      "Epoch 29/100\n",
      "830/830 [==============================] - 0s 129us/sample - loss: 10.3192 - mae: 10.3192 - val_loss: 10.0050 - val_mae: 10.0050\n",
      "Epoch 30/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 10.2267 - mae: 10.2267 - val_loss: 9.9741 - val_mae: 9.9741\n",
      "Epoch 31/100\n",
      "830/830 [==============================] - 0s 129us/sample - loss: 10.1839 - mae: 10.1839 - val_loss: 9.9030 - val_mae: 9.9030\n",
      "Epoch 32/100\n",
      "830/830 [==============================] - 0s 132us/sample - loss: 10.0851 - mae: 10.0851 - val_loss: 9.8935 - val_mae: 9.8935\n",
      "Epoch 33/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 10.0573 - mae: 10.0573 - val_loss: 9.8768 - val_mae: 9.8768\n",
      "Epoch 34/100\n",
      "830/830 [==============================] - 0s 127us/sample - loss: 10.0632 - mae: 10.0632 - val_loss: 9.8556 - val_mae: 9.8556\n",
      "Epoch 35/100\n",
      "830/830 [==============================] - 0s 138us/sample - loss: 10.1271 - mae: 10.1271 - val_loss: 9.8112 - val_mae: 9.8112\n",
      "Epoch 36/100\n",
      "830/830 [==============================] - 0s 132us/sample - loss: 10.0220 - mae: 10.0220 - val_loss: 9.7874 - val_mae: 9.7874\n",
      "Epoch 37/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 9.9938 - mae: 9.9938 - val_loss: 9.7963 - val_mae: 9.7963\n",
      "Epoch 38/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 9.9260 - mae: 9.9260 - val_loss: 9.7603 - val_mae: 9.7603\n",
      "Epoch 39/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 9.9692 - mae: 9.9692 - val_loss: 9.7506 - val_mae: 9.7506\n",
      "Epoch 40/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 9.9572 - mae: 9.9572 - val_loss: 9.7632 - val_mae: 9.7632\n",
      "Epoch 41/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 9.9235 - mae: 9.9235 - val_loss: 9.7494 - val_mae: 9.7494\n",
      "Epoch 42/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 9.9514 - mae: 9.9514 - val_loss: 9.7632 - val_mae: 9.7632\n",
      "Epoch 43/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 9.9232 - mae: 9.9232 - val_loss: 9.7467 - val_mae: 9.7467\n",
      "Epoch 44/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 9.9207 - mae: 9.9207 - val_loss: 9.7688 - val_mae: 9.7688\n",
      "Epoch 45/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 9.9345 - mae: 9.9345 - val_loss: 9.7769 - val_mae: 9.7769\n",
      "830/830 [==============================] - 0s 37us/sample - loss: 9.7769 - mae: 9.7769\n",
      "Val score is 9.776871681213379\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 1s 1ms/sample - loss: 2.7876 - mae: 2.7876 - val_loss: 2.4038 - val_mae: 2.4038\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 2.4146 - mae: 2.4146 - val_loss: 2.3722 - val_mae: 2.3722\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 2.1196 - mae: 2.1196 - val_loss: 2.2876 - val_mae: 2.2876\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 1.7949 - mae: 1.7949 - val_loss: 2.0922 - val_mae: 2.0922\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 1.4111 - mae: 1.4111 - val_loss: 1.7944 - val_mae: 1.7944\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 1.0749 - mae: 1.0749 - val_loss: 1.4647 - val_mae: 1.4647\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.8145 - mae: 0.8145 - val_loss: 1.1360 - val_mae: 1.1360\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.7120 - mae: 0.7120 - val_loss: 1.0338 - val_mae: 1.0338\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.7105 - mae: 0.7105 - val_loss: 0.9530 - val_mae: 0.9530\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.6228 - mae: 0.6228 - val_loss: 0.8123 - val_mae: 0.8123\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.6581 - mae: 0.6581 - val_loss: 0.7943 - val_mae: 0.7943\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.6443 - mae: 0.6443 - val_loss: 0.7736 - val_mae: 0.7736\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 0.6143 - mae: 0.6143 - val_loss: 0.7278 - val_mae: 0.7278\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.6253 - mae: 0.6253 - val_loss: 0.7323 - val_mae: 0.7323\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - 0s 123us/sample - loss: 0.6134 - mae: 0.6134 - val_loss: 0.6128 - val_mae: 0.6128\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.5823 - mae: 0.5823 - val_loss: 0.5857 - val_mae: 0.5857\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.5998 - mae: 0.5998 - val_loss: 0.5641 - val_mae: 0.5641\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.6468 - mae: 0.6468 - val_loss: 0.5642 - val_mae: 0.5642\n",
      "Epoch 19/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.5982 - mae: 0.5982 - val_loss: 0.5511 - val_mae: 0.5511\n",
      "Epoch 20/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.5745 - mae: 0.5745 - val_loss: 0.5184 - val_mae: 0.5184\n",
      "Epoch 21/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 0.5871 - mae: 0.5871 - val_loss: 0.5269 - val_mae: 0.5269\n",
      "Epoch 22/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.5850 - mae: 0.5850 - val_loss: 0.5220 - val_mae: 0.5220\n",
      "830/830 [==============================] - 0s 39us/sample - loss: 0.5220 - mae: 0.5220\n",
      "Val score is 0.5220461487770081\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 1s 2ms/sample - loss: 2.7186 - mae: 2.7186 - val_loss: 2.6650 - val_mae: 2.6650\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 2.4187 - mae: 2.4187 - val_loss: 2.2344 - val_mae: 2.2344\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 2.0945 - mae: 2.0945 - val_loss: 1.7940 - val_mae: 1.7940\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 1.7040 - mae: 1.7040 - val_loss: 1.5430 - val_mae: 1.5430\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 1.3002 - mae: 1.3002 - val_loss: 1.3333 - val_mae: 1.3333\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.9921 - mae: 0.9921 - val_loss: 1.1009 - val_mae: 1.1009\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.8490 - mae: 0.8490 - val_loss: 0.9861 - val_mae: 0.9861\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.8047 - mae: 0.8047 - val_loss: 0.8698 - val_mae: 0.8698\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.7848 - mae: 0.7848 - val_loss: 0.8637 - val_mae: 0.8637\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 0.7460 - mae: 0.7460 - val_loss: 0.8140 - val_mae: 0.8140\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.7289 - mae: 0.7289 - val_loss: 0.8092 - val_mae: 0.8092\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.7151 - mae: 0.7151 - val_loss: 0.7196 - val_mae: 0.7196\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.7333 - mae: 0.7333 - val_loss: 0.7207 - val_mae: 0.7207\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 0.7346 - mae: 0.7346 - val_loss: 0.6478 - val_mae: 0.6478\n",
      "Epoch 15/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.7209 - mae: 0.7209 - val_loss: 0.6432 - val_mae: 0.6432\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.6936 - mae: 0.6936 - val_loss: 0.6198 - val_mae: 0.6198\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 0.7061 - mae: 0.7061 - val_loss: 0.6652 - val_mae: 0.6652\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.7425 - mae: 0.7425 - val_loss: 0.6749 - val_mae: 0.6749\n",
      "830/830 [==============================] - 0s 39us/sample - loss: 0.6749 - mae: 0.6749\n",
      "Val score is 0.6749351620674133\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 2s 2ms/sample - loss: 2.8101 - mae: 2.8101 - val_loss: 2.5221 - val_mae: 2.5221\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 2.5400 - mae: 2.5400 - val_loss: 2.0603 - val_mae: 2.0603\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 2.1920 - mae: 2.1920 - val_loss: 1.5042 - val_mae: 1.5042\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 1.7278 - mae: 1.7278 - val_loss: 0.8408 - val_mae: 0.8408\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 1.1818 - mae: 1.1818 - val_loss: 0.2978 - val_mae: 0.2978\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.6819 - mae: 0.6819 - val_loss: 0.5093 - val_mae: 0.5093\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.4730 - mae: 0.4730 - val_loss: 0.6533 - val_mae: 0.6533\n",
      "830/830 [==============================] - 0s 39us/sample - loss: 0.6533 - mae: 0.6533\n",
      "Val score is 0.6533476710319519\n",
      "DataFrame: mammographic imput_method :MLP model :KNN score:  0.81\n",
      "DataFrame: mammographic imput_method :MLP model :XGB score:  0.84\n",
      "DataFrame: mammographic imput_method :MLP model :MLP score:  0.81\n",
      "DataFrame: marketing imput_method :LOCF model :KNN score:  0.31\n",
      "DataFrame: marketing imput_method :LOCF model :XGB score:  0.33\n",
      "DataFrame: marketing imput_method :LOCF model :MLP score:  0.32\n",
      "DataFrame: marketing imput_method :mean_mode model :KNN score:  0.31\n",
      "DataFrame: marketing imput_method :mean_mode model :XGB score:  0.33\n",
      "DataFrame: marketing imput_method :mean_mode model :MLP score:  0.32\n",
      "DataFrame: marketing imput_method :knn model :KNN score:  0.31\n",
      "DataFrame: marketing imput_method :knn model :XGB score:  0.33\n",
      "DataFrame: marketing imput_method :knn model :MLP score:  0.31\n",
      "[0]\tvalidation_0-mae:1.78765\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.29367\n",
      "[2]\tvalidation_0-mae:0.98416\n",
      "[3]\tvalidation_0-mae:0.78088\n",
      "[4]\tvalidation_0-mae:0.65621\n",
      "[5]\tvalidation_0-mae:0.57367\n",
      "[6]\tvalidation_0-mae:0.52429\n",
      "[7]\tvalidation_0-mae:0.49393\n",
      "[8]\tvalidation_0-mae:0.47288\n",
      "[9]\tvalidation_0-mae:0.45670\n",
      "[10]\tvalidation_0-mae:0.44138\n",
      "[11]\tvalidation_0-mae:0.43789\n",
      "[12]\tvalidation_0-mae:0.43829\n",
      "[13]\tvalidation_0-mae:0.43860\n",
      "Stopping. Best iteration:\n",
      "[11]\tvalidation_0-mae:0.43789\n",
      "\n",
      "[0]\tvalidation_0-mae:2.34613\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.67683\n",
      "[2]\tvalidation_0-mae:1.24964\n",
      "[3]\tvalidation_0-mae:0.98643\n",
      "[4]\tvalidation_0-mae:0.83853\n",
      "[5]\tvalidation_0-mae:0.76945\n",
      "[6]\tvalidation_0-mae:0.73344\n",
      "[7]\tvalidation_0-mae:0.71253\n",
      "[8]\tvalidation_0-mae:0.69913\n",
      "[9]\tvalidation_0-mae:0.68989\n",
      "[10]\tvalidation_0-mae:0.68598\n",
      "[11]\tvalidation_0-mae:0.68139\n",
      "[12]\tvalidation_0-mae:0.67978\n",
      "[13]\tvalidation_0-mae:0.67689\n",
      "[14]\tvalidation_0-mae:0.67634\n",
      "[15]\tvalidation_0-mae:0.67604\n",
      "[16]\tvalidation_0-mae:0.67578\n",
      "[17]\tvalidation_0-mae:0.67559\n",
      "[18]\tvalidation_0-mae:0.67502\n",
      "[19]\tvalidation_0-mae:0.67488\n",
      "[20]\tvalidation_0-mae:0.67486\n",
      "[21]\tvalidation_0-mae:0.67458\n",
      "[22]\tvalidation_0-mae:0.67462\n",
      "[23]\tvalidation_0-mae:0.67451\n",
      "[24]\tvalidation_0-mae:0.67381\n",
      "[25]\tvalidation_0-mae:0.67379\n",
      "[26]\tvalidation_0-mae:0.67372\n",
      "[27]\tvalidation_0-mae:0.67378\n",
      "[28]\tvalidation_0-mae:0.67376\n",
      "Stopping. Best iteration:\n",
      "[26]\tvalidation_0-mae:0.67372\n",
      "\n",
      "[0]\tvalidation_0-mae:2.43618\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:2.03693\n",
      "[2]\tvalidation_0-mae:1.81360\n",
      "[3]\tvalidation_0-mae:1.68205\n",
      "[4]\tvalidation_0-mae:1.59834\n",
      "[5]\tvalidation_0-mae:1.54253\n",
      "[6]\tvalidation_0-mae:1.50030\n",
      "[7]\tvalidation_0-mae:1.47205\n",
      "[8]\tvalidation_0-mae:1.45099\n",
      "[9]\tvalidation_0-mae:1.42847\n",
      "[10]\tvalidation_0-mae:1.41752\n",
      "[11]\tvalidation_0-mae:1.40885\n",
      "[12]\tvalidation_0-mae:1.40148\n",
      "[13]\tvalidation_0-mae:1.39329\n",
      "[14]\tvalidation_0-mae:1.39079\n",
      "[15]\tvalidation_0-mae:1.38412\n",
      "[16]\tvalidation_0-mae:1.37161\n",
      "[17]\tvalidation_0-mae:1.36779\n",
      "[18]\tvalidation_0-mae:1.36506\n",
      "[19]\tvalidation_0-mae:1.36043\n",
      "[20]\tvalidation_0-mae:1.35362\n",
      "[21]\tvalidation_0-mae:1.35021\n",
      "[22]\tvalidation_0-mae:1.34476\n",
      "[23]\tvalidation_0-mae:1.33833\n",
      "[24]\tvalidation_0-mae:1.33760\n",
      "[25]\tvalidation_0-mae:1.33596\n",
      "[26]\tvalidation_0-mae:1.33193\n",
      "[27]\tvalidation_0-mae:1.33075\n",
      "[28]\tvalidation_0-mae:1.32872\n",
      "[29]\tvalidation_0-mae:1.32815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalidation_0-mae:1.32480\n",
      "[31]\tvalidation_0-mae:1.32338\n",
      "[32]\tvalidation_0-mae:1.32120\n",
      "[33]\tvalidation_0-mae:1.31597\n",
      "[34]\tvalidation_0-mae:1.31246\n",
      "[35]\tvalidation_0-mae:1.31172\n",
      "[36]\tvalidation_0-mae:1.31002\n",
      "[37]\tvalidation_0-mae:1.30698\n",
      "[38]\tvalidation_0-mae:1.30506\n",
      "[39]\tvalidation_0-mae:1.30293\n",
      "[40]\tvalidation_0-mae:1.30297\n",
      "[41]\tvalidation_0-mae:1.29911\n",
      "[42]\tvalidation_0-mae:1.29934\n",
      "[43]\tvalidation_0-mae:1.29573\n",
      "[44]\tvalidation_0-mae:1.29380\n",
      "[45]\tvalidation_0-mae:1.29276\n",
      "[46]\tvalidation_0-mae:1.29068\n",
      "[47]\tvalidation_0-mae:1.28848\n",
      "[48]\tvalidation_0-mae:1.28722\n",
      "[49]\tvalidation_0-mae:1.28685\n",
      "[50]\tvalidation_0-mae:1.28324\n",
      "[51]\tvalidation_0-mae:1.28095\n",
      "[52]\tvalidation_0-mae:1.27989\n",
      "[53]\tvalidation_0-mae:1.27809\n",
      "[54]\tvalidation_0-mae:1.27725\n",
      "[55]\tvalidation_0-mae:1.27675\n",
      "[56]\tvalidation_0-mae:1.27538\n",
      "[57]\tvalidation_0-mae:1.27383\n",
      "[58]\tvalidation_0-mae:1.27249\n",
      "[59]\tvalidation_0-mae:1.27241\n",
      "[60]\tvalidation_0-mae:1.27233\n",
      "[61]\tvalidation_0-mae:1.27190\n",
      "[62]\tvalidation_0-mae:1.27100\n",
      "[63]\tvalidation_0-mae:1.27078\n",
      "[64]\tvalidation_0-mae:1.26919\n",
      "[65]\tvalidation_0-mae:1.26683\n",
      "[66]\tvalidation_0-mae:1.26446\n",
      "[67]\tvalidation_0-mae:1.26305\n",
      "[68]\tvalidation_0-mae:1.26179\n",
      "[69]\tvalidation_0-mae:1.26166\n",
      "[70]\tvalidation_0-mae:1.26055\n",
      "[71]\tvalidation_0-mae:1.26060\n",
      "[72]\tvalidation_0-mae:1.25933\n",
      "[73]\tvalidation_0-mae:1.25862\n",
      "[74]\tvalidation_0-mae:1.25705\n",
      "[75]\tvalidation_0-mae:1.25694\n",
      "[76]\tvalidation_0-mae:1.25570\n",
      "[77]\tvalidation_0-mae:1.25456\n",
      "[78]\tvalidation_0-mae:1.25463\n",
      "[79]\tvalidation_0-mae:1.25321\n",
      "[80]\tvalidation_0-mae:1.25263\n",
      "[81]\tvalidation_0-mae:1.24982\n",
      "[82]\tvalidation_0-mae:1.24933\n",
      "[83]\tvalidation_0-mae:1.24842\n",
      "[84]\tvalidation_0-mae:1.24762\n",
      "[85]\tvalidation_0-mae:1.24768\n",
      "[86]\tvalidation_0-mae:1.24645\n",
      "[87]\tvalidation_0-mae:1.24647\n",
      "[88]\tvalidation_0-mae:1.24636\n",
      "[89]\tvalidation_0-mae:1.24614\n",
      "[90]\tvalidation_0-mae:1.24471\n",
      "[91]\tvalidation_0-mae:1.24472\n",
      "[92]\tvalidation_0-mae:1.24507\n",
      "Stopping. Best iteration:\n",
      "[90]\tvalidation_0-mae:1.24471\n",
      "\n",
      "[0]\tvalidation_0-mae:2.62889\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.96806\n",
      "[2]\tvalidation_0-mae:1.57878\n",
      "[3]\tvalidation_0-mae:1.33957\n",
      "[4]\tvalidation_0-mae:1.18160\n",
      "[5]\tvalidation_0-mae:1.07360\n",
      "[6]\tvalidation_0-mae:1.00949\n",
      "[7]\tvalidation_0-mae:0.96554\n",
      "[8]\tvalidation_0-mae:0.93365\n",
      "[9]\tvalidation_0-mae:0.91005\n",
      "[10]\tvalidation_0-mae:0.89360\n",
      "[11]\tvalidation_0-mae:0.88230\n",
      "[12]\tvalidation_0-mae:0.87366\n",
      "[13]\tvalidation_0-mae:0.86733\n",
      "[14]\tvalidation_0-mae:0.86314\n",
      "[15]\tvalidation_0-mae:0.86052\n",
      "[16]\tvalidation_0-mae:0.85891\n",
      "[17]\tvalidation_0-mae:0.85433\n",
      "[18]\tvalidation_0-mae:0.85180\n",
      "[19]\tvalidation_0-mae:0.84993\n",
      "[20]\tvalidation_0-mae:0.85064\n",
      "[21]\tvalidation_0-mae:0.85120\n",
      "Stopping. Best iteration:\n",
      "[19]\tvalidation_0-mae:0.84993\n",
      "\n",
      "[0]\tvalidation_0-mae:1.66255\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:1.22280\n",
      "[2]\tvalidation_0-mae:0.95693\n",
      "[3]\tvalidation_0-mae:0.78965\n",
      "[4]\tvalidation_0-mae:0.68732\n",
      "[5]\tvalidation_0-mae:0.64010\n",
      "[6]\tvalidation_0-mae:0.61972\n",
      "[7]\tvalidation_0-mae:0.60599\n",
      "[8]\tvalidation_0-mae:0.60137\n",
      "[9]\tvalidation_0-mae:0.60253\n",
      "[10]\tvalidation_0-mae:0.60440\n",
      "Stopping. Best iteration:\n",
      "[8]\tvalidation_0-mae:0.60137\n",
      "\n",
      "[0]\tvalidation_0-mae:0.94447\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.69355\n",
      "[2]\tvalidation_0-mae:0.53548\n",
      "[3]\tvalidation_0-mae:0.44242\n",
      "[4]\tvalidation_0-mae:0.38684\n",
      "[5]\tvalidation_0-mae:0.35516\n",
      "[6]\tvalidation_0-mae:0.33889\n",
      "[7]\tvalidation_0-mae:0.32845\n",
      "[8]\tvalidation_0-mae:0.32196\n",
      "[9]\tvalidation_0-mae:0.31549\n",
      "[10]\tvalidation_0-mae:0.31490\n",
      "[11]\tvalidation_0-mae:0.31462\n",
      "[12]\tvalidation_0-mae:0.31446\n",
      "[13]\tvalidation_0-mae:0.31441\n",
      "[14]\tvalidation_0-mae:0.31435\n",
      "[15]\tvalidation_0-mae:0.31434\n",
      "[16]\tvalidation_0-mae:0.31433\n",
      "[17]\tvalidation_0-mae:0.31433\n",
      "[18]\tvalidation_0-mae:0.31057\n",
      "[19]\tvalidation_0-mae:0.31059\n",
      "[20]\tvalidation_0-mae:0.31060\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-mae:0.31057\n",
      "\n",
      "[0]\tvalidation_0-mae:0.97706\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.78399\n",
      "[2]\tvalidation_0-mae:0.68701\n",
      "[3]\tvalidation_0-mae:0.65315\n",
      "[4]\tvalidation_0-mae:0.64702\n",
      "[5]\tvalidation_0-mae:0.64620\n",
      "[6]\tvalidation_0-mae:0.64407\n",
      "[7]\tvalidation_0-mae:0.63574\n",
      "[8]\tvalidation_0-mae:0.63725\n",
      "[9]\tvalidation_0-mae:0.63298\n",
      "[10]\tvalidation_0-mae:0.63060\n",
      "[11]\tvalidation_0-mae:0.63239\n",
      "[12]\tvalidation_0-mae:0.62989\n",
      "[13]\tvalidation_0-mae:0.63058\n",
      "[14]\tvalidation_0-mae:0.62934\n",
      "[15]\tvalidation_0-mae:0.62981\n",
      "[16]\tvalidation_0-mae:0.63028\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-mae:0.62934\n",
      "\n",
      "[0]\tvalidation_0-mae:3.87111\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:2.93136\n",
      "[2]\tvalidation_0-mae:2.39491\n",
      "[3]\tvalidation_0-mae:2.01620\n",
      "[4]\tvalidation_0-mae:1.76481\n",
      "[5]\tvalidation_0-mae:1.60086\n",
      "[6]\tvalidation_0-mae:1.49240\n",
      "[7]\tvalidation_0-mae:1.41377\n",
      "[8]\tvalidation_0-mae:1.35657\n",
      "[9]\tvalidation_0-mae:1.31141\n",
      "[10]\tvalidation_0-mae:1.28424\n",
      "[11]\tvalidation_0-mae:1.26278\n",
      "[12]\tvalidation_0-mae:1.24805\n",
      "[13]\tvalidation_0-mae:1.23522\n",
      "[14]\tvalidation_0-mae:1.22803\n",
      "[15]\tvalidation_0-mae:1.22086\n",
      "[16]\tvalidation_0-mae:1.21176\n",
      "[17]\tvalidation_0-mae:1.20535\n",
      "[18]\tvalidation_0-mae:1.20266\n",
      "[19]\tvalidation_0-mae:1.19939\n",
      "[20]\tvalidation_0-mae:1.19507\n",
      "[21]\tvalidation_0-mae:1.19198\n",
      "[22]\tvalidation_0-mae:1.18789\n",
      "[23]\tvalidation_0-mae:1.18208\n",
      "[24]\tvalidation_0-mae:1.17809\n",
      "[25]\tvalidation_0-mae:1.17460\n",
      "[26]\tvalidation_0-mae:1.17025\n",
      "[27]\tvalidation_0-mae:1.16967\n",
      "[28]\tvalidation_0-mae:1.16760\n",
      "[29]\tvalidation_0-mae:1.16553\n",
      "[30]\tvalidation_0-mae:1.16475\n",
      "[31]\tvalidation_0-mae:1.16488\n",
      "[32]\tvalidation_0-mae:1.16254\n",
      "[33]\tvalidation_0-mae:1.16158\n",
      "[34]\tvalidation_0-mae:1.16097\n",
      "[35]\tvalidation_0-mae:1.15989\n",
      "[36]\tvalidation_0-mae:1.15699\n",
      "[37]\tvalidation_0-mae:1.15526\n",
      "[38]\tvalidation_0-mae:1.15430\n",
      "[39]\tvalidation_0-mae:1.15040\n",
      "[40]\tvalidation_0-mae:1.14982\n",
      "[41]\tvalidation_0-mae:1.14928\n",
      "[42]\tvalidation_0-mae:1.14952\n",
      "[43]\tvalidation_0-mae:1.14968\n",
      "Stopping. Best iteration:\n",
      "[41]\tvalidation_0-mae:1.14928\n",
      "\n",
      "[0]\tvalidation_0-mae:0.44089\n",
      "Will train until validation_0-mae hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mae:0.31119\n",
      "[2]\tvalidation_0-mae:0.24230\n",
      "[3]\tvalidation_0-mae:0.20090\n",
      "[4]\tvalidation_0-mae:0.17345\n",
      "[5]\tvalidation_0-mae:0.15680\n",
      "[6]\tvalidation_0-mae:0.15489\n",
      "[7]\tvalidation_0-mae:0.16744\n",
      "[8]\tvalidation_0-mae:0.17563\n",
      "Stopping. Best iteration:\n",
      "[6]\tvalidation_0-mae:0.15489\n",
      "\n",
      "DataFrame: marketing imput_method :trees model :KNN score:  0.31\n",
      "DataFrame: marketing imput_method :trees model :XGB score:  0.34\n",
      "DataFrame: marketing imput_method :trees model :MLP score:  0.32\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 277us/sample - loss: 1.8399 - mae: 1.8399 - val_loss: 0.7659 - val_mae: 0.7659\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.7110 - mae: 0.7110 - val_loss: 0.5675 - val_mae: 0.5675\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.6381 - mae: 0.6381 - val_loss: 0.5066 - val_mae: 0.5066\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 121us/sample - loss: 0.6091 - mae: 0.6091 - val_loss: 0.4859 - val_mae: 0.4859\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 129us/sample - loss: 0.5809 - mae: 0.5809 - val_loss: 0.4573 - val_mae: 0.4573\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 126us/sample - loss: 0.5808 - mae: 0.5808 - val_loss: 0.4429 - val_mae: 0.4429\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 123us/sample - loss: 0.5671 - mae: 0.5671 - val_loss: 0.4496 - val_mae: 0.4496\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 120us/sample - loss: 0.5364 - mae: 0.5364 - val_loss: 0.4371 - val_mae: 0.4371\n",
      "Epoch 9/100\n",
      "6876/6876 [==============================] - 1s 120us/sample - loss: 0.5560 - mae: 0.5560 - val_loss: 0.4375 - val_mae: 0.4375\n",
      "Epoch 10/100\n",
      "6876/6876 [==============================] - 1s 127us/sample - loss: 0.5472 - mae: 0.5472 - val_loss: 0.4337 - val_mae: 0.4337\n",
      "Epoch 11/100\n",
      "6876/6876 [==============================] - 1s 120us/sample - loss: 0.5282 - mae: 0.5282 - val_loss: 0.4212 - val_mae: 0.4212\n",
      "Epoch 12/100\n",
      "6876/6876 [==============================] - 1s 118us/sample - loss: 0.5257 - mae: 0.5257 - val_loss: 0.4258 - val_mae: 0.4258\n",
      "Epoch 13/100\n",
      "6876/6876 [==============================] - 1s 117us/sample - loss: 0.5393 - mae: 0.5393 - val_loss: 0.4107 - val_mae: 0.4107\n",
      "Epoch 14/100\n",
      "6876/6876 [==============================] - 1s 115us/sample - loss: 0.5244 - mae: 0.5244 - val_loss: 0.4225 - val_mae: 0.4225\n",
      "Epoch 15/100\n",
      "6876/6876 [==============================] - 1s 118us/sample - loss: 0.5291 - mae: 0.5291 - val_loss: 0.4048 - val_mae: 0.4048\n",
      "Epoch 16/100\n",
      "6876/6876 [==============================] - 1s 119us/sample - loss: 0.5276 - mae: 0.5276 - val_loss: 0.4003 - val_mae: 0.4003\n",
      "Epoch 17/100\n",
      "6876/6876 [==============================] - 1s 124us/sample - loss: 0.5176 - mae: 0.5176 - val_loss: 0.3913 - val_mae: 0.3913\n",
      "Epoch 18/100\n",
      "6876/6876 [==============================] - 1s 118us/sample - loss: 0.5005 - mae: 0.5005 - val_loss: 0.3957 - val_mae: 0.3957\n",
      "Epoch 19/100\n",
      "6876/6876 [==============================] - 1s 122us/sample - loss: 0.5149 - mae: 0.5149 - val_loss: 0.3947 - val_mae: 0.3947\n",
      "6876/6876 [==============================] - 0s 34us/sample - loss: 0.3947 - mae: 0.3947\n",
      "Val score is 0.3947280943393707\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 279us/sample - loss: 2.3470 - mae: 2.3470 - val_loss: 0.8329 - val_mae: 0.8329\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 117us/sample - loss: 0.8811 - mae: 0.8811 - val_loss: 0.7774 - val_mae: 0.7774\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 128us/sample - loss: 0.8095 - mae: 0.8095 - val_loss: 0.7257 - val_mae: 0.7257\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 133us/sample - loss: 0.7884 - mae: 0.7884 - val_loss: 0.7206 - val_mae: 0.7206\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 136us/sample - loss: 0.7637 - mae: 0.7637 - val_loss: 0.7094 - val_mae: 0.7094\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 122us/sample - loss: 0.7540 - mae: 0.7540 - val_loss: 0.6950 - val_mae: 0.6950\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 129us/sample - loss: 0.7423 - mae: 0.7423 - val_loss: 0.6999 - val_mae: 0.6999\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 123us/sample - loss: 0.7528 - mae: 0.7528 - val_loss: 0.6990 - val_mae: 0.6990\n",
      "6876/6876 [==============================] - 0s 39us/sample - loss: 0.6990 - mae: 0.6990\n",
      "Val score is 0.698995053768158\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 342us/sample - loss: 2.5559 - mae: 2.5559 - val_loss: 1.7103 - val_mae: 1.7103\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 1.6299 - mae: 1.6299 - val_loss: 1.4883 - val_mae: 1.4883\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 1.5492 - mae: 1.5492 - val_loss: 1.4353 - val_mae: 1.4353\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 1.5110 - mae: 1.5110 - val_loss: 1.4374 - val_mae: 1.4374\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.5020 - mae: 1.5020 - val_loss: 1.4107 - val_mae: 1.4107\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 1.4699 - mae: 1.4699 - val_loss: 1.4014 - val_mae: 1.4014\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 118us/sample - loss: 1.4631 - mae: 1.4631 - val_loss: 1.4053 - val_mae: 1.4053\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.4500 - mae: 1.4500 - val_loss: 1.3679 - val_mae: 1.3679\n",
      "Epoch 9/100\n",
      "6876/6876 [==============================] - 1s 109us/sample - loss: 1.4386 - mae: 1.4386 - val_loss: 1.3489 - val_mae: 1.3489\n",
      "Epoch 10/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 1.4364 - mae: 1.4364 - val_loss: 1.3608 - val_mae: 1.3608\n",
      "Epoch 11/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.4316 - mae: 1.4316 - val_loss: 1.3281 - val_mae: 1.3281\n",
      "Epoch 12/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.4204 - mae: 1.4204 - val_loss: 1.3333 - val_mae: 1.3333\n",
      "Epoch 13/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 1.4128 - mae: 1.4128 - val_loss: 1.3519 - val_mae: 1.3519\n",
      "6876/6876 [==============================] - 0s 33us/sample - loss: 1.3519 - mae: 1.3519\n",
      "Val score is 1.3518739938735962\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 272us/sample - loss: 2.6806 - mae: 2.6806 - val_loss: 1.0559 - val_mae: 1.0559\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.9699 - mae: 0.9699 - val_loss: 0.8712 - val_mae: 0.8712\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.8933 - mae: 0.8933 - val_loss: 0.8495 - val_mae: 0.8495\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.8532 - mae: 0.8532 - val_loss: 0.8013 - val_mae: 0.8013\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.8437 - mae: 0.8437 - val_loss: 0.7896 - val_mae: 0.7896\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.8271 - mae: 0.8271 - val_loss: 0.7935 - val_mae: 0.7935\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.8134 - mae: 0.8134 - val_loss: 0.7915 - val_mae: 0.7915\n",
      "6876/6876 [==============================] - 0s 33us/sample - loss: 0.7915 - mae: 0.7915\n",
      "Val score is 0.7914993762969971\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 275us/sample - loss: 1.6703 - mae: 1.6703 - val_loss: 0.7352 - val_mae: 0.7352\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.7648 - mae: 0.7648 - val_loss: 0.6472 - val_mae: 0.6472\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.7184 - mae: 0.7184 - val_loss: 0.6535 - val_mae: 0.6535\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.6946 - mae: 0.6946 - val_loss: 0.6168 - val_mae: 0.6168\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.6846 - mae: 0.6846 - val_loss: 0.5978 - val_mae: 0.5978\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 0.6693 - mae: 0.6693 - val_loss: 0.5855 - val_mae: 0.5855\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.6595 - mae: 0.6595 - val_loss: 0.5862 - val_mae: 0.5862\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.6633 - mae: 0.6633 - val_loss: 0.5855 - val_mae: 0.5855\n",
      "6876/6876 [==============================] - 0s 36us/sample - loss: 0.5855 - mae: 0.5855\n",
      "Val score is 0.5855262279510498\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 278us/sample - loss: 0.9388 - mae: 0.9388 - val_loss: 0.4145 - val_mae: 0.4145\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 116us/sample - loss: 0.4330 - mae: 0.4330 - val_loss: 0.3585 - val_mae: 0.3585\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.3780 - mae: 0.3780 - val_loss: 0.3266 - val_mae: 0.3266\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.3546 - mae: 0.3546 - val_loss: 0.3050 - val_mae: 0.3050\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 0.3390 - mae: 0.3390 - val_loss: 0.2985 - val_mae: 0.2985\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 0.3244 - mae: 0.3244 - val_loss: 0.2813 - val_mae: 0.2813\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.3192 - mae: 0.3192 - val_loss: 0.2948 - val_mae: 0.2948\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.3138 - mae: 0.3138 - val_loss: 0.2771 - val_mae: 0.2771\n",
      "Epoch 9/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 0.3107 - mae: 0.3107 - val_loss: 0.2790 - val_mae: 0.2790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.3067 - mae: 0.3067 - val_loss: 0.2626 - val_mae: 0.2626\n",
      "Epoch 11/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.2999 - mae: 0.2999 - val_loss: 0.2683 - val_mae: 0.2683\n",
      "Epoch 12/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 0.3009 - mae: 0.3009 - val_loss: 0.2531 - val_mae: 0.2531\n",
      "Epoch 13/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 0.2941 - mae: 0.2941 - val_loss: 0.2519 - val_mae: 0.2519\n",
      "Epoch 14/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.2977 - mae: 0.2977 - val_loss: 0.2607 - val_mae: 0.2607\n",
      "Epoch 15/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.2914 - mae: 0.2914 - val_loss: 0.2652 - val_mae: 0.2652\n",
      "6876/6876 [==============================] - 0s 34us/sample - loss: 0.2652 - mae: 0.2652\n",
      "Val score is 0.26520881056785583\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 281us/sample - loss: 1.1187 - mae: 1.1187 - val_loss: 0.6928 - val_mae: 0.6928\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.6897 - mae: 0.6897 - val_loss: 0.6330 - val_mae: 0.6330\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 0.6497 - mae: 0.6497 - val_loss: 0.5853 - val_mae: 0.5853\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 0.6189 - mae: 0.6189 - val_loss: 0.5681 - val_mae: 0.5681\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.6035 - mae: 0.6035 - val_loss: 0.5593 - val_mae: 0.5593\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 0.5882 - mae: 0.5882 - val_loss: 0.5483 - val_mae: 0.5483\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.5849 - mae: 0.5849 - val_loss: 0.5359 - val_mae: 0.5359\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 0.5701 - mae: 0.5701 - val_loss: 0.5287 - val_mae: 0.5287\n",
      "Epoch 9/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.5598 - mae: 0.5598 - val_loss: 0.5058 - val_mae: 0.5058\n",
      "Epoch 10/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 0.5590 - mae: 0.5590 - val_loss: 0.5081 - val_mae: 0.5081\n",
      "Epoch 11/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 0.5473 - mae: 0.5473 - val_loss: 0.5113 - val_mae: 0.5113\n",
      "6876/6876 [==============================] - 0s 35us/sample - loss: 0.5113 - mae: 0.5113\n",
      "Val score is 0.5113314986228943\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 277us/sample - loss: 4.2832 - mae: 4.2832 - val_loss: 1.6325 - val_mae: 1.6325\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 113us/sample - loss: 1.4101 - mae: 1.4101 - val_loss: 1.1547 - val_mae: 1.1547\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 122us/sample - loss: 1.1964 - mae: 1.1964 - val_loss: 1.0752 - val_mae: 1.0752\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 1.1296 - mae: 1.1296 - val_loss: 1.0573 - val_mae: 1.0573\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.0889 - mae: 1.0889 - val_loss: 1.0247 - val_mae: 1.0247\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.0766 - mae: 1.0766 - val_loss: 1.0176 - val_mae: 1.0176\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 1.0604 - mae: 1.0604 - val_loss: 1.0001 - val_mae: 1.0001\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 1.0456 - mae: 1.0456 - val_loss: 0.9816 - val_mae: 0.9816\n",
      "Epoch 9/100\n",
      "6876/6876 [==============================] - 1s 114us/sample - loss: 1.0340 - mae: 1.0340 - val_loss: 0.9915 - val_mae: 0.9915\n",
      "Epoch 10/100\n",
      "6876/6876 [==============================] - 1s 115us/sample - loss: 1.0182 - mae: 1.0182 - val_loss: 0.9713 - val_mae: 0.9713\n",
      "Epoch 11/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 1.0163 - mae: 1.0163 - val_loss: 0.9604 - val_mae: 0.9604\n",
      "Epoch 12/100\n",
      "6876/6876 [==============================] - 1s 111us/sample - loss: 1.0125 - mae: 1.0125 - val_loss: 0.9657 - val_mae: 0.9657\n",
      "Epoch 13/100\n",
      "6876/6876 [==============================] - 1s 110us/sample - loss: 1.0048 - mae: 1.0048 - val_loss: 0.9636 - val_mae: 0.9636\n",
      "6876/6876 [==============================] - 0s 33us/sample - loss: 0.9636 - mae: 0.9636\n",
      "Val score is 0.9636116623878479\n",
      "Train on 6876 samples, validate on 6876 samples\n",
      "Epoch 1/100\n",
      "6876/6876 [==============================] - 2s 271us/sample - loss: 0.5757 - mae: 0.5757 - val_loss: 0.2850 - val_mae: 0.2850\n",
      "Epoch 2/100\n",
      "6876/6876 [==============================] - 1s 112us/sample - loss: 0.2983 - mae: 0.2983 - val_loss: 0.2466 - val_mae: 0.2466\n",
      "Epoch 3/100\n",
      "6876/6876 [==============================] - 1s 93us/sample - loss: 0.2417 - mae: 0.2417 - val_loss: 0.2014 - val_mae: 0.2014\n",
      "Epoch 4/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.2083 - mae: 0.2083 - val_loss: 0.1780 - val_mae: 0.1780\n",
      "Epoch 5/100\n",
      "6876/6876 [==============================] - 1s 86us/sample - loss: 0.1850 - mae: 0.1850 - val_loss: 0.1671 - val_mae: 0.1671\n",
      "Epoch 6/100\n",
      "6876/6876 [==============================] - 1s 84us/sample - loss: 0.1726 - mae: 0.1726 - val_loss: 0.1614 - val_mae: 0.1614\n",
      "Epoch 7/100\n",
      "6876/6876 [==============================] - 1s 88us/sample - loss: 0.1629 - mae: 0.1629 - val_loss: 0.1511 - val_mae: 0.1511\n",
      "Epoch 8/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.1546 - mae: 0.1546 - val_loss: 0.1464 - val_mae: 0.1464\n",
      "Epoch 9/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.1490 - mae: 0.1490 - val_loss: 0.1442 - val_mae: 0.1442\n",
      "Epoch 10/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.1429 - mae: 0.1429 - val_loss: 0.1399 - val_mae: 0.1399\n",
      "Epoch 11/100\n",
      "6876/6876 [==============================] - 1s 84us/sample - loss: 0.1402 - mae: 0.1402 - val_loss: 0.1346 - val_mae: 0.1346\n",
      "Epoch 12/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.1370 - mae: 0.1370 - val_loss: 0.1339 - val_mae: 0.1339\n",
      "Epoch 13/100\n",
      "6876/6876 [==============================] - 1s 83us/sample - loss: 0.1348 - mae: 0.1348 - val_loss: 0.1309 - val_mae: 0.1309\n",
      "Epoch 14/100\n",
      "6876/6876 [==============================] - 1s 84us/sample - loss: 0.1323 - mae: 0.1323 - val_loss: 0.1272 - val_mae: 0.1272\n",
      "Epoch 15/100\n",
      "6876/6876 [==============================] - 1s 82us/sample - loss: 0.1312 - mae: 0.1312 - val_loss: 0.1256 - val_mae: 0.1256\n",
      "Epoch 16/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.1303 - mae: 0.1303 - val_loss: 0.1287 - val_mae: 0.1287\n",
      "Epoch 17/100\n",
      "6876/6876 [==============================] - 1s 85us/sample - loss: 0.1284 - mae: 0.1284 - val_loss: 0.1305 - val_mae: 0.1305\n",
      "6876/6876 [==============================] - 0s 25us/sample - loss: 0.1305 - mae: 0.1305\n",
      "Val score is 0.13047513365745544\n",
      "DataFrame: marketing imput_method :MLP model :KNN score:  0.31\n",
      "DataFrame: marketing imput_method :MLP model :XGB score:  0.34\n",
      "DataFrame: marketing imput_method :MLP model :MLP score:  0.32\n",
      "DataFrame: mushroom imput_method :LOCF model :KNN score:  0.91\n",
      "DataFrame: mushroom imput_method :LOCF model :XGB score:  0.89\n",
      "DataFrame: mushroom imput_method :LOCF model :MLP score:  1.0\n",
      "DataFrame: mushroom imput_method :mean_mode model :KNN score:  0.9\n",
      "DataFrame: mushroom imput_method :mean_mode model :XGB score:  0.89\n",
      "DataFrame: mushroom imput_method :mean_mode model :MLP score:  1.0\n",
      "DataFrame: mushroom imput_method :knn model :KNN score:  0.9\n",
      "DataFrame: mushroom imput_method :knn model :XGB score:  0.89\n",
      "DataFrame: mushroom imput_method :knn model :MLP score:  1.0\n",
      "[0]\tvalidation_0-mlogloss:0.88187\n",
      "Will train until validation_0-mlogloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mlogloss:0.61307\n",
      "[2]\tvalidation_0-mlogloss:0.44039\n",
      "[3]\tvalidation_0-mlogloss:0.32256\n",
      "[4]\tvalidation_0-mlogloss:0.23712\n",
      "[5]\tvalidation_0-mlogloss:0.17662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\tvalidation_0-mlogloss:0.13497\n",
      "[7]\tvalidation_0-mlogloss:0.10444\n",
      "[8]\tvalidation_0-mlogloss:0.08278\n",
      "[9]\tvalidation_0-mlogloss:0.06764\n",
      "[10]\tvalidation_0-mlogloss:0.05473\n",
      "[11]\tvalidation_0-mlogloss:0.04561\n",
      "[12]\tvalidation_0-mlogloss:0.04120\n",
      "[13]\tvalidation_0-mlogloss:0.03734\n",
      "[14]\tvalidation_0-mlogloss:0.03651\n",
      "[15]\tvalidation_0-mlogloss:0.03539\n",
      "[16]\tvalidation_0-mlogloss:0.03248\n",
      "[17]\tvalidation_0-mlogloss:0.03220\n",
      "[18]\tvalidation_0-mlogloss:0.02907\n",
      "[19]\tvalidation_0-mlogloss:0.02894\n",
      "[20]\tvalidation_0-mlogloss:0.02803\n",
      "[21]\tvalidation_0-mlogloss:0.02796\n",
      "[22]\tvalidation_0-mlogloss:0.02792\n",
      "[23]\tvalidation_0-mlogloss:0.02789\n",
      "[24]\tvalidation_0-mlogloss:0.02787\n",
      "[25]\tvalidation_0-mlogloss:0.02786\n",
      "[26]\tvalidation_0-mlogloss:0.02784\n",
      "[27]\tvalidation_0-mlogloss:0.02784\n",
      "[28]\tvalidation_0-mlogloss:0.02783\n",
      "[29]\tvalidation_0-mlogloss:0.02783\n",
      "[30]\tvalidation_0-mlogloss:0.02636\n",
      "[31]\tvalidation_0-mlogloss:0.02636\n",
      "[32]\tvalidation_0-mlogloss:0.02636\n",
      "[33]\tvalidation_0-mlogloss:0.02636\n",
      "[34]\tvalidation_0-mlogloss:0.02636\n",
      "[35]\tvalidation_0-mlogloss:0.02636\n",
      "Stopping. Best iteration:\n",
      "[33]\tvalidation_0-mlogloss:0.02636\n",
      "\n",
      "DataFrame: mushroom imput_method :trees model :KNN score:  0.92\n",
      "DataFrame: mushroom imput_method :trees model :XGB score:  0.91\n",
      "DataFrame: mushroom imput_method :trees model :MLP score:  0.99\n",
      "Train on 5644 samples, validate on 5644 samples\n",
      "Epoch 1/100\n",
      "5644/5644 [==============================] - 2s 416us/sample - loss: 0.3719 - accuracy: 0.8831 - val_loss: 0.0569 - val_accuracy: 0.9989\n",
      "Epoch 2/100\n",
      "5644/5644 [==============================] - 1s 124us/sample - loss: 0.0301 - accuracy: 0.9986 - val_loss: 0.0080 - val_accuracy: 0.9996\n",
      "Epoch 3/100\n",
      "5644/5644 [==============================] - 1s 126us/sample - loss: 0.0112 - accuracy: 0.9996 - val_loss: 0.0028 - val_accuracy: 0.9998\n",
      "Epoch 4/100\n",
      "5644/5644 [==============================] - 1s 122us/sample - loss: 0.0065 - accuracy: 0.9996 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5644/5644 [==============================] - 1s 123us/sample - loss: 0.0040 - accuracy: 1.0000 - val_loss: 8.5576e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5644/5644 [==============================] - 1s 131us/sample - loss: 0.0063 - accuracy: 0.9989 - val_loss: 6.5448e-04 - val_accuracy: 1.0000\n",
      "5644/5644 [==============================] - 0s 35us/sample - loss: 6.5448e-04 - accuracy: 1.0000\n",
      "Val score is 1.0\n",
      "DataFrame: mushroom imput_method :MLP model :KNN score:  0.9\n",
      "DataFrame: mushroom imput_method :MLP model :XGB score:  0.89\n",
      "DataFrame: mushroom imput_method :MLP model :MLP score:  1.0\n",
      "DataFrame: adult imput_method :LOCF model :KNN score:  0.8\n",
      "DataFrame: adult imput_method :LOCF model :XGB score:  0.87\n",
      "DataFrame: adult imput_method :LOCF model :MLP score:  0.84\n",
      "DataFrame: adult imput_method :mean_mode model :KNN score:  0.8\n",
      "DataFrame: adult imput_method :mean_mode model :XGB score:  0.87\n",
      "DataFrame: adult imput_method :mean_mode model :MLP score:  0.84\n",
      "DataFrame: adult imput_method :knn model :KNN score:  0.8\n",
      "DataFrame: adult imput_method :knn model :XGB score:  0.87\n",
      "DataFrame: adult imput_method :knn model :MLP score:  0.85\n",
      "[0]\tvalidation_0-mlogloss:1.49647\n",
      "Will train until validation_0-mlogloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mlogloss:1.27254\n",
      "[2]\tvalidation_0-mlogloss:1.13831\n",
      "[3]\tvalidation_0-mlogloss:1.04964\n",
      "[4]\tvalidation_0-mlogloss:0.98891\n",
      "[5]\tvalidation_0-mlogloss:0.94574\n",
      "[6]\tvalidation_0-mlogloss:0.91352\n",
      "[7]\tvalidation_0-mlogloss:0.88989\n",
      "[8]\tvalidation_0-mlogloss:0.87199\n",
      "[9]\tvalidation_0-mlogloss:0.85808\n",
      "[10]\tvalidation_0-mlogloss:0.84626\n",
      "[11]\tvalidation_0-mlogloss:0.83806\n",
      "[12]\tvalidation_0-mlogloss:0.83049\n",
      "[13]\tvalidation_0-mlogloss:0.82476\n",
      "[14]\tvalidation_0-mlogloss:0.81975\n",
      "[15]\tvalidation_0-mlogloss:0.81654\n",
      "[16]\tvalidation_0-mlogloss:0.81353\n",
      "[17]\tvalidation_0-mlogloss:0.81083\n",
      "[18]\tvalidation_0-mlogloss:0.80841\n",
      "[19]\tvalidation_0-mlogloss:0.80601\n",
      "[20]\tvalidation_0-mlogloss:0.80405\n",
      "[21]\tvalidation_0-mlogloss:0.80280\n",
      "[22]\tvalidation_0-mlogloss:0.80092\n",
      "[23]\tvalidation_0-mlogloss:0.80019\n",
      "[24]\tvalidation_0-mlogloss:0.79976\n",
      "[25]\tvalidation_0-mlogloss:0.79919\n",
      "[26]\tvalidation_0-mlogloss:0.79859\n",
      "[27]\tvalidation_0-mlogloss:0.79811\n",
      "[28]\tvalidation_0-mlogloss:0.79790\n",
      "[29]\tvalidation_0-mlogloss:0.79714\n",
      "[30]\tvalidation_0-mlogloss:0.79694\n",
      "[31]\tvalidation_0-mlogloss:0.79682\n",
      "[32]\tvalidation_0-mlogloss:0.79634\n",
      "[33]\tvalidation_0-mlogloss:0.79590\n",
      "[34]\tvalidation_0-mlogloss:0.79584\n",
      "[35]\tvalidation_0-mlogloss:0.79578\n",
      "[36]\tvalidation_0-mlogloss:0.79560\n",
      "[37]\tvalidation_0-mlogloss:0.79556\n",
      "[38]\tvalidation_0-mlogloss:0.79553\n",
      "[39]\tvalidation_0-mlogloss:0.79532\n",
      "[40]\tvalidation_0-mlogloss:0.79497\n",
      "[41]\tvalidation_0-mlogloss:0.79495\n",
      "[42]\tvalidation_0-mlogloss:0.79473\n",
      "[43]\tvalidation_0-mlogloss:0.79448\n",
      "[44]\tvalidation_0-mlogloss:0.79425\n",
      "[45]\tvalidation_0-mlogloss:0.79418\n",
      "[46]\tvalidation_0-mlogloss:0.79418\n",
      "[47]\tvalidation_0-mlogloss:0.79418\n",
      "[48]\tvalidation_0-mlogloss:0.79417\n",
      "[49]\tvalidation_0-mlogloss:0.79417\n",
      "[50]\tvalidation_0-mlogloss:0.79417\n",
      "[51]\tvalidation_0-mlogloss:0.79417\n",
      "[52]\tvalidation_0-mlogloss:0.79416\n",
      "[53]\tvalidation_0-mlogloss:0.79416\n",
      "[54]\tvalidation_0-mlogloss:0.79411\n",
      "[55]\tvalidation_0-mlogloss:0.79411\n",
      "[56]\tvalidation_0-mlogloss:0.79385\n",
      "[57]\tvalidation_0-mlogloss:0.79364\n",
      "[58]\tvalidation_0-mlogloss:0.79364\n",
      "[59]\tvalidation_0-mlogloss:0.79364\n",
      "[60]\tvalidation_0-mlogloss:0.79364\n",
      "Stopping. Best iteration:\n",
      "[58]\tvalidation_0-mlogloss:0.79364\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:2.35344\n",
      "Will train until validation_0-mlogloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mlogloss:2.21531\n",
      "[2]\tvalidation_0-mlogloss:2.12593\n",
      "[3]\tvalidation_0-mlogloss:2.06152\n",
      "[4]\tvalidation_0-mlogloss:2.01467\n",
      "[5]\tvalidation_0-mlogloss:1.97936\n",
      "[6]\tvalidation_0-mlogloss:1.95064\n",
      "[7]\tvalidation_0-mlogloss:1.92918\n",
      "[8]\tvalidation_0-mlogloss:1.91219\n",
      "[9]\tvalidation_0-mlogloss:1.89853\n",
      "[10]\tvalidation_0-mlogloss:1.88777\n",
      "[11]\tvalidation_0-mlogloss:1.87871\n",
      "[12]\tvalidation_0-mlogloss:1.87154\n",
      "[13]\tvalidation_0-mlogloss:1.86516\n",
      "[14]\tvalidation_0-mlogloss:1.85996\n",
      "[15]\tvalidation_0-mlogloss:1.85520\n",
      "[16]\tvalidation_0-mlogloss:1.85233\n",
      "[17]\tvalidation_0-mlogloss:1.84931\n",
      "[18]\tvalidation_0-mlogloss:1.84700\n",
      "[19]\tvalidation_0-mlogloss:1.84521\n",
      "[20]\tvalidation_0-mlogloss:1.84319\n",
      "[21]\tvalidation_0-mlogloss:1.84200\n",
      "[22]\tvalidation_0-mlogloss:1.84116\n",
      "[23]\tvalidation_0-mlogloss:1.83999\n",
      "[24]\tvalidation_0-mlogloss:1.83943\n",
      "[25]\tvalidation_0-mlogloss:1.83866\n",
      "[26]\tvalidation_0-mlogloss:1.83832\n",
      "[27]\tvalidation_0-mlogloss:1.83799\n",
      "[28]\tvalidation_0-mlogloss:1.83736\n",
      "[29]\tvalidation_0-mlogloss:1.83699\n",
      "[30]\tvalidation_0-mlogloss:1.83655\n",
      "[31]\tvalidation_0-mlogloss:1.83604\n",
      "[32]\tvalidation_0-mlogloss:1.83554\n",
      "[33]\tvalidation_0-mlogloss:1.83550\n",
      "[34]\tvalidation_0-mlogloss:1.83508\n",
      "[35]\tvalidation_0-mlogloss:1.83477\n",
      "[36]\tvalidation_0-mlogloss:1.83460\n",
      "[37]\tvalidation_0-mlogloss:1.83452\n",
      "[38]\tvalidation_0-mlogloss:1.83451\n",
      "[39]\tvalidation_0-mlogloss:1.83412\n",
      "[40]\tvalidation_0-mlogloss:1.83399\n",
      "[41]\tvalidation_0-mlogloss:1.83377\n",
      "[42]\tvalidation_0-mlogloss:1.83353\n",
      "[43]\tvalidation_0-mlogloss:1.83309\n",
      "[44]\tvalidation_0-mlogloss:1.83302\n",
      "[45]\tvalidation_0-mlogloss:1.83302\n",
      "[46]\tvalidation_0-mlogloss:1.83302\n",
      "[47]\tvalidation_0-mlogloss:1.83292\n",
      "[48]\tvalidation_0-mlogloss:1.83292\n",
      "[49]\tvalidation_0-mlogloss:1.83282\n",
      "[50]\tvalidation_0-mlogloss:1.83256\n",
      "[51]\tvalidation_0-mlogloss:1.83256\n",
      "[52]\tvalidation_0-mlogloss:1.83244\n",
      "[53]\tvalidation_0-mlogloss:1.83244\n",
      "[54]\tvalidation_0-mlogloss:1.83224\n",
      "[55]\tvalidation_0-mlogloss:1.83224\n",
      "[56]\tvalidation_0-mlogloss:1.83203\n",
      "[57]\tvalidation_0-mlogloss:1.83203\n",
      "[58]\tvalidation_0-mlogloss:1.83203\n",
      "[59]\tvalidation_0-mlogloss:1.83203\n",
      "Stopping. Best iteration:\n",
      "[57]\tvalidation_0-mlogloss:1.83203\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:0.54047\n",
      "Will train until validation_0-mlogloss hasn't improved in 2 rounds.\n",
      "[1]\tvalidation_0-mlogloss:0.49984\n",
      "[2]\tvalidation_0-mlogloss:0.48009\n",
      "[3]\tvalidation_0-mlogloss:0.46664\n",
      "[4]\tvalidation_0-mlogloss:0.45633\n",
      "[5]\tvalidation_0-mlogloss:0.44865\n",
      "[6]\tvalidation_0-mlogloss:0.44228\n",
      "[7]\tvalidation_0-mlogloss:0.43812\n",
      "[8]\tvalidation_0-mlogloss:0.43487\n",
      "[9]\tvalidation_0-mlogloss:0.43207\n",
      "[10]\tvalidation_0-mlogloss:0.43007\n",
      "[11]\tvalidation_0-mlogloss:0.42869\n",
      "[12]\tvalidation_0-mlogloss:0.42777\n",
      "[13]\tvalidation_0-mlogloss:0.42628\n",
      "[14]\tvalidation_0-mlogloss:0.42529\n",
      "[15]\tvalidation_0-mlogloss:0.42446\n",
      "[16]\tvalidation_0-mlogloss:0.42416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17]\tvalidation_0-mlogloss:0.42386\n",
      "[18]\tvalidation_0-mlogloss:0.42354\n",
      "[19]\tvalidation_0-mlogloss:0.42320\n",
      "[20]\tvalidation_0-mlogloss:0.42295\n",
      "[21]\tvalidation_0-mlogloss:0.42273\n",
      "[22]\tvalidation_0-mlogloss:0.42238\n",
      "[23]\tvalidation_0-mlogloss:0.42210\n",
      "[24]\tvalidation_0-mlogloss:0.42200\n",
      "[25]\tvalidation_0-mlogloss:0.42198\n",
      "[26]\tvalidation_0-mlogloss:0.42197\n",
      "[27]\tvalidation_0-mlogloss:0.42183\n",
      "[28]\tvalidation_0-mlogloss:0.42165\n",
      "[29]\tvalidation_0-mlogloss:0.42164\n",
      "[30]\tvalidation_0-mlogloss:0.42163\n",
      "[31]\tvalidation_0-mlogloss:0.42147\n",
      "[32]\tvalidation_0-mlogloss:0.42141\n",
      "[33]\tvalidation_0-mlogloss:0.42129\n",
      "[34]\tvalidation_0-mlogloss:0.42129\n",
      "[35]\tvalidation_0-mlogloss:0.42128\n",
      "[36]\tvalidation_0-mlogloss:0.42128\n",
      "[37]\tvalidation_0-mlogloss:0.42122\n",
      "[38]\tvalidation_0-mlogloss:0.42112\n",
      "[39]\tvalidation_0-mlogloss:0.42112\n",
      "[40]\tvalidation_0-mlogloss:0.42111\n",
      "[41]\tvalidation_0-mlogloss:0.42111\n",
      "[42]\tvalidation_0-mlogloss:0.42089\n",
      "[43]\tvalidation_0-mlogloss:0.42089\n",
      "[44]\tvalidation_0-mlogloss:0.42084\n",
      "[45]\tvalidation_0-mlogloss:0.42064\n",
      "[46]\tvalidation_0-mlogloss:0.42064\n",
      "[47]\tvalidation_0-mlogloss:0.42054\n",
      "[48]\tvalidation_0-mlogloss:0.42031\n",
      "[49]\tvalidation_0-mlogloss:0.42025\n",
      "[50]\tvalidation_0-mlogloss:0.42019\n",
      "[51]\tvalidation_0-mlogloss:0.42013\n",
      "[52]\tvalidation_0-mlogloss:0.42013\n",
      "[53]\tvalidation_0-mlogloss:0.42013\n",
      "[54]\tvalidation_0-mlogloss:0.42002\n",
      "[55]\tvalidation_0-mlogloss:0.42002\n",
      "[56]\tvalidation_0-mlogloss:0.42002\n",
      "Stopping. Best iteration:\n",
      "[54]\tvalidation_0-mlogloss:0.42002\n",
      "\n",
      "DataFrame: adult imput_method :trees model :KNN score:  0.8\n",
      "DataFrame: adult imput_method :trees model :XGB score:  0.87\n",
      "DataFrame: adult imput_method :trees model :MLP score:  0.85\n",
      "Train on 45222 samples, validate on 45222 samples\n",
      "Epoch 1/100\n",
      "45222/45222 [==============================] - 7s 150us/sample - loss: 0.9899 - accuracy: 0.7070 - val_loss: 0.8288 - val_accuracy: 0.7478\n",
      "Epoch 2/100\n",
      "45222/45222 [==============================] - 6s 123us/sample - loss: 0.8369 - accuracy: 0.7465 - val_loss: 0.8062 - val_accuracy: 0.7478\n",
      "Epoch 3/100\n",
      "45222/45222 [==============================] - 5s 117us/sample - loss: 0.8210 - accuracy: 0.7475 - val_loss: 0.7947 - val_accuracy: 0.7507\n",
      "Epoch 4/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.8093 - accuracy: 0.7503 - val_loss: 0.7864 - val_accuracy: 0.7521\n",
      "Epoch 5/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.8037 - accuracy: 0.7498 - val_loss: 0.7820 - val_accuracy: 0.7512\n",
      "Epoch 6/100\n",
      "45222/45222 [==============================] - 5s 115us/sample - loss: 0.7992 - accuracy: 0.7504 - val_loss: 0.7788 - val_accuracy: 0.7530\n",
      "Epoch 7/100\n",
      "45222/45222 [==============================] - 5s 116us/sample - loss: 0.7945 - accuracy: 0.7516 - val_loss: 0.7732 - val_accuracy: 0.7535\n",
      "Epoch 8/100\n",
      "45222/45222 [==============================] - 5s 116us/sample - loss: 0.7912 - accuracy: 0.7521 - val_loss: 0.7688 - val_accuracy: 0.7535\n",
      "Epoch 9/100\n",
      "45222/45222 [==============================] - 5s 116us/sample - loss: 0.7881 - accuracy: 0.7511 - val_loss: 0.7653 - val_accuracy: 0.7549\n",
      "Epoch 10/100\n",
      "45222/45222 [==============================] - 5s 115us/sample - loss: 0.7838 - accuracy: 0.7518 - val_loss: 0.7618 - val_accuracy: 0.7561\n",
      "Epoch 11/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.7827 - accuracy: 0.7519 - val_loss: 0.7605 - val_accuracy: 0.7567\n",
      "Epoch 12/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.7816 - accuracy: 0.7530 - val_loss: 0.7572 - val_accuracy: 0.7561\n",
      "Epoch 13/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.7785 - accuracy: 0.7534 - val_loss: 0.7538 - val_accuracy: 0.7572\n",
      "Epoch 14/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.7770 - accuracy: 0.7533 - val_loss: 0.7547 - val_accuracy: 0.7563\n",
      "Epoch 15/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 0.7743 - accuracy: 0.7537 - val_loss: 0.7542 - val_accuracy: 0.7575\n",
      "Epoch 16/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 0.7722 - accuracy: 0.7550 - val_loss: 0.7478 - val_accuracy: 0.7574\n",
      "Epoch 17/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 0.7705 - accuracy: 0.7547 - val_loss: 0.7466 - val_accuracy: 0.7579\n",
      "Epoch 18/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 0.7708 - accuracy: 0.7537 - val_loss: 0.7433 - val_accuracy: 0.7588\n",
      "Epoch 19/100\n",
      "45222/45222 [==============================] - 5s 115us/sample - loss: 0.7677 - accuracy: 0.7549 - val_loss: 0.7428 - val_accuracy: 0.7595\n",
      "Epoch 20/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 0.7654 - accuracy: 0.7550 - val_loss: 0.7427 - val_accuracy: 0.7595\n",
      "Epoch 21/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 0.7639 - accuracy: 0.7551 - val_loss: 0.7409 - val_accuracy: 0.7575\n",
      "Epoch 22/100\n",
      "45222/45222 [==============================] - 5s 119us/sample - loss: 0.7635 - accuracy: 0.7551 - val_loss: 0.7391 - val_accuracy: 0.7587\n",
      "45222/45222 [==============================] - 2s 34us/sample - loss: 0.7391 - accuracy: 0.7587\n",
      "Val score is 0.7586572766304016\n",
      "Train on 45222 samples, validate on 45222 samples\n",
      "Epoch 1/100\n",
      "45222/45222 [==============================] - 6s 141us/sample - loss: 2.0931 - accuracy: 0.2919 - val_loss: 1.8940 - val_accuracy: 0.3370\n",
      "Epoch 2/100\n",
      "45222/45222 [==============================] - 5s 116us/sample - loss: 1.9140 - accuracy: 0.3327 - val_loss: 1.8590 - val_accuracy: 0.3489\n",
      "Epoch 3/100\n",
      "45222/45222 [==============================] - 5s 117us/sample - loss: 1.8882 - accuracy: 0.3384 - val_loss: 1.8444 - val_accuracy: 0.3525\n",
      "Epoch 4/100\n",
      "45222/45222 [==============================] - 5s 116us/sample - loss: 1.8739 - accuracy: 0.3425 - val_loss: 1.8283 - val_accuracy: 0.3576\n",
      "Epoch 5/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 1.8660 - accuracy: 0.3461 - val_loss: 1.8210 - val_accuracy: 0.3578\n",
      "Epoch 6/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 1.8535 - accuracy: 0.3491 - val_loss: 1.8128 - val_accuracy: 0.3630\n",
      "Epoch 7/100\n",
      "45222/45222 [==============================] - 5s 115us/sample - loss: 1.8505 - accuracy: 0.3494 - val_loss: 1.8101 - val_accuracy: 0.3600\n",
      "Epoch 8/100\n",
      "45222/45222 [==============================] - 5s 118us/sample - loss: 1.8460 - accuracy: 0.3516 - val_loss: 1.8029 - val_accuracy: 0.3652\n",
      "Epoch 9/100\n",
      "45222/45222 [==============================] - 5s 113us/sample - loss: 1.8401 - accuracy: 0.3521 - val_loss: 1.7974 - val_accuracy: 0.3657\n",
      "Epoch 10/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 1.8372 - accuracy: 0.3552 - val_loss: 1.7946 - val_accuracy: 0.3652\n",
      "Epoch 11/100\n",
      "45222/45222 [==============================] - 5s 117us/sample - loss: 1.8319 - accuracy: 0.3550 - val_loss: 1.7931 - val_accuracy: 0.3682\n",
      "Epoch 12/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 1.8282 - accuracy: 0.3571 - val_loss: 1.7906 - val_accuracy: 0.3682\n",
      "Epoch 13/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 1.8275 - accuracy: 0.3565 - val_loss: 1.7849 - val_accuracy: 0.3689\n",
      "Epoch 14/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 1.8222 - accuracy: 0.3599 - val_loss: 1.7810 - val_accuracy: 0.3736\n",
      "Epoch 15/100\n",
      "45222/45222 [==============================] - 5s 114us/sample - loss: 1.8199 - accuracy: 0.3597 - val_loss: 1.7795 - val_accuracy: 0.3715\n",
      "Epoch 16/100\n",
      "45222/45222 [==============================] - 4s 89us/sample - loss: 1.8179 - accuracy: 0.3605 - val_loss: 1.7774 - val_accuracy: 0.3734\n",
      "45222/45222 [==============================] - 1s 26us/sample - loss: 1.7774 - accuracy: 0.3734\n",
      "Val score is 0.3734465539455414\n",
      "Train on 45222 samples, validate on 45222 samples\n",
      "Epoch 1/100\n",
      "45222/45222 [==============================] - 5s 115us/sample - loss: 0.8043 - accuracy: 0.8623 - val_loss: 0.4290 - val_accuracy: 0.9193\n",
      "Epoch 2/100\n",
      "45222/45222 [==============================] - 4s 87us/sample - loss: 0.4363 - accuracy: 0.9183 - val_loss: 0.4086 - val_accuracy: 0.9206\n",
      "Epoch 3/100\n",
      "45222/45222 [==============================] - 4s 87us/sample - loss: 0.4213 - accuracy: 0.9194 - val_loss: 0.4036 - val_accuracy: 0.9202\n",
      "Epoch 4/100\n",
      "45222/45222 [==============================] - 4s 88us/sample - loss: 0.4142 - accuracy: 0.9202 - val_loss: 0.3976 - val_accuracy: 0.9203\n",
      "45222/45222 [==============================] - 1s 27us/sample - loss: 0.3976 - accuracy: 0.9203\n",
      "Val score is 0.9203042984008789\n",
      "DataFrame: adult imput_method :MLP model :KNN score:  0.8\n",
      "DataFrame: adult imput_method :MLP model :XGB score:  0.87\n",
      "DataFrame: adult imput_method :MLP model :MLP score:  0.85\n",
      "1:31:49.921357\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "#importa los conjuntos de datos\n",
    "names=['automobile','bands','cleveland','dermatology','hepatitis','housevotes','mammographic','marketing','mushroom','adult']\n",
    "methods=['LOCF','mean_mode','knn','trees','MLP']\n",
    "models=['KNN','XGB','MLP']\n",
    "results_LOCF=[]\n",
    "results_mean_mode=[]\n",
    "results_knn=[]\n",
    "results_trees=[]\n",
    "results_MLP=[]\n",
    "for name in names: #abre los conjuntos de datos\n",
    "    DataFrame = pd.read_csv(name+'.csv', header=None, na_values=[' ','  ','?',None])\n",
    "    for method in methods: # imputa datos faltantes con cada metodo\n",
    "        df_complete_i = MissingValueImputation(method = method).fit_transform(DataFrame)\n",
    "        for model in models: #ajusta modelos de Machine Learning\n",
    "            result=Data_Prer_Mod_train(df_complete_i,model,5)\n",
    "            #imprime y guarda resultados\n",
    "            print('DataFrame: '+name+' imput_method :'+method+' model :'+model+' score: ',result)\n",
    "            if method=='LOCF':\n",
    "                results_LOCF.append(result)\n",
    "            elif method=='mean_mode':\n",
    "                results_mean_mode.append(result)\n",
    "            elif method=='knn':\n",
    "                results_knn.append(result)\n",
    "            elif method=='trees':\n",
    "                results_trees.append(result)\n",
    "            elif method=='MLP':\n",
    "                results_MLP.append(result)\n",
    "        df_clean(df_complete_i) #libera memoria\n",
    "    df_clean(DataFrame) #libera memoria\n",
    "print(datetime.now() - startTime) #tiempo total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guarda los resultados\n",
    "results_dict={'LOCF':results_LOCF,\n",
    "              'mean_mode':results_mean_mode,\n",
    "              'knn':results_knn,\n",
    "              'trees': results_trees,\n",
    "              'MLP': results_MLP\n",
    "             }\n",
    "\n",
    "df_results=pd.DataFrame(results_dict)\n",
    "df_results.to_csv('Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Análisis estadístico de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('Results.csv',  index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOCF</th>\n",
       "      <th>mean_mode</th>\n",
       "      <th>knn</th>\n",
       "      <th>trees</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716333</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.726333</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.214995</td>\n",
       "      <td>0.215238</td>\n",
       "      <td>0.217099</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.213144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.885000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LOCF  mean_mode        knn      trees        MLP\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000\n",
       "mean    0.716667   0.716333   0.723000   0.726333   0.718000\n",
       "std     0.214995   0.215238   0.217099   0.211538   0.213144\n",
       "min     0.310000   0.310000   0.310000   0.310000   0.310000\n",
       "25%     0.530000   0.535000   0.547500   0.575000   0.542500\n",
       "50%     0.800000   0.800000   0.800000   0.800000   0.795000\n",
       "75%     0.885000   0.885000   0.885000   0.900000   0.885000\n",
       "max     1.000000   1.000000   1.000000   0.990000   1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cf9d514ba8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "sns.boxplot(data=df_results,orient=\"h\") #boxplot horizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que todos los métodos tienen una variación entre 0.3 y 1. Además, aparentemente el método de imputación basado en arboles de decisión fue un poco mejor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checamos normalidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro test:\n",
      "Method: LOCF W: 0.889 p_v: 0.005\n",
      "Method: mean_mode W: 0.898 p_v: 0.007\n",
      "Method: knn W: 0.891 p_v: 0.005\n",
      "Method: trees W: 0.892 p_v: 0.006\n",
      "Method: MLP W: 0.893 p_v: 0.006\n"
     ]
    }
   ],
   "source": [
    "#checa normalidad univariada\n",
    "methods=df_results.columns\n",
    "print('Shapiro test:')\n",
    "for method in methods:\n",
    "    W,p_v=stats.shapiro(df_results[method].values) #W>0.9 or p_v>0.05  Normalidad univariada \n",
    "    print('Method: '+method+' W: '+str(np.round(W,3))+' p_v: '+str(np.round(p_v,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se rechaza la normalidad y la aproximamos por la transformación Box-Cox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aproximar a normal\n",
    "pt1 = PowerTransformer(method='box-cox') #‘yeo-johnson’ boxcox solo positivos\n",
    "pt1.fit(df_results.values) #estima los lambdas optimos para la transformacion\n",
    "X_t=pt1.transform(df_results.values) #aplica la transformacion a los datos\n",
    "df_results_norm=pd.DataFrame(X_t)\n",
    "df_results_norm.columns=methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro test:\n",
      "Method: LOCF W: 0.91 p_v: 0.014\n",
      "Method: mean_mode W: 0.917 p_v: 0.022\n",
      "Method: knn W: 0.914 p_v: 0.019\n",
      "Method: trees W: 0.919 p_v: 0.025\n",
      "Method: MLP W: 0.914 p_v: 0.018\n"
     ]
    }
   ],
   "source": [
    "#checa normalidad univariada\n",
    "methods=df_results.columns\n",
    "print('Shapiro test:')\n",
    "for method in methods:\n",
    "    W,p_v=stats.shapiro(df_results_norm[method].values) #W>0.9 or p_v>0.05  Normalidad univariada \n",
    "    print('Method: '+method+' W: '+str(np.round(W,3))+' p_v: '+str(np.round(p_v,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aceptamos el supuesto de normalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test para diferencia en medias al 5%: \n",
    "\n",
    "${H_0}:{\\mu _1} - {\\mu _2} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test para diferencia en medias al 5%:\n",
      "LOCF-LOCF acepta H0 :True\n",
      "LOCF-mean_mode acepta H0 :True\n",
      "LOCF-knn acepta H0 :True\n",
      "LOCF-trees acepta H0 :True\n",
      "LOCF-MLP acepta H0 :True\n",
      "mean_mode-LOCF acepta H0 :True\n",
      "mean_mode-mean_mode acepta H0 :True\n",
      "mean_mode-knn acepta H0 :True\n",
      "mean_mode-trees acepta H0 :True\n",
      "mean_mode-MLP acepta H0 :True\n",
      "knn-LOCF acepta H0 :True\n",
      "knn-mean_mode acepta H0 :True\n",
      "knn-knn acepta H0 :True\n",
      "knn-trees acepta H0 :True\n",
      "knn-MLP acepta H0 :True\n",
      "trees-LOCF acepta H0 :True\n",
      "trees-mean_mode acepta H0 :True\n",
      "trees-knn acepta H0 :True\n",
      "trees-trees acepta H0 :True\n",
      "trees-MLP acepta H0 :True\n",
      "MLP-LOCF acepta H0 :True\n",
      "MLP-mean_mode acepta H0 :True\n",
      "MLP-knn acepta H0 :True\n",
      "MLP-trees acepta H0 :True\n",
      "MLP-MLP acepta H0 :True\n"
     ]
    }
   ],
   "source": [
    "print('t-test para diferencia en medias al 5%:')\n",
    "for method1 in methods:\n",
    "    for method2 in methods:\n",
    "        _,pvalue=stats.ttest_ind(df_results_norm[method1].values,df_results_norm[method2].values)\n",
    "        print(method1+'-'+method2+' acepta H0 :'+str(np.round(pvalue,3)>0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este experimento, ningún método resultó significativamente diferente a otro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados:\n",
    "\n",
    "- En este experimento se utilizó la métrica de accuracy con validación cruzada para la tarea de clasificación con 3 modelos de aprendizaje maquina (vecinos mas cercanos, arboles de decisión, y redes neuronales) para 10 conjuntos de datos con valores faltantes con diferentes características (numero de ejemplos, clases, porcentaje de valores faltantes) para los cuales se emplearon 5 métodos diferentes de imputación de datos (no estadísticos, estadísticos y de aprendizaje maquina) con los cuales se obtuvieron 30   observaciones para cada uno de los métodos de imputación.\n",
    "\n",
    "- Los resultados para todos los métodos variaron en el rango de 0.3 a 1 significando un mal ajuste o sobre ajuste de los modelos. Esto era de esperarse porque no se ajustó un modelo especifico para cada tarea de clasificación, sino que fueron los mismos modelos para cada conjunto de datos, además cada conjunto de datos tenia diferentes numero de clases y en algunos casos desbalanceadas o el tamaño de la muestra muy pequeño como para entrenar bien los modelos.\n",
    "\n",
    "- A pesar de lo anterior el análisis descriptivo parece mostrar una ligera diferencia en el método de árboles de decisión sobre los demás, aunque el t-test no mostró que esta diferencia sea significativa, por lo que se puede concluir que para este experimento en particular bajo las condiciones descritas ninguno de los métodos de clasificación resultó ser significativamente mejor que los demás, sin embargo quizá si creamos más muestras bajo otras condiciones, por ejemplo si se ajustan modelos específicos para cada caso y utilizamos mas conjuntos de datos podríamos encontrar diferencias significativas en alguno de los métodos.\n",
    "\n",
    "- Por otro lado, también, para cada conjunto de datos se tendría que hacer un análisis más específico sobre la naturaleza de las variables y del problema para determinar el mejor método de imputación de datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
