{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Victor Gómez\n",
    "### victor.gomez@cimat.mx\n",
    "\n",
    "\n",
    "# Maestría en Cómputo Estadístico\n",
    "# CIMAT Monterrey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tópicos selectos de Cómputo<center>\n",
    "# <center>Tarea 1: Valores faltantes<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrucciones: \n",
    "- Usa python 3.x , tensorflow 2.x y xgboost 1.x\n",
    "- ejecuta todas las celdas para replicar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías necesarias\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import  cross_val_score, RepeatedKFold, RandomizedSearchCV, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from baycomp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase que realiza la imputación de datos faltantes en un DataFrame por distintos métodos.\n",
    "\n",
    "## Métodos de imputación de valores faltantes:\n",
    "\n",
    "- 1 Métodos no estadísticos: LOCF (Last observation carried forward)\n",
    "- 2 Métodos estadísticos:    mean_mode (media para atributos numéricos, moda para atributos categoricos)\n",
    "\n",
    "Métodos de Machine Learning. Ajuste mediante RandomizedSearchCV con 3-Fold: \n",
    "- 3 Vecinos mas cercanos: knn (KNeighbors: k: [2-40])\n",
    "- 4 Arboles de decisión:  trees (XGBoost: learning rate: [0.1-0.6], n_estimators: [50-300], subsample: 0.9, regularización: gamma: [0-100], lambda: [1-10])\n",
    "- 5 Redes neuronales:     MLP (epocas: 100, optimizador: adam, función de costo: binary_crossentropy(o categorical_crossentropy), metrica: accuracy, hiddenlayers:  2 con 50 neuronas, función de activación: relu. outputlayer: función de activación: sigmoid (o softmax) con 1 (o número de clases) neuronas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _missing_value import MissingValueImputation #Clase que realiza la imputacion de datos faltantes en un DataFrame por distintos metodos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos de datos con valores faltantes (%MVs):\n",
    "datasets de: https://sci2s.ugr.es/keel/missing.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29f5af9d0b8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "names=['automobile','bands','breast','cleveland','crx','dermatology','hepatitis','housevotes','mammographic','wisconsin']\n",
    "Ejemplos=[205,539,286,303,690,366,155,435,961,699]\n",
    "clases=[6,2,2,5,2,6,2,2,2,2]\n",
    "mvs=[26.83,32.28,3.15,1.98,5.36,2.19,48.39,46.67,13.63,2.29]\n",
    "\n",
    "df_mvs_dict={\n",
    "            'Ejemplos': Ejemplos,\n",
    "             'Clases': clases,\n",
    "             'MVs': mvs}\n",
    "\n",
    "df_mvs=pd.DataFrame(df_mvs_dict)\n",
    "df_mvs.index=names\n",
    "\n",
    "axes = df_mvs.plot.bar(rot=45, subplots=True,figsize=(7,7))\n",
    "axes[1].legend(loc=2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los 10 conjuntos de datos presentan diferentes características, la cantidad de ejemplos que  esta entre 250 y 1000, la mayoría son de 2 clases, máximo 6 y de 5 a 50% de datos faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de ML para clasificación. Ajuste mediante RandomizedSearchCV con validación cruzada (10x2) y metrica accuracy:\n",
    "\n",
    "- 1 KNeighbors: k: [2-40]\n",
    "- 2 XGBoost: learning rate: [0.1-0.6], n_estimators: [50-300], subsample: 0.9, regularización: gamma: [0-100], lambda: [1-10]\n",
    "- 3 MLP, epocas: 100, optimizador: adam, función de costo: binary_crossentropy(o categorical_crossentropy), hiddenlayers:  2 con 50 neuronas, función de activación: relu. outputlayer: función de activación: sigmoid (o softmax) con 1 (o número de clases) neuronas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_clean(df): #libera memoria\n",
    "    df=pd.DataFrame()\n",
    "    del df\n",
    "    \n",
    "#Models:\n",
    "#K Neighbors\n",
    "def KNN_model():\n",
    "    params={'n_neighbors':np.random.randint(2,40, size=10)}\n",
    "    return KNeighborsClassifier(), params\n",
    "\n",
    "#XGBoost\n",
    "def XGBoost_model(num_classes=2):\n",
    "    \n",
    "    gbm_param_grid={'learning_rate':[0.1,0.3,0.6],\n",
    "               'n_estimators':[50,150,300],\n",
    "               'subsample':[0.9],\n",
    "               'gamma':[0,10,100],\n",
    "               'lambda':[1,10,100]}\n",
    "    \n",
    "    if num_classes ==2:\n",
    "        gbm=xgb.XGBClassifier()\n",
    "                \n",
    "    else:\n",
    "        gbm=xgb.XGBClassifier(objective='multi:softmax')\n",
    "    \n",
    "    return gbm, gbm_param_grid \n",
    "\n",
    "\n",
    "\n",
    "#MLP\n",
    "def create_model(input_features,neurons_output,activation,loss):\n",
    "    neur_Hid_layer=[50,50]\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(neur_Hid_layer[0],activation='relu',input_shape=(input_features,))) #first layer\n",
    "    model.add(tf.keras.layers.BatchNormalization()) #batch normalization\n",
    "        \n",
    "    layers=len(neur_Hid_layer)\n",
    "        \n",
    "    if layers>1: #next hidden layers\n",
    "        for layer in range(1,layers):\n",
    "            model.add(tf.keras.layers.Dense(neur_Hid_layer[layer],activation='relu'))\n",
    "            model.add(tf.keras.layers.BatchNormalization()) #batch normalization\n",
    "        \n",
    "    if activation!='linear':\n",
    "        model.add(tf.keras.layers.Dense(neurons_output,activation=activation)) #output layer\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(neurons_output)) #output layer\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy']) #compìla el modelo\n",
    "        \n",
    "    return model\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=10)\n",
    "\n",
    "#Cross validation \n",
    "def model_train_CV_(model,params,X_train,y_train,cv=rkf): \n",
    "    kfold=RandomizedSearchCV(model,param_distributions=params,scoring='accuracy',cv=cv,n_jobs=-1)\n",
    "    kfold.fit(X_train,y_train)\n",
    "    score=kfold.best_score_\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "#prepara los datos, entrena, y obtiene scores para cada modelo\n",
    "def Data_Prer_Mod_train(df,model_name,cv=rkf): \n",
    "    _,column=df.shape\n",
    "    column=column-1\n",
    "    flat= lambda x,num_features: x.reshape([-1, num_features]) # Flatten function\n",
    "    if model_name=='KNN':\n",
    "        y=df.iloc[:,column].values\n",
    "        X=pd.get_dummies(df.drop([column],axis=1),drop_first=True).values\n",
    "        scaler = StandardScaler() #Standardize\n",
    "        scaler.fit(X)\n",
    "        X=scaler.transform(X)\n",
    "        model,params=KNN_model()\n",
    "        score=model_train_CV_(model,params,X,y,cv)\n",
    "        \n",
    "    elif model_name=='XGB':\n",
    "        y=df.iloc[:,column].values\n",
    "        X=pd.get_dummies(df.drop([column],axis=1),drop_first=True).values\n",
    "        num_classes=np.shape(df.iloc[:,column].unique())[0]\n",
    "        model,params=XGBoost_model(num_classes)\n",
    "        score=model_train_CV_(model,params,X,y,cv)\n",
    "    elif model_name=='MLP':\n",
    "        y=df.iloc[:,column].values\n",
    "        X=pd.get_dummies(df.drop([column],axis=1),drop_first=True).values\n",
    "        num_classes=np.shape(df.iloc[:,column].unique())[0]\n",
    "        num_features=X.shape[1]\n",
    "        scaler = StandardScaler() #Standardize\n",
    "        scaler.fit(X)\n",
    "        X=scaler.transform(X)\n",
    "        X=flat(X,num_features) #flatten\n",
    "        \n",
    "        if num_classes==2:\n",
    "            y=np.reshape(y,(-1, 1))\n",
    "            enc=OneHotEncoder(drop='first').fit(y)\n",
    "            y = enc.transform(y).toarray()\n",
    "            lossf='binary_crossentropy'\n",
    "            activ_output='sigmoid'\n",
    "            neurons_output=1\n",
    "            \n",
    "        else:\n",
    "            y=np.reshape(y,(-1, 1))\n",
    "            enc=OneHotEncoder().fit(y)\n",
    "            y = enc.transform(y).toarray()\n",
    "            lossf='categorical_crossentropy'\n",
    "            activ_output='softmax'\n",
    "            neurons_output=num_classes\n",
    "        \n",
    "        model=KerasClassifier(build_fn=create_model, epochs=100,  input_features=num_features,neurons_output=neurons_output,activation=activ_output,loss=lossf)\n",
    "\n",
    "        kfold=cross_val_score(model,X,y,cv=cv,n_jobs=-1)\n",
    "        score=kfold.mean()\n",
    "        \n",
    "    return np.round(score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Importación de los conjuntos de datos, Imputación de los datos faltantes, Ajuste de modelos y Validacion cruzada (10x2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :LOCF model :KNN score:  0.49\n",
      "DataFrame: automobile imput_method :LOCF model :XGB score:  0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: automobile imput_method :LOCF model :MLP score:  0.64\n",
      "DataFrame: automobile imput_method :mean_mode model :KNN score:  0.5\n",
      "DataFrame: automobile imput_method :mean_mode model :XGB score:  0.73\n",
      "DataFrame: automobile imput_method :mean_mode model :MLP score:  0.65\n",
      "DataFrame: automobile imput_method :knn model :KNN score:  0.53\n",
      "DataFrame: automobile imput_method :knn model :XGB score:  0.74\n",
      "DataFrame: automobile imput_method :knn model :MLP score:  0.64\n",
      "Best score is -12.139809991427677\n",
      "Best score is 0.9015949663447468\n",
      "Best score is -0.05837722617002269\n",
      "Best score is -0.08714400604589662\n",
      "Best score is -7.216644583789445\n",
      "Best score is -103.73169252542435\n",
      "Best score is -1585.585833041822\n",
      "DataFrame: automobile imput_method :trees model :KNN score:  0.54\n",
      "DataFrame: automobile imput_method :trees model :XGB score:  0.76\n",
      "DataFrame: automobile imput_method :trees model :MLP score:  0.66\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 8ms/sample - loss: 121.1128 - mae: 121.1128 - val_loss: 120.5734 - val_mae: 120.5734\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 121.0630 - mae: 121.0630 - val_loss: 120.5264 - val_mae: 120.5264\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 121.0105 - mae: 121.0105 - val_loss: 120.4760 - val_mae: 120.4760\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 120.9548 - mae: 120.9548 - val_loss: 120.4226 - val_mae: 120.4226\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 120.8957 - mae: 120.8957 - val_loss: 120.3644 - val_mae: 120.3644\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 120.8328 - mae: 120.8328 - val_loss: 120.3029 - val_mae: 120.3029\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 120.7657 - mae: 120.7657 - val_loss: 120.2383 - val_mae: 120.2383\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 120.6941 - mae: 120.6941 - val_loss: 120.1688 - val_mae: 120.1689\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 120.6179 - mae: 120.6179 - val_loss: 120.0958 - val_mae: 120.0958\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 120.5369 - mae: 120.5369 - val_loss: 120.0184 - val_mae: 120.0183\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 120.4507 - mae: 120.4507 - val_loss: 119.9359 - val_mae: 119.9359\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 120.3593 - mae: 120.3593 - val_loss: 119.8476 - val_mae: 119.8476\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 120.2626 - mae: 120.2626 - val_loss: 119.7563 - val_mae: 119.7563\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 120.1603 - mae: 120.1603 - val_loss: 119.6578 - val_mae: 119.6578\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 201us/sample - loss: 120.0524 - mae: 120.0524 - val_loss: 119.5552 - val_mae: 119.5552\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 207us/sample - loss: 119.9388 - mae: 119.9388 - val_loss: 119.4470 - val_mae: 119.4470\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 119.8194 - mae: 119.8194 - val_loss: 119.3322 - val_mae: 119.3322\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 119.6942 - mae: 119.6942 - val_loss: 119.2129 - val_mae: 119.2128\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 119.5630 - mae: 119.5630 - val_loss: 119.0873 - val_mae: 119.0873\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 119.4258 - mae: 119.4258 - val_loss: 118.9555 - val_mae: 118.9555\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 119.2825 - mae: 119.2825 - val_loss: 118.8188 - val_mae: 118.8188\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 119.1331 - mae: 119.1332 - val_loss: 118.6744 - val_mae: 118.6744\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 118.9776 - mae: 118.9776 - val_loss: 118.5226 - val_mae: 118.5226\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 118.8159 - mae: 118.8159 - val_loss: 118.3666 - val_mae: 118.3666\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 118.6479 - mae: 118.6479 - val_loss: 118.2071 - val_mae: 118.2071\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 118.4736 - mae: 118.4736 - val_loss: 118.0376 - val_mae: 118.0376\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 118.2930 - mae: 118.2930 - val_loss: 117.8638 - val_mae: 117.8638\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 118.1061 - mae: 118.1061 - val_loss: 117.6842 - val_mae: 117.6842\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 117.9129 - mae: 117.9129 - val_loss: 117.4997 - val_mae: 117.4997\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 117.7132 - mae: 117.7132 - val_loss: 117.3131 - val_mae: 117.3131\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 207us/sample - loss: 117.5071 - mae: 117.5071 - val_loss: 117.1150 - val_mae: 117.1150\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 117.2947 - mae: 117.2947 - val_loss: 116.9099 - val_mae: 116.9099\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 117.0757 - mae: 117.0757 - val_loss: 116.6949 - val_mae: 116.6949\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 116.8503 - mae: 116.8503 - val_loss: 116.4722 - val_mae: 116.4722\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 116.6185 - mae: 116.6185 - val_loss: 116.2451 - val_mae: 116.2451\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 116.3801 - mae: 116.3801 - val_loss: 116.0074 - val_mae: 116.0074\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 116.1353 - mae: 116.1353 - val_loss: 115.7650 - val_mae: 115.7650\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 196us/sample - loss: 115.8840 - mae: 115.8840 - val_loss: 115.5138 - val_mae: 115.5138\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 115.6261 - mae: 115.6261 - val_loss: 115.2585 - val_mae: 115.2585\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 115.3618 - mae: 115.3618 - val_loss: 114.9940 - val_mae: 114.9940\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 115.0909 - mae: 115.0909 - val_loss: 114.7261 - val_mae: 114.7261\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 114.8135 - mae: 114.8135 - val_loss: 114.4498 - val_mae: 114.4498\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 114.5295 - mae: 114.5295 - val_loss: 114.1669 - val_mae: 114.1669\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 114.2390 - mae: 114.2390 - val_loss: 113.8731 - val_mae: 113.8731\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 113.9420 - mae: 113.9420 - val_loss: 113.5784 - val_mae: 113.5784\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 113.6385 - mae: 113.6385 - val_loss: 113.2753 - val_mae: 113.2753\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 201us/sample - loss: 113.3284 - mae: 113.3284 - val_loss: 112.9705 - val_mae: 112.9705\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 113.0117 - mae: 113.0117 - val_loss: 112.6536 - val_mae: 112.6536\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 112.6886 - mae: 112.6886 - val_loss: 112.3365 - val_mae: 112.3365\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 112.3589 - mae: 112.3589 - val_loss: 112.0140 - val_mae: 112.0140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 112.0226 - mae: 112.0227 - val_loss: 111.6845 - val_mae: 111.6845\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 111.6799 - mae: 111.6799 - val_loss: 111.3471 - val_mae: 111.3471\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 179us/sample - loss: 111.3306 - mae: 111.3306 - val_loss: 111.0040 - val_mae: 111.0040\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 110.9748 - mae: 110.9748 - val_loss: 110.6373 - val_mae: 110.6373\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 110.6124 - mae: 110.6124 - val_loss: 110.2610 - val_mae: 110.2611\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 110.2436 - mae: 110.2436 - val_loss: 109.8756 - val_mae: 109.8756\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 109.8682 - mae: 109.8682 - val_loss: 109.4824 - val_mae: 109.4824\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 174us/sample - loss: 109.4863 - mae: 109.4863 - val_loss: 109.0773 - val_mae: 109.0773\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 109.0979 - mae: 109.0979 - val_loss: 108.6701 - val_mae: 108.6701\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 108.7030 - mae: 108.7030 - val_loss: 108.2510 - val_mae: 108.2509\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 108.3017 - mae: 108.3017 - val_loss: 107.8315 - val_mae: 107.8315\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 107.8938 - mae: 107.8938 - val_loss: 107.4241 - val_mae: 107.4241\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 107.4795 - mae: 107.4795 - val_loss: 107.0123 - val_mae: 107.0123\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 107.0587 - mae: 107.0587 - val_loss: 106.5903 - val_mae: 106.5904\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 106.6314 - mae: 106.6314 - val_loss: 106.1679 - val_mae: 106.1679\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 201us/sample - loss: 106.1977 - mae: 106.1977 - val_loss: 105.7450 - val_mae: 105.7449\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 105.7576 - mae: 105.7576 - val_loss: 105.3080 - val_mae: 105.3080\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 105.3109 - mae: 105.3110 - val_loss: 104.8622 - val_mae: 104.8622\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 104.8579 - mae: 104.8579 - val_loss: 104.4065 - val_mae: 104.4065\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 104.3984 - mae: 104.3984 - val_loss: 103.9300 - val_mae: 103.9300\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 103.9325 - mae: 103.9325 - val_loss: 103.4617 - val_mae: 103.4617\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 103.4602 - mae: 103.4603 - val_loss: 103.0004 - val_mae: 103.0004\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 102.9816 - mae: 102.9816 - val_loss: 102.5415 - val_mae: 102.5415\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 102.4965 - mae: 102.4965 - val_loss: 102.0832 - val_mae: 102.0832\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 102.0050 - mae: 102.0050 - val_loss: 101.6009 - val_mae: 101.6009\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 101.5071 - mae: 101.5071 - val_loss: 101.1070 - val_mae: 101.1070\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 101.0029 - mae: 101.0029 - val_loss: 100.5830 - val_mae: 100.5830\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 100.4924 - mae: 100.4924 - val_loss: 100.0431 - val_mae: 100.0431\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 99.9754 - mae: 99.9754 - val_loss: 99.5167 - val_mae: 99.5167\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 99.4522 - mae: 99.4522 - val_loss: 98.9798 - val_mae: 98.9798\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 179us/sample - loss: 98.9226 - mae: 98.9226 - val_loss: 98.4583 - val_mae: 98.4583\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 98.3867 - mae: 98.3867 - val_loss: 97.9232 - val_mae: 97.9232\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 97.8445 - mae: 97.8445 - val_loss: 97.3742 - val_mae: 97.3742\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 97.2960 - mae: 97.2960 - val_loss: 96.8001 - val_mae: 96.8001\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 96.7412 - mae: 96.7412 - val_loss: 96.2031 - val_mae: 96.2031\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 96.1801 - mae: 96.1801 - val_loss: 95.5887 - val_mae: 95.5887\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 95.6128 - mae: 95.6128 - val_loss: 94.9678 - val_mae: 94.9678\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 95.0391 - mae: 95.0391 - val_loss: 94.3762 - val_mae: 94.3762\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 94.4593 - mae: 94.4593 - val_loss: 93.7751 - val_mae: 93.7751\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 93.8732 - mae: 93.8732 - val_loss: 93.1723 - val_mae: 93.1723\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 93.2808 - mae: 93.2808 - val_loss: 92.5456 - val_mae: 92.5456\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 92.6823 - mae: 92.6823 - val_loss: 91.9385 - val_mae: 91.9385\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 92.0775 - mae: 92.0775 - val_loss: 91.3322 - val_mae: 91.3322\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 91.4665 - mae: 91.4666 - val_loss: 90.7263 - val_mae: 90.7263\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 90.8494 - mae: 90.8494 - val_loss: 90.1111 - val_mae: 90.1111\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 90.2261 - mae: 90.2261 - val_loss: 89.4896 - val_mae: 89.4896\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 171us/sample - loss: 89.5965 - mae: 89.5965 - val_loss: 88.8538 - val_mae: 88.8538\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 201us/sample - loss: 88.9609 - mae: 88.9609 - val_loss: 88.1809 - val_mae: 88.1809\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 88.3191 - mae: 88.3190 - val_loss: 87.5289 - val_mae: 87.5289\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 87.6711 - mae: 87.6711 - val_loss: 86.9233 - val_mae: 86.9233\n",
      "159/159 [==============================] - 0s 63us/sample - loss: 86.9233 - mae: 86.9233\n",
      "Val score is 86.92330932617188\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 8ms/sample - loss: 0.9255 - accuracy: 0.4969 - val_loss: 0.8533 - val_accuracy: 0.4088\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 181us/sample - loss: 0.7079 - accuracy: 0.6352 - val_loss: 0.7580 - val_accuracy: 0.4591\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.5224 - accuracy: 0.7421 - val_loss: 0.6831 - val_accuracy: 0.5220\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.4470 - accuracy: 0.8302 - val_loss: 0.6215 - val_accuracy: 0.6164\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.3776 - accuracy: 0.8742 - val_loss: 0.5715 - val_accuracy: 0.6855\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 189us/sample - loss: 0.3493 - accuracy: 0.8805 - val_loss: 0.5304 - val_accuracy: 0.7484\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.3112 - accuracy: 0.8994 - val_loss: 0.4947 - val_accuracy: 0.7736\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.2998 - accuracy: 0.9308 - val_loss: 0.4631 - val_accuracy: 0.8113\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.2543 - accuracy: 0.9308 - val_loss: 0.4340 - val_accuracy: 0.8553\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.2453 - accuracy: 0.9245 - val_loss: 0.4083 - val_accuracy: 0.8742\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.2259 - accuracy: 0.9308 - val_loss: 0.3836 - val_accuracy: 0.8994\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.1930 - accuracy: 0.9497 - val_loss: 0.3613 - val_accuracy: 0.9119\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 185us/sample - loss: 0.2052 - accuracy: 0.9371 - val_loss: 0.3413 - val_accuracy: 0.9308\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.1845 - accuracy: 0.9308 - val_loss: 0.3230 - val_accuracy: 0.9371\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.1558 - accuracy: 0.9686 - val_loss: 0.3052 - val_accuracy: 0.9371\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.2028 - accuracy: 0.9245 - val_loss: 0.2887 - val_accuracy: 0.9560\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 184us/sample - loss: 0.1535 - accuracy: 0.9497 - val_loss: 0.2734 - val_accuracy: 0.9560\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.1543 - accuracy: 0.9560 - val_loss: 0.2593 - val_accuracy: 0.9560\n",
      "159/159 [==============================] - 0s 67us/sample - loss: 0.2593 - accuracy: 0.9560\n",
      "Val score is 0.955974817276001\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 8ms/sample - loss: 3.2835 - mae: 3.2835 - val_loss: 4.1696 - val_mae: 4.1696\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 3.2401 - mae: 3.2401 - val_loss: 4.0818 - val_mae: 4.0818\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 179us/sample - loss: 3.1948 - mae: 3.1948 - val_loss: 3.9903 - val_mae: 3.9903\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 3.1464 - mae: 3.1464 - val_loss: 3.8870 - val_mae: 3.8870\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 3.0965 - mae: 3.0965 - val_loss: 3.7903 - val_mae: 3.7903\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 3.0429 - mae: 3.0429 - val_loss: 3.6959 - val_mae: 3.6959\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 2.9857 - mae: 2.9857 - val_loss: 3.5912 - val_mae: 3.5912\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 2.9273 - mae: 2.9273 - val_loss: 3.4606 - val_mae: 3.4606\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 2.8601 - mae: 2.8601 - val_loss: 3.3304 - val_mae: 3.3304\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 185us/sample - loss: 2.7943 - mae: 2.7943 - val_loss: 3.2035 - val_mae: 3.2035\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.7161 - mae: 2.7161 - val_loss: 3.0702 - val_mae: 3.0702\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.6444 - mae: 2.6444 - val_loss: 2.9231 - val_mae: 2.9231\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.5543 - mae: 2.5543 - val_loss: 2.7813 - val_mae: 2.7813\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 2.4690 - mae: 2.4690 - val_loss: 2.6415 - val_mae: 2.6415\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.3830 - mae: 2.3830 - val_loss: 2.4561 - val_mae: 2.4561\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.2763 - mae: 2.2763 - val_loss: 2.2808 - val_mae: 2.2808\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.1909 - mae: 2.1909 - val_loss: 2.1129 - val_mae: 2.1129\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.0699 - mae: 2.0699 - val_loss: 1.9627 - val_mae: 1.9627\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 1.9602 - mae: 1.9602 - val_loss: 1.8184 - val_mae: 1.8184\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.8466 - mae: 1.8466 - val_loss: 1.6687 - val_mae: 1.6687\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.7199 - mae: 1.7199 - val_loss: 1.5289 - val_mae: 1.5289\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 1.5916 - mae: 1.5916 - val_loss: 1.3773 - val_mae: 1.3773\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 1.4830 - mae: 1.4830 - val_loss: 1.2275 - val_mae: 1.2275\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.3753 - mae: 1.3753 - val_loss: 1.0390 - val_mae: 1.0390\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.2239 - mae: 1.2239 - val_loss: 0.8343 - val_mae: 0.8343\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 1.1210 - mae: 1.1210 - val_loss: 0.6820 - val_mae: 0.6820\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.9891 - mae: 0.9891 - val_loss: 0.5837 - val_mae: 0.5837\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 0.8577 - mae: 0.8577 - val_loss: 0.5388 - val_mae: 0.5388\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.7748 - mae: 0.7748 - val_loss: 0.4602 - val_mae: 0.4602\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 0.7041 - mae: 0.7041 - val_loss: 0.4004 - val_mae: 0.4004\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.5598 - mae: 0.5598 - val_loss: 0.3268 - val_mae: 0.3268\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.5904 - mae: 0.5904 - val_loss: 0.3396 - val_mae: 0.3396\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.5123 - mae: 0.5123 - val_loss: 0.3443 - val_mae: 0.3443\n",
      "159/159 [==============================] - 0s 62us/sample - loss: 0.3443 - mae: 0.3443\n",
      "Val score is 0.34425556659698486\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 7ms/sample - loss: 3.2196 - mae: 3.2196 - val_loss: 2.7212 - val_mae: 2.7212\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 3.1681 - mae: 3.1681 - val_loss: 2.6647 - val_mae: 2.6647\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 3.1168 - mae: 3.1168 - val_loss: 2.6230 - val_mae: 2.6230\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 3.0659 - mae: 3.0659 - val_loss: 2.5789 - val_mae: 2.5789\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 3.0127 - mae: 3.0127 - val_loss: 2.5288 - val_mae: 2.5288\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 180us/sample - loss: 2.9577 - mae: 2.9577 - val_loss: 2.4781 - val_mae: 2.4781\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 2.8996 - mae: 2.8996 - val_loss: 2.4256 - val_mae: 2.4256\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 2.8387 - mae: 2.8387 - val_loss: 2.3793 - val_mae: 2.3793\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.7733 - mae: 2.7733 - val_loss: 2.3281 - val_mae: 2.3281\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.7040 - mae: 2.7040 - val_loss: 2.2694 - val_mae: 2.2694\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 191us/sample - loss: 2.6299 - mae: 2.6299 - val_loss: 2.2027 - val_mae: 2.2027\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.5529 - mae: 2.5529 - val_loss: 2.1120 - val_mae: 2.1120\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.4732 - mae: 2.4732 - val_loss: 2.0144 - val_mae: 2.0144\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 2.3889 - mae: 2.3889 - val_loss: 1.9211 - val_mae: 1.9211\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.2903 - mae: 2.2903 - val_loss: 1.8262 - val_mae: 1.8262\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 2.1931 - mae: 2.1931 - val_loss: 1.7356 - val_mae: 1.7356\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 2.1105 - mae: 2.1105 - val_loss: 1.6166 - val_mae: 1.6166\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.9901 - mae: 1.9901 - val_loss: 1.4786 - val_mae: 1.4786\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 1.9112 - mae: 1.9112 - val_loss: 1.4118 - val_mae: 1.4118\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.7721 - mae: 1.7721 - val_loss: 1.3503 - val_mae: 1.3503\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 1.6607 - mae: 1.6607 - val_loss: 1.2611 - val_mae: 1.2611\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.5457 - mae: 1.5457 - val_loss: 1.1441 - val_mae: 1.1441\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 1.4324 - mae: 1.4324 - val_loss: 0.9938 - val_mae: 0.9938\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 1.2987 - mae: 1.2987 - val_loss: 0.8320 - val_mae: 0.8320\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 1.2213 - mae: 1.2213 - val_loss: 0.7256 - val_mae: 0.7256\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 1.1293 - mae: 1.1293 - val_loss: 0.6578 - val_mae: 0.6578\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.9624 - mae: 0.9624 - val_loss: 0.5640 - val_mae: 0.5640\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 0.8721 - mae: 0.8721 - val_loss: 0.4758 - val_mae: 0.4758\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 183us/sample - loss: 0.7954 - mae: 0.7954 - val_loss: 0.4700 - val_mae: 0.4700\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 0.6746 - mae: 0.6746 - val_loss: 0.4053 - val_mae: 0.4053\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 173us/sample - loss: 0.5626 - mae: 0.5626 - val_loss: 0.4092 - val_mae: 0.4092\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 0.5629 - mae: 0.5629 - val_loss: 0.3906 - val_mae: 0.3906\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 201us/sample - loss: 0.5069 - mae: 0.5069 - val_loss: 0.4055 - val_mae: 0.4055\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.4594 - mae: 0.4594 - val_loss: 0.3697 - val_mae: 0.3697\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 0.4058 - mae: 0.4058 - val_loss: 0.3804 - val_mae: 0.3804\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 0.4145 - mae: 0.4145 - val_loss: 0.4122 - val_mae: 0.4122\n",
      "159/159 [==============================] - 0s 63us/sample - loss: 0.4122 - mae: 0.4122\n",
      "Val score is 0.4121820628643036\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 7ms/sample - loss: 95.8182 - mae: 95.8182 - val_loss: 95.5978 - val_mae: 95.5978\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 95.7707 - mae: 95.7707 - val_loss: 95.5658 - val_mae: 95.5658\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 95.7206 - mae: 95.7206 - val_loss: 95.5298 - val_mae: 95.5298\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 95.6673 - mae: 95.6673 - val_loss: 95.4906 - val_mae: 95.4906\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 95.6105 - mae: 95.6105 - val_loss: 95.4489 - val_mae: 95.4489\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 95.5499 - mae: 95.5499 - val_loss: 95.4028 - val_mae: 95.4028\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 95.4851 - mae: 95.4851 - val_loss: 95.3520 - val_mae: 95.3520\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 95.4159 - mae: 95.4159 - val_loss: 95.2967 - val_mae: 95.2967\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 95.3419 - mae: 95.3419 - val_loss: 95.2350 - val_mae: 95.2350\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 195us/sample - loss: 95.2630 - mae: 95.2630 - val_loss: 95.1687 - val_mae: 95.1687\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 95.1789 - mae: 95.1789 - val_loss: 95.0970 - val_mae: 95.0970\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 180us/sample - loss: 95.0896 - mae: 95.0896 - val_loss: 95.0192 - val_mae: 95.0192\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 181us/sample - loss: 94.9948 - mae: 94.9948 - val_loss: 94.9382 - val_mae: 94.9382\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 182us/sample - loss: 94.8945 - mae: 94.8945 - val_loss: 94.8525 - val_mae: 94.8525\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 94.7885 - mae: 94.7885 - val_loss: 94.7573 - val_mae: 94.7573\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 94.6767 - mae: 94.6767 - val_loss: 94.6584 - val_mae: 94.6584\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 176us/sample - loss: 94.5590 - mae: 94.5590 - val_loss: 94.5528 - val_mae: 94.5528\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 171us/sample - loss: 94.4354 - mae: 94.4354 - val_loss: 94.4388 - val_mae: 94.4388\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 94.3058 - mae: 94.3058 - val_loss: 94.3174 - val_mae: 94.3174\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 189us/sample - loss: 94.1701 - mae: 94.1701 - val_loss: 94.1909 - val_mae: 94.1909\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 170us/sample - loss: 94.0284 - mae: 94.0284 - val_loss: 94.0576 - val_mae: 94.0576\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 93.8804 - mae: 93.8804 - val_loss: 93.9181 - val_mae: 93.9181\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 93.7262 - mae: 93.7262 - val_loss: 93.7719 - val_mae: 93.7719\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 93.5657 - mae: 93.5657 - val_loss: 93.6189 - val_mae: 93.6189\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 93.3990 - mae: 93.3990 - val_loss: 93.4566 - val_mae: 93.4566\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 93.2259 - mae: 93.2259 - val_loss: 93.2908 - val_mae: 93.2908\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 93.0465 - mae: 93.0465 - val_loss: 93.1177 - val_mae: 93.1177\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 92.8607 - mae: 92.8606 - val_loss: 92.9348 - val_mae: 92.9348\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 92.6684 - mae: 92.6684 - val_loss: 92.7408 - val_mae: 92.7408\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 92.4698 - mae: 92.4698 - val_loss: 92.5427 - val_mae: 92.5427\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 92.2647 - mae: 92.2647 - val_loss: 92.3407 - val_mae: 92.3407\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 147us/sample - loss: 92.0531 - mae: 92.0531 - val_loss: 92.1333 - val_mae: 92.1333\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.8350 - mae: 91.8350 - val_loss: 91.9180 - val_mae: 91.9180\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 91.6105 - mae: 91.6105 - val_loss: 91.6946 - val_mae: 91.6946\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 91.3794 - mae: 91.3794 - val_loss: 91.4614 - val_mae: 91.4615\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 91.1418 - mae: 91.1418 - val_loss: 91.2311 - val_mae: 91.2311\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 90.8977 - mae: 90.8977 - val_loss: 90.9868 - val_mae: 90.9868\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 90.6471 - mae: 90.6471 - val_loss: 90.7345 - val_mae: 90.7345\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 90.3899 - mae: 90.3899 - val_loss: 90.4733 - val_mae: 90.4733\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 90.1261 - mae: 90.1262 - val_loss: 90.2030 - val_mae: 90.2030\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 89.8559 - mae: 89.8559 - val_loss: 89.9242 - val_mae: 89.9242\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 89.5790 - mae: 89.5790 - val_loss: 89.6437 - val_mae: 89.6437\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 89.2956 - mae: 89.2956 - val_loss: 89.3576 - val_mae: 89.3576\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 89.0057 - mae: 89.0057 - val_loss: 89.0611 - val_mae: 89.0611\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 88.7092 - mae: 88.7092 - val_loss: 88.7562 - val_mae: 88.7562\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 153us/sample - loss: 88.4061 - mae: 88.4061 - val_loss: 88.4475 - val_mae: 88.4475\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 88.0965 - mae: 88.0965 - val_loss: 88.1312 - val_mae: 88.1312\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 87.7803 - mae: 87.7803 - val_loss: 87.8091 - val_mae: 87.8091\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 87.4575 - mae: 87.4575 - val_loss: 87.4789 - val_mae: 87.4789\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 87.1282 - mae: 87.1282 - val_loss: 87.1402 - val_mae: 87.1402\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 86.7923 - mae: 86.7923 - val_loss: 86.7907 - val_mae: 86.7907\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 86.4499 - mae: 86.4499 - val_loss: 86.4397 - val_mae: 86.4397\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 86.1010 - mae: 86.1010 - val_loss: 86.0803 - val_mae: 86.0803\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 85.7455 - mae: 85.7455 - val_loss: 85.7236 - val_mae: 85.7236\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 85.3835 - mae: 85.3835 - val_loss: 85.3625 - val_mae: 85.3625\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 85.0149 - mae: 85.0149 - val_loss: 84.9874 - val_mae: 84.9874\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - ETA: 0s - loss: 91.1410 - mae: 91.141 - 0s 136us/sample - loss: 84.6398 - mae: 84.6398 - val_loss: 84.6036 - val_mae: 84.6036\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 84.2582 - mae: 84.2582 - val_loss: 84.1957 - val_mae: 84.1957\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 83.8701 - mae: 83.8701 - val_loss: 83.7932 - val_mae: 83.7932\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 83.4754 - mae: 83.4754 - val_loss: 83.3839 - val_mae: 83.3839\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 83.0743 - mae: 83.0743 - val_loss: 82.9687 - val_mae: 82.9687\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 82.6667 - mae: 82.6667 - val_loss: 82.5317 - val_mae: 82.5317\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 82.2525 - mae: 82.2525 - val_loss: 82.0929 - val_mae: 82.0929\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 81.8319 - mae: 81.8319 - val_loss: 81.6576 - val_mae: 81.6576\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 81.4049 - mae: 81.4049 - val_loss: 81.2177 - val_mae: 81.2177\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 80.9713 - mae: 80.9713 - val_loss: 80.7822 - val_mae: 80.7822\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 80.5313 - mae: 80.5313 - val_loss: 80.3435 - val_mae: 80.3435\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 80.0849 - mae: 80.0849 - val_loss: 79.8949 - val_mae: 79.8949\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 79.6320 - mae: 79.6320 - val_loss: 79.4199 - val_mae: 79.4199\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 79.1727 - mae: 79.1727 - val_loss: 78.9418 - val_mae: 78.9418\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 78.7069 - mae: 78.7069 - val_loss: 78.4526 - val_mae: 78.4526\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 78.2347 - mae: 78.2347 - val_loss: 77.9615 - val_mae: 77.9615\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 77.7562 - mae: 77.7562 - val_loss: 77.4626 - val_mae: 77.4626\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 77.2712 - mae: 77.2712 - val_loss: 76.9722 - val_mae: 76.9722\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 76.7798 - mae: 76.7798 - val_loss: 76.4758 - val_mae: 76.4758\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 76.2821 - mae: 76.2821 - val_loss: 75.9738 - val_mae: 75.9738\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 75.7780 - mae: 75.7780 - val_loss: 75.4688 - val_mae: 75.4688\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 75.2675 - mae: 75.2675 - val_loss: 74.9481 - val_mae: 74.9481\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 74.7507 - mae: 74.7507 - val_loss: 74.4277 - val_mae: 74.4277\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 74.2275 - mae: 74.2275 - val_loss: 73.8968 - val_mae: 73.8968\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 73.6980 - mae: 73.6980 - val_loss: 73.3702 - val_mae: 73.3702\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 73.1621 - mae: 73.1621 - val_loss: 72.8446 - val_mae: 72.8446\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 72.6387 - mae: 72.6387 - val_loss: 71.7994 - val_mae: 71.7994\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 72.0748 - mae: 72.0748 - val_loss: 70.5269 - val_mae: 70.5269\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 71.5226 - mae: 71.5226 - val_loss: 69.6096 - val_mae: 69.6096\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 70.9630 - mae: 70.9630 - val_loss: 68.8781 - val_mae: 68.8781\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 70.3965 - mae: 70.3966 - val_loss: 68.2449 - val_mae: 68.2449\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 69.8234 - mae: 69.8234 - val_loss: 67.6587 - val_mae: 67.6587\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 132us/sample - loss: 69.2438 - mae: 69.2438 - val_loss: 67.1061 - val_mae: 67.1061\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 68.6578 - mae: 68.6578 - val_loss: 66.5620 - val_mae: 66.5620\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 68.0656 - mae: 68.0656 - val_loss: 66.0255 - val_mae: 66.0255\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 67.4670 - mae: 67.4670 - val_loss: 65.4703 - val_mae: 65.4703\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 66.8622 - mae: 66.8622 - val_loss: 64.9260 - val_mae: 64.9260\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 66.2512 - mae: 66.2512 - val_loss: 64.3709 - val_mae: 64.3709\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 130us/sample - loss: 65.6340 - mae: 65.6340 - val_loss: 63.8039 - val_mae: 63.8039\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 65.0106 - mae: 65.0106 - val_loss: 63.2333 - val_mae: 63.2332\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 64.3810 - mae: 64.3810 - val_loss: 62.6594 - val_mae: 62.6594\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 63.7453 - mae: 63.7453 - val_loss: 62.0637 - val_mae: 62.0637\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 63.1034 - mae: 63.1034 - val_loss: 61.4655 - val_mae: 61.4655\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 62.4553 - mae: 62.4553 - val_loss: 60.8660 - val_mae: 60.8660\n",
      "159/159 [==============================] - 0s 50us/sample - loss: 60.8660 - mae: 60.8660\n",
      "Val score is 60.865966796875\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/sample - loss: 5113.8170 - mae: 5113.8169 - val_loss: 5113.3668 - val_mae: 5113.3667\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5113.7671 - mae: 5113.7671 - val_loss: 5113.3290 - val_mae: 5113.3291\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5113.7144 - mae: 5113.7148 - val_loss: 5113.2880 - val_mae: 5113.2881\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 5113.6586 - mae: 5113.6582 - val_loss: 5113.2417 - val_mae: 5113.2412\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5113.5990 - mae: 5113.5991 - val_loss: 5113.1907 - val_mae: 5113.1909\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5113.5360 - mae: 5113.5361 - val_loss: 5113.1368 - val_mae: 5113.1367\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 5113.4686 - mae: 5113.4688 - val_loss: 5113.0772 - val_mae: 5113.0771\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5113.3970 - mae: 5113.3965 - val_loss: 5113.0125 - val_mae: 5113.0122\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5113.3204 - mae: 5113.3198 - val_loss: 5112.9442 - val_mae: 5112.9443\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 5113.2390 - mae: 5113.2393 - val_loss: 5112.8718 - val_mae: 5112.8721\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 5113.1526 - mae: 5113.1528 - val_loss: 5112.7915 - val_mae: 5112.7915\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 152us/sample - loss: 5113.0609 - mae: 5113.0605 - val_loss: 5112.7050 - val_mae: 5112.7051\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.9637 - mae: 5112.9639 - val_loss: 5112.6144 - val_mae: 5112.6138\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 5112.8611 - mae: 5112.8613 - val_loss: 5112.5172 - val_mae: 5112.5171\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5112.7527 - mae: 5112.7529 - val_loss: 5112.4142 - val_mae: 5112.4146\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 5112.6390 - mae: 5112.6392 - val_loss: 5112.3070 - val_mae: 5112.3071\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.5191 - mae: 5112.5190 - val_loss: 5112.1921 - val_mae: 5112.1919\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5112.3932 - mae: 5112.3931 - val_loss: 5112.0675 - val_mae: 5112.0674\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.2615 - mae: 5112.2617 - val_loss: 5111.9399 - val_mae: 5111.9399\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5112.1238 - mae: 5112.1240 - val_loss: 5111.8039 - val_mae: 5111.8037\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5111.9800 - mae: 5111.9805 - val_loss: 5111.6647 - val_mae: 5111.6646\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5111.8298 - mae: 5111.8301 - val_loss: 5111.5173 - val_mae: 5111.5171\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5111.6739 - mae: 5111.6738 - val_loss: 5111.3640 - val_mae: 5111.3638\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5111.5118 - mae: 5111.5117 - val_loss: 5111.2033 - val_mae: 5111.2026\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5111.3429 - mae: 5111.3433 - val_loss: 5111.0363 - val_mae: 5111.0361\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5111.1683 - mae: 5111.1685 - val_loss: 5110.8638 - val_mae: 5110.8638\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5110.9869 - mae: 5110.9868 - val_loss: 5110.6831 - val_mae: 5110.6831\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5110.7995 - mae: 5110.7993 - val_loss: 5110.5008 - val_mae: 5110.5010\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 131us/sample - loss: 5110.6056 - mae: 5110.6060 - val_loss: 5110.3115 - val_mae: 5110.3115\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5110.4051 - mae: 5110.4048 - val_loss: 5110.1149 - val_mae: 5110.1147\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 5110.1985 - mae: 5110.1987 - val_loss: 5109.9094 - val_mae: 5109.9097\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5109.9856 - mae: 5109.9854 - val_loss: 5109.6972 - val_mae: 5109.6973\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5109.7659 - mae: 5109.7656 - val_loss: 5109.4800 - val_mae: 5109.4800\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5109.5397 - mae: 5109.5396 - val_loss: 5109.2543 - val_mae: 5109.2544\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 5109.3073 - mae: 5109.3076 - val_loss: 5109.0217 - val_mae: 5109.0215\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5109.0682 - mae: 5109.0684 - val_loss: 5108.7892 - val_mae: 5108.7896\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5108.8226 - mae: 5108.8223 - val_loss: 5108.5470 - val_mae: 5108.5474\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5108.5704 - mae: 5108.5703 - val_loss: 5108.3021 - val_mae: 5108.3018\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5108.3120 - mae: 5108.3120 - val_loss: 5108.0502 - val_mae: 5108.0503\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5108.0470 - mae: 5108.0469 - val_loss: 5107.7870 - val_mae: 5107.7871\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5107.7753 - mae: 5107.7754 - val_loss: 5107.5133 - val_mae: 5107.5132\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5107.4971 - mae: 5107.4971 - val_loss: 5107.2380 - val_mae: 5107.2383\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 138us/sample - loss: 5107.2123 - mae: 5107.2129 - val_loss: 5106.9561 - val_mae: 5106.9565\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5106.9215 - mae: 5106.9214 - val_loss: 5106.6665 - val_mae: 5106.6665\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 5106.6237 - mae: 5106.6235 - val_loss: 5106.3702 - val_mae: 5106.3701\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 5106.3192 - mae: 5106.3193 - val_loss: 5106.0669 - val_mae: 5106.0669\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 5106.0085 - mae: 5106.0088 - val_loss: 5105.7561 - val_mae: 5105.7563\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 131us/sample - loss: 5105.6912 - mae: 5105.6914 - val_loss: 5105.4352 - val_mae: 5105.4351\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5105.3672 - mae: 5105.3672 - val_loss: 5105.1003 - val_mae: 5105.1001\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5105.0370 - mae: 5105.0371 - val_loss: 5104.7636 - val_mae: 5104.7637\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5104.6999 - mae: 5104.6997 - val_loss: 5104.4259 - val_mae: 5104.4263\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 5104.3564 - mae: 5104.3560 - val_loss: 5104.0870 - val_mae: 5104.0869\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 5104.0064 - mae: 5104.0063 - val_loss: 5103.7447 - val_mae: 5103.7451\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5103.6497 - mae: 5103.6494 - val_loss: 5103.3933 - val_mae: 5103.3936\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 5103.2867 - mae: 5103.2871 - val_loss: 5103.0362 - val_mae: 5103.0366\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 5102.9171 - mae: 5102.9170 - val_loss: 5102.6674 - val_mae: 5102.6670\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5102.5412 - mae: 5102.5415 - val_loss: 5102.3007 - val_mae: 5102.3013\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5102.1585 - mae: 5102.1587 - val_loss: 5101.9184 - val_mae: 5101.9185\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5101.7694 - mae: 5101.7695 - val_loss: 5101.5262 - val_mae: 5101.5259\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 5101.3739 - mae: 5101.3740 - val_loss: 5101.1422 - val_mae: 5101.1421\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5100.9720 - mae: 5100.9717 - val_loss: 5100.7355 - val_mae: 5100.7358\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 5100.5633 - mae: 5100.5630 - val_loss: 5100.3122 - val_mae: 5100.3120\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5100.1483 - mae: 5100.1479 - val_loss: 5099.8895 - val_mae: 5099.8896\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5099.7269 - mae: 5099.7266 - val_loss: 5099.4464 - val_mae: 5099.4463\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 5099.2989 - mae: 5099.2993 - val_loss: 5099.0033 - val_mae: 5099.0029\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 5098.8644 - mae: 5098.8643 - val_loss: 5098.5598 - val_mae: 5098.5596\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5098.4236 - mae: 5098.4238 - val_loss: 5098.1240 - val_mae: 5098.1240\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5097.9764 - mae: 5097.9766 - val_loss: 5097.6950 - val_mae: 5097.6948\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 5097.5228 - mae: 5097.5229 - val_loss: 5097.2578 - val_mae: 5097.2578\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5097.0624 - mae: 5097.0620 - val_loss: 5096.7989 - val_mae: 5096.7988\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 5096.5960 - mae: 5096.5962 - val_loss: 5096.3378 - val_mae: 5096.3379\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5096.1230 - mae: 5096.1230 - val_loss: 5095.8642 - val_mae: 5095.8638\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 150us/sample - loss: 5095.6437 - mae: 5095.6440 - val_loss: 5095.3874 - val_mae: 5095.3877\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5095.1579 - mae: 5095.1582 - val_loss: 5094.8996 - val_mae: 5094.8994\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5094.6657 - mae: 5094.6655 - val_loss: 5094.4206 - val_mae: 5094.4209\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5094.1674 - mae: 5094.1675 - val_loss: 5093.9453 - val_mae: 5093.9453\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5093.6624 - mae: 5093.6621 - val_loss: 5093.4642 - val_mae: 5093.4644\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5093.1510 - mae: 5093.1509 - val_loss: 5092.9826 - val_mae: 5092.9829\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 5092.6336 - mae: 5092.6338 - val_loss: 5092.4893 - val_mae: 5092.4897\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5092.1097 - mae: 5092.1099 - val_loss: 5091.9850 - val_mae: 5091.9849\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5091.5796 - mae: 5091.5796 - val_loss: 5091.4761 - val_mae: 5091.4761\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5091.0431 - mae: 5091.0435 - val_loss: 5090.9204 - val_mae: 5090.9204\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5090.5002 - mae: 5090.5000 - val_loss: 5090.3795 - val_mae: 5090.3789\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 5089.9510 - mae: 5089.9507 - val_loss: 5089.8215 - val_mae: 5089.8218\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5089.3956 - mae: 5089.3960 - val_loss: 5089.2354 - val_mae: 5089.2354\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5088.8338 - mae: 5088.8340 - val_loss: 5088.6468 - val_mae: 5088.6470\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 5088.2660 - mae: 5088.2661 - val_loss: 5088.0690 - val_mae: 5088.0688\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5087.6920 - mae: 5087.6924 - val_loss: 5087.4774 - val_mae: 5087.4771\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5087.1113 - mae: 5087.1108 - val_loss: 5086.8920 - val_mae: 5086.8921\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 5086.5245 - mae: 5086.5244 - val_loss: 5086.3059 - val_mae: 5086.3057\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 5085.9316 - mae: 5085.9316 - val_loss: 5085.7102 - val_mae: 5085.7104\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 5085.3324 - mae: 5085.3325 - val_loss: 5085.1270 - val_mae: 5085.1270\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5084.7271 - mae: 5084.7271 - val_loss: 5084.5038 - val_mae: 5084.5039\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5084.1153 - mae: 5084.1157 - val_loss: 5083.8866 - val_mae: 5083.8867\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5083.4979 - mae: 5083.4976 - val_loss: 5083.2496 - val_mae: 5083.2500\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 5082.8737 - mae: 5082.8740 - val_loss: 5082.6105 - val_mae: 5082.6108\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5082.2436 - mae: 5082.2432 - val_loss: 5081.9941 - val_mae: 5081.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 5081.6075 - mae: 5081.6079 - val_loss: 5081.3881 - val_mae: 5081.3882\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 5080.9650 - mae: 5080.9648 - val_loss: 5080.7681 - val_mae: 5080.7681\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 5080.3166 - mae: 5080.3164 - val_loss: 5080.1276 - val_mae: 5080.1279\n",
      "159/159 [==============================] - 0s 52us/sample - loss: 5080.1276 - mae: 5080.1279\n",
      "Val score is 5080.1279296875\n",
      "Train on 159 samples, validate on 159 samples\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 1s 5ms/sample - loss: 11445.7075 - mae: 11445.7070 - val_loss: 11446.1926 - val_mae: 11446.1924\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.6510 - mae: 11445.6514 - val_loss: 11446.1289 - val_mae: 11446.1289\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 0s 135us/sample - loss: 11445.5908 - mae: 11445.5898 - val_loss: 11446.0657 - val_mae: 11446.0664\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11445.5280 - mae: 11445.5283 - val_loss: 11445.9964 - val_mae: 11445.9961\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.4622 - mae: 11445.4619 - val_loss: 11445.9246 - val_mae: 11445.9248\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 11445.3922 - mae: 11445.3926 - val_loss: 11445.8498 - val_mae: 11445.8496\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 0s 141us/sample - loss: 11445.3180 - mae: 11445.3184 - val_loss: 11445.7710 - val_mae: 11445.7715\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 11445.2395 - mae: 11445.2402 - val_loss: 11445.6859 - val_mae: 11445.6865\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 11445.1560 - mae: 11445.1553 - val_loss: 11445.5957 - val_mae: 11445.5957\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11445.0684 - mae: 11445.0684 - val_loss: 11445.5014 - val_mae: 11445.5020\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 0s 130us/sample - loss: 11444.9754 - mae: 11444.9756 - val_loss: 11445.4007 - val_mae: 11445.4004\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 11444.8769 - mae: 11444.8770 - val_loss: 11445.2941 - val_mae: 11445.2939\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11444.7731 - mae: 11444.7734 - val_loss: 11445.1787 - val_mae: 11445.1787\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 11444.6641 - mae: 11444.6631 - val_loss: 11445.0585 - val_mae: 11445.0586\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11444.5497 - mae: 11444.5508 - val_loss: 11444.9326 - val_mae: 11444.9326\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 11444.4298 - mae: 11444.4297 - val_loss: 11444.8033 - val_mae: 11444.8037\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11444.3034 - mae: 11444.3027 - val_loss: 11444.6659 - val_mae: 11444.6660\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 11444.1723 - mae: 11444.1709 - val_loss: 11444.5244 - val_mae: 11444.5244\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 11444.0347 - mae: 11444.0342 - val_loss: 11444.3748 - val_mae: 11444.3740\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11443.8916 - mae: 11443.8916 - val_loss: 11444.2198 - val_mae: 11444.2197\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 0s 131us/sample - loss: 11443.7411 - mae: 11443.7402 - val_loss: 11444.0613 - val_mae: 11444.0605\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 11443.5863 - mae: 11443.5869 - val_loss: 11443.8928 - val_mae: 11443.8936\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11443.4250 - mae: 11443.4248 - val_loss: 11443.7232 - val_mae: 11443.7236\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 11443.2573 - mae: 11443.2568 - val_loss: 11443.5447 - val_mae: 11443.5439\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11443.0835 - mae: 11443.0830 - val_loss: 11443.3590 - val_mae: 11443.3594\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11442.9033 - mae: 11442.9033 - val_loss: 11443.1667 - val_mae: 11443.1670\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11442.7173 - mae: 11442.7168 - val_loss: 11442.9651 - val_mae: 11442.9648\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 11442.5249 - mae: 11442.5254 - val_loss: 11442.7596 - val_mae: 11442.7598\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11442.3266 - mae: 11442.3271 - val_loss: 11442.5472 - val_mae: 11442.5469\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11442.1214 - mae: 11442.1211 - val_loss: 11442.3295 - val_mae: 11442.3291\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11441.9099 - mae: 11441.9092 - val_loss: 11442.1054 - val_mae: 11442.1055\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11441.6929 - mae: 11441.6934 - val_loss: 11441.8734 - val_mae: 11441.8730\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11441.4689 - mae: 11441.4688 - val_loss: 11441.6383 - val_mae: 11441.6387\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 11441.2383 - mae: 11441.2383 - val_loss: 11441.3980 - val_mae: 11441.3975\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11441.0016 - mae: 11441.0020 - val_loss: 11441.1497 - val_mae: 11441.1514\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 0s 133us/sample - loss: 11440.7590 - mae: 11440.7598 - val_loss: 11440.8976 - val_mae: 11440.8975\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11440.5095 - mae: 11440.5098 - val_loss: 11440.6333 - val_mae: 11440.6328\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 11440.2534 - mae: 11440.2539 - val_loss: 11440.3640 - val_mae: 11440.3643\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 11439.9908 - mae: 11439.9902 - val_loss: 11440.0882 - val_mae: 11440.0879\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11439.7221 - mae: 11439.7227 - val_loss: 11439.8080 - val_mae: 11439.8086\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11439.4471 - mae: 11439.4473 - val_loss: 11439.5185 - val_mae: 11439.5186\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11439.1655 - mae: 11439.1650 - val_loss: 11439.2246 - val_mae: 11439.2246\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11438.8771 - mae: 11438.8770 - val_loss: 11438.9237 - val_mae: 11438.9238\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11438.5827 - mae: 11438.5830 - val_loss: 11438.6162 - val_mae: 11438.6162\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11438.2815 - mae: 11438.2812 - val_loss: 11438.3123 - val_mae: 11438.3125\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 0s 144us/sample - loss: 11437.9740 - mae: 11437.9736 - val_loss: 11437.9962 - val_mae: 11437.9971\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11437.6600 - mae: 11437.6602 - val_loss: 11437.6655 - val_mae: 11437.6660\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 0s 157us/sample - loss: 11437.3395 - mae: 11437.3408 - val_loss: 11437.3234 - val_mae: 11437.3242\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11437.0128 - mae: 11437.0137 - val_loss: 11436.9795 - val_mae: 11436.9785\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 0s 132us/sample - loss: 11436.6793 - mae: 11436.6797 - val_loss: 11436.6323 - val_mae: 11436.6328\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11436.3393 - mae: 11436.3389 - val_loss: 11436.2740 - val_mae: 11436.2744\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11435.9933 - mae: 11435.9941 - val_loss: 11435.9120 - val_mae: 11435.9121\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 11435.6405 - mae: 11435.6406 - val_loss: 11435.5484 - val_mae: 11435.5488\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 11435.2810 - mae: 11435.2812 - val_loss: 11435.1901 - val_mae: 11435.1904\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11434.9153 - mae: 11434.9150 - val_loss: 11434.8232 - val_mae: 11434.8232\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 0s 147us/sample - loss: 11434.5434 - mae: 11434.5430 - val_loss: 11434.4420 - val_mae: 11434.4414\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11434.1640 - mae: 11434.1650 - val_loss: 11434.0467 - val_mae: 11434.0459\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11433.7795 - mae: 11433.7803 - val_loss: 11433.6378 - val_mae: 11433.6377\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11433.3877 - mae: 11433.3877 - val_loss: 11433.2288 - val_mae: 11433.2285\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11432.9897 - mae: 11432.9902 - val_loss: 11432.8227 - val_mae: 11432.8223\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 11432.5854 - mae: 11432.5850 - val_loss: 11432.4124 - val_mae: 11432.4121\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11432.1748 - mae: 11432.1758 - val_loss: 11431.9922 - val_mae: 11431.9922\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11431.7573 - mae: 11431.7559 - val_loss: 11431.5698 - val_mae: 11431.5703\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11431.3333 - mae: 11431.3330 - val_loss: 11431.1518 - val_mae: 11431.1514\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11430.9031 - mae: 11430.9033 - val_loss: 11430.7168 - val_mae: 11430.7168\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 11430.4665 - mae: 11430.4668 - val_loss: 11430.2786 - val_mae: 11430.2783\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 0s 132us/sample - loss: 11430.0235 - mae: 11430.0234 - val_loss: 11429.8358 - val_mae: 11429.8350\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11429.5744 - mae: 11429.5742 - val_loss: 11429.3701 - val_mae: 11429.3701\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 0s 134us/sample - loss: 11429.1190 - mae: 11429.1191 - val_loss: 11428.9122 - val_mae: 11428.9121\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 0s 142us/sample - loss: 11428.6561 - mae: 11428.6553 - val_loss: 11428.4436 - val_mae: 11428.4424\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 0s 148us/sample - loss: 11428.1877 - mae: 11428.1885 - val_loss: 11427.9722 - val_mae: 11427.9717\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 11427.7128 - mae: 11427.7129 - val_loss: 11427.4836 - val_mae: 11427.4834\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11427.2316 - mae: 11427.2314 - val_loss: 11426.9946 - val_mae: 11426.9951\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 0s 137us/sample - loss: 11426.7441 - mae: 11426.7441 - val_loss: 11426.5056 - val_mae: 11426.5059\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11426.2500 - mae: 11426.2490 - val_loss: 11426.0103 - val_mae: 11426.0107\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11425.7495 - mae: 11425.7500 - val_loss: 11425.5222 - val_mae: 11425.5225\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11425.2426 - mae: 11425.2422 - val_loss: 11425.0335 - val_mae: 11425.0332\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11424.7297 - mae: 11424.7295 - val_loss: 11424.5205 - val_mae: 11424.5205\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11424.2104 - mae: 11424.2109 - val_loss: 11424.0027 - val_mae: 11424.0029\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11423.6851 - mae: 11423.6846 - val_loss: 11423.4738 - val_mae: 11423.4736\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 0s 133us/sample - loss: 11423.1533 - mae: 11423.1523 - val_loss: 11422.9477 - val_mae: 11422.9482\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11422.6150 - mae: 11422.6152 - val_loss: 11422.4172 - val_mae: 11422.4170\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 0s 139us/sample - loss: 11422.0702 - mae: 11422.0703 - val_loss: 11421.8681 - val_mae: 11421.8682\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11421.5197 - mae: 11421.5186 - val_loss: 11421.3076 - val_mae: 11421.3086\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11420.9626 - mae: 11420.9629 - val_loss: 11420.7396 - val_mae: 11420.7393\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 0s 136us/sample - loss: 11420.3988 - mae: 11420.3984 - val_loss: 11420.1692 - val_mae: 11420.1699\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11419.8301 - mae: 11419.8301 - val_loss: 11419.5660 - val_mae: 11419.5664\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11419.2542 - mae: 11419.2549 - val_loss: 11418.9736 - val_mae: 11418.9736\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 0s 146us/sample - loss: 11418.6719 - mae: 11418.6709 - val_loss: 11418.3828 - val_mae: 11418.3818\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11418.0837 - mae: 11418.0830 - val_loss: 11417.7545 - val_mae: 11417.7549\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 0s 149us/sample - loss: 11417.4892 - mae: 11417.4893 - val_loss: 11417.1227 - val_mae: 11417.1230\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11416.8887 - mae: 11416.8887 - val_loss: 11416.4721 - val_mae: 11416.4736\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 0s 143us/sample - loss: 11416.2821 - mae: 11416.2822 - val_loss: 11415.8667 - val_mae: 11415.8662\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11415.6688 - mae: 11415.6699 - val_loss: 11415.2424 - val_mae: 11415.2422\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11415.0495 - mae: 11415.0498 - val_loss: 11414.5983 - val_mae: 11414.5986\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 0s 145us/sample - loss: 11414.4245 - mae: 11414.4248 - val_loss: 11413.9481 - val_mae: 11413.9482\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 0s 138us/sample - loss: 11413.7931 - mae: 11413.7920 - val_loss: 11413.2759 - val_mae: 11413.2764\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 0s 140us/sample - loss: 11413.1553 - mae: 11413.1553 - val_loss: 11412.6056 - val_mae: 11412.6055\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 0s 214us/sample - loss: 11412.5118 - mae: 11412.5117 - val_loss: 11411.9604 - val_mae: 11411.9609\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 0s 151us/sample - loss: 11411.8618 - mae: 11411.8613 - val_loss: 11411.2852 - val_mae: 11411.2852\n",
      "159/159 [==============================] - 0s 46us/sample - loss: 11411.2852 - mae: 11411.2852\n",
      "Val score is 11411.28515625\n",
      "DataFrame: automobile imput_method :MLP model :KNN score:  0.53\n",
      "DataFrame: automobile imput_method :MLP model :XGB score:  0.72\n",
      "DataFrame: automobile imput_method :MLP model :MLP score:  0.63\n",
      "DataFrame: bands imput_method :LOCF model :KNN score:  0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: bands imput_method :LOCF model :XGB score:  0.75\n",
      "DataFrame: bands imput_method :LOCF model :MLP score:  0.65\n",
      "DataFrame: bands imput_method :mean_mode model :KNN score:  0.66\n",
      "DataFrame: bands imput_method :mean_mode model :XGB score:  0.73\n",
      "DataFrame: bands imput_method :mean_mode model :MLP score:  0.65\n",
      "DataFrame: bands imput_method :knn model :KNN score:  0.65\n",
      "DataFrame: bands imput_method :knn model :XGB score:  0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: bands imput_method :knn model :MLP score:  0.66\n",
      "Best score is -6.827911504879992\n",
      "Best score is -7.582080862495336\n",
      "Best score is -0.05964681004592685\n",
      "Best score is -0.9334678468757501\n",
      "Best score is -6.14385281946998\n",
      "Best score is -0.1455316149386636\n",
      "Best score is -5.908304020290689\n",
      "Best score is -0.8891749501100357\n",
      "Best score is -247.0807745061168\n",
      "Best score is -0.7178825620133447\n",
      "Best score is -1.0586290024430085\n",
      "Best score is -1.7060706868428372\n",
      "Best score is -0.06304273566755937\n",
      "Best score is -0.3557356930346103\n",
      "Best score is -0.24340054903399885\n",
      "Best score is -3.0541510331747403\n",
      "Best score is -1.985958690187652\n",
      "Best score is -4.421056663177232\n",
      "Best score is -0.7442619451916684\n",
      "DataFrame: bands imput_method :trees model :KNN score:  0.69\n",
      "DataFrame: bands imput_method :trees model :XGB score:  0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: bands imput_method :trees model :MLP score:  0.7\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 4ms/sample - loss: 44.9157 - mae: 44.9157 - val_loss: 44.4803 - val_mae: 44.4803\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 44.7872 - mae: 44.7872 - val_loss: 44.3406 - val_mae: 44.3406\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 44.6385 - mae: 44.6385 - val_loss: 44.1724 - val_mae: 44.1724\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 44.4653 - mae: 44.4653 - val_loss: 43.9745 - val_mae: 43.9744\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 44.2640 - mae: 44.2640 - val_loss: 43.7436 - val_mae: 43.7436\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 44.0323 - mae: 44.0323 - val_loss: 43.4815 - val_mae: 43.4815\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 43.7683 - mae: 43.7683 - val_loss: 43.1840 - val_mae: 43.1840\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 43.4707 - mae: 43.4707 - val_loss: 42.8542 - val_mae: 42.8541\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 43.1385 - mae: 43.1385 - val_loss: 42.4910 - val_mae: 42.4910\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 42.7709 - mae: 42.7709 - val_loss: 42.0933 - val_mae: 42.0933\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 42.3674 - mae: 42.3674 - val_loss: 41.6620 - val_mae: 41.6620\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 162us/sample - loss: 41.9275 - mae: 41.9275 - val_loss: 41.2002 - val_mae: 41.2002\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 41.4507 - mae: 41.4507 - val_loss: 40.7041 - val_mae: 40.7041\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 40.9369 - mae: 40.9369 - val_loss: 40.1736 - val_mae: 40.1736\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 40.3859 - mae: 40.3859 - val_loss: 39.6013 - val_mae: 39.6013\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 39.7973 - mae: 39.7973 - val_loss: 38.9992 - val_mae: 38.9992\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 39.1713 - mae: 39.1712 - val_loss: 38.3515 - val_mae: 38.3515\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 38.5075 - mae: 38.5075 - val_loss: 37.6749 - val_mae: 37.6749\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 37.8061 - mae: 37.8061 - val_loss: 36.9619 - val_mae: 36.9619\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 37.0671 - mae: 37.0671 - val_loss: 36.2221 - val_mae: 36.2221\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 36.2903 - mae: 36.2903 - val_loss: 35.4414 - val_mae: 35.4414\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 35.4758 - mae: 35.4758 - val_loss: 34.6235 - val_mae: 34.6235\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 34.6238 - mae: 34.6238 - val_loss: 33.7705 - val_mae: 33.7705\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 33.7342 - mae: 33.7342 - val_loss: 32.8843 - val_mae: 32.8843\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 32.8071 - mae: 32.8071 - val_loss: 31.9423 - val_mae: 31.9423\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 31.8426 - mae: 31.8426 - val_loss: 30.9447 - val_mae: 30.9447\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 30.8408 - mae: 30.8408 - val_loss: 29.9447 - val_mae: 29.9447\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 29.8017 - mae: 29.8017 - val_loss: 28.8891 - val_mae: 28.8891\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 28.7255 - mae: 28.7255 - val_loss: 27.7978 - val_mae: 27.7978\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 27.6413 - mae: 27.6413 - val_loss: 25.9387 - val_mae: 25.9387\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 26.4741 - mae: 26.4741 - val_loss: 24.4987 - val_mae: 24.4987\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 25.2900 - mae: 25.2901 - val_loss: 23.2969 - val_mae: 23.2969\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 24.0806 - mae: 24.0806 - val_loss: 22.1861 - val_mae: 22.1861\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 22.8253 - mae: 22.8253 - val_loss: 20.9690 - val_mae: 20.9690\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 21.5405 - mae: 21.5405 - val_loss: 19.6794 - val_mae: 19.6794\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 20.2330 - mae: 20.2330 - val_loss: 18.3997 - val_mae: 18.3997\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 18.8805 - mae: 18.8805 - val_loss: 17.0189 - val_mae: 17.0189\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 17.4962 - mae: 17.4962 - val_loss: 15.3313 - val_mae: 15.3313\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 16.0930 - mae: 16.0930 - val_loss: 13.5982 - val_mae: 13.5982\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 170us/sample - loss: 14.7020 - mae: 14.7020 - val_loss: 12.2454 - val_mae: 12.2455\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 13.3959 - mae: 13.3959 - val_loss: 10.5722 - val_mae: 10.5722\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 11.9232 - mae: 11.9232 - val_loss: 9.4263 - val_mae: 9.4263\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 10.5539 - mae: 10.5539 - val_loss: 8.3670 - val_mae: 8.3670\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 9.3716 - mae: 9.3716 - val_loss: 7.4012 - val_mae: 7.4012\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 8.2300 - mae: 8.2300 - val_loss: 6.4308 - val_mae: 6.4308\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 7.3374 - mae: 7.3374 - val_loss: 5.8053 - val_mae: 5.8053\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 6.3196 - mae: 6.3196 - val_loss: 5.1467 - val_mae: 5.1467\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 5.6754 - mae: 5.6754 - val_loss: 4.7469 - val_mae: 4.7469\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 5.3863 - mae: 5.3863 - val_loss: 4.4165 - val_mae: 4.4165\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 4.8071 - mae: 4.8071 - val_loss: 4.1884 - val_mae: 4.1884\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 4.5464 - mae: 4.5464 - val_loss: 3.9809 - val_mae: 3.9809\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 4.3759 - mae: 4.3759 - val_loss: 3.7772 - val_mae: 3.7772\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 4.3361 - mae: 4.3361 - val_loss: 3.7679 - val_mae: 3.7679\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 4.0681 - mae: 4.0681 - val_loss: 3.6454 - val_mae: 3.6454\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 4.2274 - mae: 4.2274 - val_loss: 3.4447 - val_mae: 3.4447\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 3.7746 - mae: 3.7746 - val_loss: 3.3793 - val_mae: 3.3793\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 3.7746 - mae: 3.7746 - val_loss: 3.2354 - val_mae: 3.2354\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 148us/sample - loss: 3.5615 - mae: 3.5615 - val_loss: 3.0402 - val_mae: 3.0402\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 3.8274 - mae: 3.8274 - val_loss: 3.0037 - val_mae: 3.0037\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.3141 - mae: 3.3141 - val_loss: 2.8681 - val_mae: 2.8681\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 3.5913 - mae: 3.5913 - val_loss: 2.9571 - val_mae: 2.9571\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.3204 - mae: 3.3204 - val_loss: 3.0057 - val_mae: 3.0057\n",
      "365/365 [==============================] - 0s 52us/sample - loss: 3.0057 - mae: 3.0057\n",
      "Val score is 3.005693197250366\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 50.7355 - mae: 50.7355 - val_loss: 50.9030 - val_mae: 50.9030\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 50.6147 - mae: 50.6147 - val_loss: 50.7604 - val_mae: 50.7604\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 50.4737 - mae: 50.4737 - val_loss: 50.5980 - val_mae: 50.5980\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 50.3080 - mae: 50.3080 - val_loss: 50.4090 - val_mae: 50.4090\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 50.1141 - mae: 50.1141 - val_loss: 50.1835 - val_mae: 50.1835\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 49.8894 - mae: 49.8894 - val_loss: 49.9288 - val_mae: 49.9288\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 49.6323 - mae: 49.6323 - val_loss: 49.6404 - val_mae: 49.6404\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 49.3414 - mae: 49.3414 - val_loss: 49.3137 - val_mae: 49.3137\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 49.0156 - mae: 49.0156 - val_loss: 48.9546 - val_mae: 48.9546\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 48.6542 - mae: 48.6542 - val_loss: 48.5603 - val_mae: 48.5603\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 48.2566 - mae: 48.2566 - val_loss: 48.1200 - val_mae: 48.1200\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 47.8224 - mae: 47.8224 - val_loss: 47.6537 - val_mae: 47.6537\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 47.3512 - mae: 47.3512 - val_loss: 47.1428 - val_mae: 47.1428\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 46.8427 - mae: 46.8427 - val_loss: 46.6055 - val_mae: 46.6055\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 46.2968 - mae: 46.2968 - val_loss: 46.0400 - val_mae: 46.0400\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 45.7132 - mae: 45.7132 - val_loss: 45.4334 - val_mae: 45.4334\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 45.0919 - mae: 45.0919 - val_loss: 44.7781 - val_mae: 44.7781\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 44.4328 - mae: 44.4328 - val_loss: 44.1052 - val_mae: 44.1052\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 43.7358 - mae: 43.7358 - val_loss: 43.3850 - val_mae: 43.3850\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 43.0011 - mae: 43.0011 - val_loss: 42.6126 - val_mae: 42.6126\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 42.2285 - mae: 42.2285 - val_loss: 41.8186 - val_mae: 41.8186\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 41.4181 - mae: 41.4181 - val_loss: 40.9935 - val_mae: 40.9935\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 40.5700 - mae: 40.5700 - val_loss: 40.1258 - val_mae: 40.1258\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 159us/sample - loss: 39.6842 - mae: 39.6842 - val_loss: 39.2110 - val_mae: 39.2110\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 38.7607 - mae: 38.7607 - val_loss: 38.2416 - val_mae: 38.2416\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 37.7998 - mae: 37.7998 - val_loss: 37.2680 - val_mae: 37.2680\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 36.8015 - mae: 36.8015 - val_loss: 36.2516 - val_mae: 36.2516\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 35.7658 - mae: 35.7658 - val_loss: 35.1653 - val_mae: 35.1653\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 34.6930 - mae: 34.6930 - val_loss: 34.0715 - val_mae: 34.0715\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 33.5830 - mae: 33.5830 - val_loss: 32.9360 - val_mae: 32.9360\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 32.4361 - mae: 32.4361 - val_loss: 31.7709 - val_mae: 31.7709\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 31.2523 - mae: 31.2523 - val_loss: 30.5633 - val_mae: 30.5633\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 30.0318 - mae: 30.0318 - val_loss: 29.3320 - val_mae: 29.3320\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 28.7747 - mae: 28.7747 - val_loss: 28.0415 - val_mae: 28.0415\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 27.4812 - mae: 27.4812 - val_loss: 26.7709 - val_mae: 26.7709\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 26.1514 - mae: 26.1514 - val_loss: 25.3855 - val_mae: 25.3855\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 24.7853 - mae: 24.7853 - val_loss: 23.9761 - val_mae: 23.9761\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 23.3845 - mae: 23.3845 - val_loss: 22.2927 - val_mae: 22.2927\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 21.9495 - mae: 21.9495 - val_loss: 20.1137 - val_mae: 20.1137\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 20.4884 - mae: 20.4884 - val_loss: 17.6460 - val_mae: 17.6460\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 19.0000 - mae: 19.0000 - val_loss: 16.0790 - val_mae: 16.0790\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 17.5417 - mae: 17.5417 - val_loss: 14.7082 - val_mae: 14.7082\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 15.9675 - mae: 15.9675 - val_loss: 13.3718 - val_mae: 13.3718\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 14.4900 - mae: 14.4900 - val_loss: 11.8863 - val_mae: 11.8863\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 12.9007 - mae: 12.9007 - val_loss: 10.9199 - val_mae: 10.9199\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 11.3871 - mae: 11.3871 - val_loss: 9.4153 - val_mae: 9.4153\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 9.8438 - mae: 9.8438 - val_loss: 8.3671 - val_mae: 8.3671\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 8.5604 - mae: 8.5604 - val_loss: 7.1923 - val_mae: 7.1923\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 7.3974 - mae: 7.3974 - val_loss: 6.3927 - val_mae: 6.3927\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 6.4573 - mae: 6.4573 - val_loss: 5.7211 - val_mae: 5.7211\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 5.8286 - mae: 5.8286 - val_loss: 5.2139 - val_mae: 5.2139\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 5.3419 - mae: 5.3419 - val_loss: 4.9189 - val_mae: 4.9189\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 142us/sample - loss: 4.9758 - mae: 4.9758 - val_loss: 4.6486 - val_mae: 4.6486\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 4.8722 - mae: 4.8722 - val_loss: 4.3680 - val_mae: 4.3680\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 4.6157 - mae: 4.6157 - val_loss: 4.0813 - val_mae: 4.0813\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 4.5702 - mae: 4.5702 - val_loss: 3.8727 - val_mae: 3.8727\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 4.4734 - mae: 4.4734 - val_loss: 3.7904 - val_mae: 3.7904\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 4.2738 - mae: 4.2738 - val_loss: 3.7810 - val_mae: 3.7810\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 4.1817 - mae: 4.1817 - val_loss: 3.5861 - val_mae: 3.5861\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 3.9821 - mae: 3.9821 - val_loss: 3.5234 - val_mae: 3.5234\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.9046 - mae: 3.9046 - val_loss: 3.3625 - val_mae: 3.3625\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.9540 - mae: 3.9540 - val_loss: 3.2593 - val_mae: 3.2593\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 4.1327 - mae: 4.1327 - val_loss: 3.2344 - val_mae: 3.2344\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.6218 - mae: 3.6218 - val_loss: 3.2121 - val_mae: 3.2121\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 3.6432 - mae: 3.6432 - val_loss: 3.1428 - val_mae: 3.1428\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.5693 - mae: 3.5693 - val_loss: 3.1217 - val_mae: 3.1217\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 3.7299 - mae: 3.7299 - val_loss: 2.9480 - val_mae: 2.9480\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.6032 - mae: 3.6032 - val_loss: 2.8935 - val_mae: 2.8935\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.3093 - mae: 3.3093 - val_loss: 2.8718 - val_mae: 2.8718\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.4471 - mae: 3.4471 - val_loss: 2.8637 - val_mae: 2.8637\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 3.3049 - mae: 3.3049 - val_loss: 2.8192 - val_mae: 2.8192\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.6283 - mae: 3.6283 - val_loss: 2.6815 - val_mae: 2.6815\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.3574 - mae: 3.3574 - val_loss: 2.7441 - val_mae: 2.7441\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.0371 - mae: 3.0371 - val_loss: 2.5949 - val_mae: 2.5949\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.0542 - mae: 3.0542 - val_loss: 2.5743 - val_mae: 2.5743\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.2662 - mae: 3.2662 - val_loss: 2.6443 - val_mae: 2.6443\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 3.2145 - mae: 3.2145 - val_loss: 2.5730 - val_mae: 2.5730\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 3.4059 - mae: 3.4059 - val_loss: 2.5611 - val_mae: 2.5611\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 3.3019 - mae: 3.3019 - val_loss: 2.6614 - val_mae: 2.6614\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 3.0821 - mae: 3.0821 - val_loss: 2.4884 - val_mae: 2.4884\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 3.0348 - mae: 3.0348 - val_loss: 2.3808 - val_mae: 2.3808\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 2.9830 - mae: 2.9830 - val_loss: 2.4000 - val_mae: 2.4000\n",
      "Epoch 83/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 3.0595 - mae: 3.0595 - val_loss: 2.3691 - val_mae: 2.3691\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 2.9837 - mae: 2.9837 - val_loss: 2.4392 - val_mae: 2.4392\n",
      "Epoch 85/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 2.9700 - mae: 2.9700 - val_loss: 2.3029 - val_mae: 2.3029\n",
      "Epoch 86/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 2.9786 - mae: 2.9786 - val_loss: 2.4505 - val_mae: 2.4505\n",
      "Epoch 87/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 2.9227 - mae: 2.9227 - val_loss: 2.5056 - val_mae: 2.5056\n",
      "365/365 [==============================] - 0s 47us/sample - loss: 2.5056 - mae: 2.5056\n",
      "Val score is 2.5055792331695557\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 4ms/sample - loss: 0.9666 - mae: 0.9666 - val_loss: 0.2287 - val_mae: 0.2287\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 178us/sample - loss: 0.7062 - mae: 0.7062 - val_loss: 0.1952 - val_mae: 0.1952\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 181us/sample - loss: 0.5552 - mae: 0.5552 - val_loss: 0.1846 - val_mae: 0.1846\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 164us/sample - loss: 0.4952 - mae: 0.4952 - val_loss: 0.1801 - val_mae: 0.1801\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 0.4068 - mae: 0.4068 - val_loss: 0.1761 - val_mae: 0.1761\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 0.3862 - mae: 0.3862 - val_loss: 0.1691 - val_mae: 0.1691\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 0.3635 - mae: 0.3635 - val_loss: 0.1662 - val_mae: 0.1662\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 0.3351 - mae: 0.3351 - val_loss: 0.1545 - val_mae: 0.1545\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 0.3073 - mae: 0.3073 - val_loss: 0.1454 - val_mae: 0.1454\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 172us/sample - loss: 0.3052 - mae: 0.3052 - val_loss: 0.1580 - val_mae: 0.1580\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 162us/sample - loss: 0.3049 - mae: 0.3049 - val_loss: 0.1507 - val_mae: 0.1507\n",
      "365/365 [==============================] - 0s 55us/sample - loss: 0.1507 - mae: 0.1507\n",
      "Val score is 0.15070095658302307\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 15.3084 - mae: 15.3084 - val_loss: 14.9802 - val_mae: 14.9802\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 15.1761 - mae: 15.1761 - val_loss: 14.8372 - val_mae: 14.8372\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 15.0238 - mae: 15.0238 - val_loss: 14.6659 - val_mae: 14.6659\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 14.8469 - mae: 14.8469 - val_loss: 14.4651 - val_mae: 14.4651\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 147us/sample - loss: 14.6422 - mae: 14.6422 - val_loss: 14.2297 - val_mae: 14.2297\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 14.4072 - mae: 14.4072 - val_loss: 13.9615 - val_mae: 13.9615\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 14.1402 - mae: 14.1402 - val_loss: 13.6608 - val_mae: 13.6608\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 147us/sample - loss: 13.8398 - mae: 13.8398 - val_loss: 13.3216 - val_mae: 13.3216\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 13.5052 - mae: 13.5052 - val_loss: 12.9437 - val_mae: 12.9437\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 13.1355 - mae: 13.1355 - val_loss: 12.5349 - val_mae: 12.5349\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 12.7300 - mae: 12.7300 - val_loss: 12.1044 - val_mae: 12.1044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 12.2884 - mae: 12.2884 - val_loss: 11.6364 - val_mae: 11.6364\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 11.8103 - mae: 11.8103 - val_loss: 11.1347 - val_mae: 11.1347\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 11.2953 - mae: 11.2953 - val_loss: 10.6076 - val_mae: 10.6076\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 144us/sample - loss: 10.7432 - mae: 10.7432 - val_loss: 10.0415 - val_mae: 10.0415\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 10.1592 - mae: 10.1592 - val_loss: 9.2474 - val_mae: 9.2474\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 9.5307 - mae: 9.5307 - val_loss: 8.5211 - val_mae: 8.5211\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 8.8690 - mae: 8.8690 - val_loss: 7.6849 - val_mae: 7.6848\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 8.1712 - mae: 8.1712 - val_loss: 6.8967 - val_mae: 6.8967\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 7.4513 - mae: 7.4513 - val_loss: 6.0456 - val_mae: 6.0456\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 6.7070 - mae: 6.7070 - val_loss: 5.2168 - val_mae: 5.2168\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 144us/sample - loss: 5.9009 - mae: 5.9009 - val_loss: 4.4371 - val_mae: 4.4371\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 147us/sample - loss: 5.1075 - mae: 5.1075 - val_loss: 3.6404 - val_mae: 3.6404\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 4.2680 - mae: 4.2680 - val_loss: 3.0252 - val_mae: 3.0252\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 3.4831 - mae: 3.4831 - val_loss: 2.4152 - val_mae: 2.4152\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 2.7251 - mae: 2.7251 - val_loss: 1.8204 - val_mae: 1.8204\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 1.9972 - mae: 1.9972 - val_loss: 1.4701 - val_mae: 1.4701\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 149us/sample - loss: 1.6267 - mae: 1.6267 - val_loss: 1.2643 - val_mae: 1.2643\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 1.4432 - mae: 1.4432 - val_loss: 1.0605 - val_mae: 1.0605\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 1.2539 - mae: 1.2539 - val_loss: 1.0071 - val_mae: 1.0071\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 1.1209 - mae: 1.1209 - val_loss: 0.9435 - val_mae: 0.9435\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 1.0469 - mae: 1.0469 - val_loss: 0.9000 - val_mae: 0.9000\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 1.0232 - mae: 1.0232 - val_loss: 0.7769 - val_mae: 0.7769\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 1.0092 - mae: 1.0092 - val_loss: 0.7664 - val_mae: 0.7664\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 0.9262 - mae: 0.9262 - val_loss: 0.7155 - val_mae: 0.7155\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 0.9905 - mae: 0.9905 - val_loss: 0.6840 - val_mae: 0.6840\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 0.8713 - mae: 0.8713 - val_loss: 0.6512 - val_mae: 0.6512\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 0.9006 - mae: 0.9006 - val_loss: 0.6631 - val_mae: 0.6631\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 0.8881 - mae: 0.8881 - val_loss: 0.6219 - val_mae: 0.6219\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 0.7841 - mae: 0.7841 - val_loss: 0.6245 - val_mae: 0.6245\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 162us/sample - loss: 0.7776 - mae: 0.7776 - val_loss: 0.6115 - val_mae: 0.6115\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 167us/sample - loss: 0.8404 - mae: 0.8404 - val_loss: 0.5621 - val_mae: 0.5621\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 151us/sample - loss: 0.7581 - mae: 0.7581 - val_loss: 0.5882 - val_mae: 0.5882\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 156us/sample - loss: 0.7484 - mae: 0.7484 - val_loss: 0.5822 - val_mae: 0.5822\n",
      "365/365 [==============================] - 0s 47us/sample - loss: 0.5822 - mae: 0.5822\n",
      "Val score is 0.5821779370307922\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 4ms/sample - loss: 78.6318 - mae: 78.6318 - val_loss: 78.5600 - val_mae: 78.5600\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 167us/sample - loss: 78.4993 - mae: 78.4993 - val_loss: 78.4420 - val_mae: 78.4420\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 78.3467 - mae: 78.3467 - val_loss: 78.3036 - val_mae: 78.3036\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 78.1694 - mae: 78.1694 - val_loss: 78.1427 - val_mae: 78.1427\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 150us/sample - loss: 77.9642 - mae: 77.9642 - val_loss: 77.9562 - val_mae: 77.9562\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 77.7285 - mae: 77.7285 - val_loss: 77.7331 - val_mae: 77.7331\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 77.4606 - mae: 77.4606 - val_loss: 77.4823 - val_mae: 77.4823\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 77.1593 - mae: 77.1593 - val_loss: 77.1910 - val_mae: 77.1910\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 147us/sample - loss: 76.8235 - mae: 76.8235 - val_loss: 76.8631 - val_mae: 76.8631\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 76.4525 - mae: 76.4525 - val_loss: 76.4960 - val_mae: 76.4960\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 76.0456 - mae: 76.0456 - val_loss: 76.0949 - val_mae: 76.0949\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 75.6025 - mae: 75.6025 - val_loss: 75.6506 - val_mae: 75.6506\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 75.1228 - mae: 75.1228 - val_loss: 75.1774 - val_mae: 75.1774\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 74.6062 - mae: 74.6062 - val_loss: 74.6550 - val_mae: 74.6550\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 144us/sample - loss: 74.0525 - mae: 74.0525 - val_loss: 74.0864 - val_mae: 74.0864\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 73.4614 - mae: 73.4614 - val_loss: 73.4852 - val_mae: 73.4852\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 72.8329 - mae: 72.8329 - val_loss: 72.8301 - val_mae: 72.8301\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 72.1670 - mae: 72.1670 - val_loss: 72.1339 - val_mae: 72.1339\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 71.4635 - mae: 71.4635 - val_loss: 71.3944 - val_mae: 71.3944\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 70.7224 - mae: 70.7224 - val_loss: 70.6239 - val_mae: 70.6239\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 69.9437 - mae: 69.9437 - val_loss: 69.8123 - val_mae: 69.8123\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 69.1275 - mae: 69.1275 - val_loss: 68.9735 - val_mae: 68.9736\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 68.2737 - mae: 68.2737 - val_loss: 68.0869 - val_mae: 68.0869\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 67.3825 - mae: 67.3825 - val_loss: 67.1553 - val_mae: 67.1553\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 140us/sample - loss: 66.4539 - mae: 66.4539 - val_loss: 66.1872 - val_mae: 66.1872\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 65.4880 - mae: 65.4880 - val_loss: 65.1834 - val_mae: 65.1834\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 64.4848 - mae: 64.4848 - val_loss: 64.1475 - val_mae: 64.1475\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 63.4444 - mae: 63.4444 - val_loss: 63.0785 - val_mae: 63.0785\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 62.3670 - mae: 62.3670 - val_loss: 61.9793 - val_mae: 61.9793\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 148us/sample - loss: 61.2527 - mae: 61.2527 - val_loss: 60.8282 - val_mae: 60.8282\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 144us/sample - loss: 60.1015 - mae: 60.1015 - val_loss: 59.6983 - val_mae: 59.6983\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 58.9136 - mae: 58.9135 - val_loss: 58.4679 - val_mae: 58.4679\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 57.6890 - mae: 57.6890 - val_loss: 57.2117 - val_mae: 57.2117\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 56.4281 - mae: 56.4281 - val_loss: 55.9410 - val_mae: 55.9410\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 55.1307 - mae: 55.1307 - val_loss: 54.5981 - val_mae: 54.5981\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 149us/sample - loss: 53.7972 - mae: 53.7972 - val_loss: 53.1992 - val_mae: 53.1992\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 52.4275 - mae: 52.4275 - val_loss: 51.7878 - val_mae: 51.7878\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 51.0220 - mae: 51.0220 - val_loss: 50.3938 - val_mae: 50.3938\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 49.5806 - mae: 49.5806 - val_loss: 48.9810 - val_mae: 48.9810\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 48.1035 - mae: 48.1035 - val_loss: 47.4536 - val_mae: 47.4536\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 46.5909 - mae: 46.5909 - val_loss: 45.9312 - val_mae: 45.9312\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 45.0429 - mae: 45.0429 - val_loss: 44.3067 - val_mae: 44.3067\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 144us/sample - loss: 43.4596 - mae: 43.4596 - val_loss: 42.6331 - val_mae: 42.6331\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 41.8412 - mae: 41.8413 - val_loss: 40.9222 - val_mae: 40.9222\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 40.1879 - mae: 40.1879 - val_loss: 39.2102 - val_mae: 39.2102\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 38.4997 - mae: 38.4997 - val_loss: 37.3885 - val_mae: 37.3885\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 36.7943 - mae: 36.7943 - val_loss: 35.1260 - val_mae: 35.1260\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 35.0258 - mae: 35.0258 - val_loss: 32.3719 - val_mae: 32.3719\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 33.2375 - mae: 33.2375 - val_loss: 30.4351 - val_mae: 30.4351\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 31.4122 - mae: 31.4122 - val_loss: 28.6947 - val_mae: 28.6947\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 29.5520 - mae: 29.5520 - val_loss: 26.9927 - val_mae: 26.9927\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 27.6643 - mae: 27.6643 - val_loss: 25.6084 - val_mae: 25.6084\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 25.7526 - mae: 25.7526 - val_loss: 23.7351 - val_mae: 23.7351\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 23.9107 - mae: 23.9107 - val_loss: 20.7887 - val_mae: 20.7887\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 21.9112 - mae: 21.9112 - val_loss: 17.9940 - val_mae: 17.9940\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 20.0008 - mae: 20.0008 - val_loss: 15.2380 - val_mae: 15.2380\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 18.0097 - mae: 18.0097 - val_loss: 12.6909 - val_mae: 12.6909\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 142us/sample - loss: 15.8945 - mae: 15.8945 - val_loss: 11.1418 - val_mae: 11.1418\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 14.0268 - mae: 14.0268 - val_loss: 9.3529 - val_mae: 9.3529\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 12.0588 - mae: 12.0588 - val_loss: 8.1028 - val_mae: 8.1028\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 10.2825 - mae: 10.2825 - val_loss: 7.2041 - val_mae: 7.2041\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 8.4697 - mae: 8.4697 - val_loss: 6.0641 - val_mae: 6.0641\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 138us/sample - loss: 7.2172 - mae: 7.2172 - val_loss: 5.1749 - val_mae: 5.1749\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 145us/sample - loss: 6.1859 - mae: 6.1859 - val_loss: 4.6821 - val_mae: 4.6821\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 5.1496 - mae: 5.1496 - val_loss: 4.4228 - val_mae: 4.4228\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 4.7464 - mae: 4.7464 - val_loss: 4.3287 - val_mae: 4.3287\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 146us/sample - loss: 4.7041 - mae: 4.7041 - val_loss: 4.1853 - val_mae: 4.1853\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 4.5413 - mae: 4.5413 - val_loss: 3.9569 - val_mae: 3.9569\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 4.0956 - mae: 4.0956 - val_loss: 3.5906 - val_mae: 3.5906\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 143us/sample - loss: 4.0455 - mae: 4.0455 - val_loss: 3.5268 - val_mae: 3.5268\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 137us/sample - loss: 4.1243 - mae: 4.1243 - val_loss: 3.4658 - val_mae: 3.4658\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 3.9632 - mae: 3.9632 - val_loss: 3.4869 - val_mae: 3.4869\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 139us/sample - loss: 3.9795 - mae: 3.9795 - val_loss: 3.2887 - val_mae: 3.2887\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 3.7085 - mae: 3.7085 - val_loss: 3.1721 - val_mae: 3.1721\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 3.7763 - mae: 3.7763 - val_loss: 3.1492 - val_mae: 3.1492\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 136us/sample - loss: 3.6880 - mae: 3.6880 - val_loss: 3.0366 - val_mae: 3.0366\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 3.7753 - mae: 3.7753 - val_loss: 2.9267 - val_mae: 2.9267\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 141us/sample - loss: 3.4967 - mae: 3.4967 - val_loss: 3.1438 - val_mae: 3.1438\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 135us/sample - loss: 3.4699 - mae: 3.4699 - val_loss: 2.8587 - val_mae: 2.8587\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 138us/sample - loss: 3.3456 - mae: 3.3456 - val_loss: 2.7945 - val_mae: 2.7945\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 135us/sample - loss: 3.3251 - mae: 3.3251 - val_loss: 2.7470 - val_mae: 2.7470\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 136us/sample - loss: 3.5001 - mae: 3.5001 - val_loss: 2.8242 - val_mae: 2.8242\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 148us/sample - loss: 3.1554 - mae: 3.1554 - val_loss: 2.5565 - val_mae: 2.5565\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 137us/sample - loss: 3.2209 - mae: 3.2209 - val_loss: 2.5839 - val_mae: 2.5839\n",
      "Epoch 85/100\n",
      "365/365 [==============================] - 0s 134us/sample - loss: 3.2629 - mae: 3.2629 - val_loss: 2.5360 - val_mae: 2.5360\n",
      "Epoch 86/100\n",
      "365/365 [==============================] - 0s 140us/sample - loss: 3.1501 - mae: 3.1501 - val_loss: 2.4502 - val_mae: 2.4502\n",
      "Epoch 87/100\n",
      "365/365 [==============================] - 0s 136us/sample - loss: 3.2850 - mae: 3.2850 - val_loss: 2.4098 - val_mae: 2.4098\n",
      "Epoch 88/100\n",
      "365/365 [==============================] - 0s 138us/sample - loss: 3.0925 - mae: 3.0925 - val_loss: 2.5500 - val_mae: 2.5500\n",
      "Epoch 89/100\n",
      "365/365 [==============================] - 0s 137us/sample - loss: 3.0738 - mae: 3.0738 - val_loss: 2.4985 - val_mae: 2.4985\n",
      "365/365 [==============================] - 0s 46us/sample - loss: 2.4985 - mae: 2.4985\n",
      "Val score is 2.4984705448150635\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 1.0162 - mae: 1.0162 - val_loss: 0.9483 - val_mae: 0.9483\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.7828 - mae: 0.7828 - val_loss: 0.7468 - val_mae: 0.7468\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.6486 - mae: 0.6486 - val_loss: 0.5753 - val_mae: 0.5753\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.5556 - mae: 0.5556 - val_loss: 0.4413 - val_mae: 0.4413\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 0.4880 - mae: 0.4880 - val_loss: 0.3209 - val_mae: 0.3209\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.4375 - mae: 0.4375 - val_loss: 0.2414 - val_mae: 0.2414\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 0.3731 - mae: 0.3731 - val_loss: 0.2074 - val_mae: 0.2074\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 0.3335 - mae: 0.3335 - val_loss: 0.1974 - val_mae: 0.1974\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 0.3175 - mae: 0.3175 - val_loss: 0.1851 - val_mae: 0.1851\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 0.2825 - mae: 0.2825 - val_loss: 0.1948 - val_mae: 0.1948\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.2972 - mae: 0.2972 - val_loss: 0.1899 - val_mae: 0.1899\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.1899 - mae: 0.1899\n",
      "Val score is 0.18994645774364471\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 30.8424 - mae: 30.8424 - val_loss: 30.8152 - val_mae: 30.8152\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 30.7217 - mae: 30.7217 - val_loss: 30.6545 - val_mae: 30.6545\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 30.5808 - mae: 30.5808 - val_loss: 30.4683 - val_mae: 30.4683\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 30.4154 - mae: 30.4154 - val_loss: 30.2521 - val_mae: 30.2521\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 30.2219 - mae: 30.2219 - val_loss: 30.0041 - val_mae: 30.0041\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 29.9979 - mae: 29.9979 - val_loss: 29.7219 - val_mae: 29.7219\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 29.7415 - mae: 29.7415 - val_loss: 29.4029 - val_mae: 29.4029\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 29.4514 - mae: 29.4514 - val_loss: 29.0532 - val_mae: 29.0532\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 29.1266 - mae: 29.1266 - val_loss: 28.6675 - val_mae: 28.6675\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 28.7664 - mae: 28.7664 - val_loss: 28.2529 - val_mae: 28.2529\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 28.3700 - mae: 28.3700 - val_loss: 27.8020 - val_mae: 27.8020\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 27.9370 - mae: 27.9370 - val_loss: 27.3212 - val_mae: 27.3212\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 27.4671 - mae: 27.4671 - val_loss: 26.8096 - val_mae: 26.8096\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 26.9600 - mae: 26.9600 - val_loss: 26.2542 - val_mae: 26.2542\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 26.4154 - mae: 26.4154 - val_loss: 25.6703 - val_mae: 25.6703\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 25.8332 - mae: 25.8332 - val_loss: 25.0638 - val_mae: 25.0638\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 25.2133 - mae: 25.2133 - val_loss: 24.4072 - val_mae: 24.4072\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 24.5555 - mae: 24.5555 - val_loss: 23.7187 - val_mae: 23.7187\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 23.8600 - mae: 23.8600 - val_loss: 23.0037 - val_mae: 23.0037\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 23.1265 - mae: 23.1265 - val_loss: 22.2530 - val_mae: 22.2530\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 22.3553 - mae: 22.3553 - val_loss: 21.4815 - val_mae: 21.4815\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 21.5462 - mae: 21.5462 - val_loss: 20.6623 - val_mae: 20.6623\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 20.6994 - mae: 20.6994 - val_loss: 19.7969 - val_mae: 19.7969\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 19.8148 - mae: 19.8148 - val_loss: 18.8839 - val_mae: 18.8839\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 18.9058 - mae: 18.9058 - val_loss: 17.8566 - val_mae: 17.8566\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 17.9461 - mae: 17.9461 - val_loss: 16.8130 - val_mae: 16.8130\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 104us/sample - loss: 16.9578 - mae: 16.9578 - val_loss: 15.5947 - val_mae: 15.5947\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 15.9744 - mae: 15.9744 - val_loss: 14.4460 - val_mae: 14.4460\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 14.9428 - mae: 14.9428 - val_loss: 13.2014 - val_mae: 13.2014\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 13.8338 - mae: 13.8338 - val_loss: 12.1644 - val_mae: 12.1644\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 12.7354 - mae: 12.7354 - val_loss: 11.2503 - val_mae: 11.2503\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 11.6540 - mae: 11.6540 - val_loss: 10.5113 - val_mae: 10.5113\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 10.5105 - mae: 10.5105 - val_loss: 9.7012 - val_mae: 9.7012\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 9.5012 - mae: 9.5012 - val_loss: 8.6716 - val_mae: 8.6716\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 8.5444 - mae: 8.5444 - val_loss: 7.5415 - val_mae: 7.5415\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 7.6067 - mae: 7.6067 - val_loss: 6.5848 - val_mae: 6.5848\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 6.8471 - mae: 6.8471 - val_loss: 5.8849 - val_mae: 5.8849\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 6.2410 - mae: 6.2410 - val_loss: 5.4717 - val_mae: 5.4717\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 107us/sample - loss: 5.7235 - mae: 5.7235 - val_loss: 5.1635 - val_mae: 5.1635\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 5.2793 - mae: 5.2793 - val_loss: 4.8305 - val_mae: 4.8305\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 5.1355 - mae: 5.1355 - val_loss: 4.5183 - val_mae: 4.5183\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 4.8631 - mae: 4.8631 - val_loss: 4.3511 - val_mae: 4.3510\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 4.5592 - mae: 4.5592 - val_loss: 4.1445 - val_mae: 4.1445\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 4.4327 - mae: 4.4327 - val_loss: 3.9184 - val_mae: 3.9184\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 4.3649 - mae: 4.3649 - val_loss: 3.7497 - val_mae: 3.7497\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 4.0637 - mae: 4.0637 - val_loss: 3.5726 - val_mae: 3.5726\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 4.1399 - mae: 4.1399 - val_loss: 3.5074 - val_mae: 3.5074\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 4.0652 - mae: 4.0652 - val_loss: 3.4345 - val_mae: 3.4345\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 4.0578 - mae: 4.0578 - val_loss: 3.4069 - val_mae: 3.4069\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 3.8636 - mae: 3.8636 - val_loss: 3.2913 - val_mae: 3.2913\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.7426 - mae: 3.7426 - val_loss: 3.2261 - val_mae: 3.2261\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 3.5162 - mae: 3.5162 - val_loss: 3.1689 - val_mae: 3.1689\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.6631 - mae: 3.6631 - val_loss: 3.0712 - val_mae: 3.0712\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.4922 - mae: 3.4922 - val_loss: 2.9694 - val_mae: 2.9694\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.5549 - mae: 3.5549 - val_loss: 2.8872 - val_mae: 2.8872\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.4622 - mae: 3.4622 - val_loss: 2.9447 - val_mae: 2.9447\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.2281 - mae: 3.2281 - val_loss: 2.7592 - val_mae: 2.7592\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 3.1281 - mae: 3.1281 - val_loss: 2.7245 - val_mae: 2.7245\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 3.3725 - mae: 3.3725 - val_loss: 2.7176 - val_mae: 2.7176\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 3.2415 - mae: 3.2415 - val_loss: 2.6772 - val_mae: 2.6772\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.3256 - mae: 3.3256 - val_loss: 2.6370 - val_mae: 2.6370\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 3.2027 - mae: 3.2027 - val_loss: 2.5142 - val_mae: 2.5142\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.0313 - mae: 3.0313 - val_loss: 2.4814 - val_mae: 2.4814\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.0637 - mae: 3.0637 - val_loss: 2.4575 - val_mae: 2.4575\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.9337 - mae: 2.9337 - val_loss: 2.5046 - val_mae: 2.5046\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.7713 - mae: 2.7713 - val_loss: 2.4664 - val_mae: 2.4664\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 2.4664 - mae: 2.4664\n",
      "Val score is 2.466418743133545\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 6.2687 - mae: 6.2687 - val_loss: 5.6132 - val_mae: 5.6132\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 5.8685 - mae: 5.8685 - val_loss: 5.4878 - val_mae: 5.4878\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 5.5973 - mae: 5.5973 - val_loss: 5.3816 - val_mae: 5.3816\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 5.3795 - mae: 5.3795 - val_loss: 5.2725 - val_mae: 5.2725\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 5.2331 - mae: 5.2331 - val_loss: 5.1677 - val_mae: 5.1677\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 5.0685 - mae: 5.0685 - val_loss: 5.0484 - val_mae: 5.0484\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.9029 - mae: 4.9029 - val_loss: 4.9156 - val_mae: 4.9156\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 4.7212 - mae: 4.7212 - val_loss: 4.7616 - val_mae: 4.7616\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.5043 - mae: 4.5043 - val_loss: 4.5926 - val_mae: 4.5926\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.2886 - mae: 4.2886 - val_loss: 4.3922 - val_mae: 4.3922\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 4.1023 - mae: 4.1023 - val_loss: 4.1683 - val_mae: 4.1683\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.7842 - mae: 3.7842 - val_loss: 3.9190 - val_mae: 3.9190\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 3.5114 - mae: 3.5114 - val_loss: 3.6305 - val_mae: 3.6305\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.2691 - mae: 3.2691 - val_loss: 3.3028 - val_mae: 3.3028\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 3.0267 - mae: 3.0267 - val_loss: 2.9315 - val_mae: 2.9315\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.6291 - mae: 2.6291 - val_loss: 2.6173 - val_mae: 2.6173\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 2.4086 - mae: 2.4086 - val_loss: 2.3535 - val_mae: 2.3535\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 2.0752 - mae: 2.0752 - val_loss: 2.1450 - val_mae: 2.1450\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.8956 - mae: 1.8956 - val_loss: 1.7087 - val_mae: 1.7087\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.6830 - mae: 1.6830 - val_loss: 1.5552 - val_mae: 1.5552\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.5872 - mae: 1.5872 - val_loss: 1.4144 - val_mae: 1.4144\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4981 - mae: 1.4981 - val_loss: 1.2180 - val_mae: 1.2180\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.3477 - mae: 1.3477 - val_loss: 0.9835 - val_mae: 0.9835\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.3788 - mae: 1.3788 - val_loss: 1.0285 - val_mae: 1.0285\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.3217 - mae: 1.3217 - val_loss: 0.9518 - val_mae: 0.9518\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.0870 - mae: 1.0870 - val_loss: 0.7892 - val_mae: 0.7892\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.5327 - mae: 1.5327 - val_loss: 0.8279 - val_mae: 0.8279\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.4226 - mae: 1.4226 - val_loss: 0.7516 - val_mae: 0.7516\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.5305 - mae: 1.5305 - val_loss: 0.7420 - val_mae: 0.7420\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.0926 - mae: 1.0926 - val_loss: 0.7398 - val_mae: 0.7398\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 112us/sample - loss: 1.3595 - mae: 1.3595 - val_loss: 0.6434 - val_mae: 0.6434\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1.2406 - mae: 1.2406 - val_loss: 0.6136 - val_mae: 0.6136\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.3422 - mae: 1.3422 - val_loss: 0.6291 - val_mae: 0.6291\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.1980 - mae: 1.1980 - val_loss: 0.6372 - val_mae: 0.6372\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.6372 - mae: 0.6372\n",
      "Val score is 0.6372067928314209\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 1851.1513 - mae: 1851.1514 - val_loss: 1850.5311 - val_mae: 1850.5311\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1851.0164 - mae: 1851.0165 - val_loss: 1850.4400 - val_mae: 1850.4399\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1850.8615 - mae: 1850.8615 - val_loss: 1850.3214 - val_mae: 1850.3214\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1850.6820 - mae: 1850.6820 - val_loss: 1850.1845 - val_mae: 1850.1846\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1850.4746 - mae: 1850.4749 - val_loss: 1850.0247 - val_mae: 1850.0247\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1850.2371 - mae: 1850.2372 - val_loss: 1849.8270 - val_mae: 1849.8270\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1849.9676 - mae: 1849.9674 - val_loss: 1849.6021 - val_mae: 1849.6021\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1849.6733 - mae: 1849.6733 - val_loss: 1849.2860 - val_mae: 1849.2861\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1849.3289 - mae: 1849.3290 - val_loss: 1848.9448 - val_mae: 1848.9451\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1848.9573 - mae: 1848.9574 - val_loss: 1848.6012 - val_mae: 1848.6013\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1848.5497 - mae: 1848.5498 - val_loss: 1848.2263 - val_mae: 1848.2262\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1848.1060 - mae: 1848.1060 - val_loss: 1847.8134 - val_mae: 1847.8134\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1847.6257 - mae: 1847.6257 - val_loss: 1847.3676 - val_mae: 1847.3678\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1847.1086 - mae: 1847.1088 - val_loss: 1846.8700 - val_mae: 1846.8700\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1846.5545 - mae: 1846.5546 - val_loss: 1846.3316 - val_mae: 1846.3317\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1845.9632 - mae: 1845.9634 - val_loss: 1845.7488 - val_mae: 1845.7487\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1845.3345 - mae: 1845.3342 - val_loss: 1845.1251 - val_mae: 1845.1251\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1844.6683 - mae: 1844.6683 - val_loss: 1844.4632 - val_mae: 1844.4634\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1843.9646 - mae: 1843.9646 - val_loss: 1843.7569 - val_mae: 1843.7568\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1843.2234 - mae: 1843.2235 - val_loss: 1843.0124 - val_mae: 1843.0123\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1842.4446 - mae: 1842.4446 - val_loss: 1842.2195 - val_mae: 1842.2195\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1841.6282 - mae: 1841.6283 - val_loss: 1841.3806 - val_mae: 1841.3806\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1840.7850 - mae: 1840.7849 - val_loss: 1840.4715 - val_mae: 1840.4716\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1839.8863 - mae: 1839.8864 - val_loss: 1839.3452 - val_mae: 1839.3452\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1838.9604 - mae: 1838.9603 - val_loss: 1838.3226 - val_mae: 1838.3224\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1837.9952 - mae: 1837.9954 - val_loss: 1837.2990 - val_mae: 1837.2990\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1836.9921 - mae: 1836.9921 - val_loss: 1836.2874 - val_mae: 1836.2874\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1835.9517 - mae: 1835.9517 - val_loss: 1835.2401 - val_mae: 1835.2402\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1834.8743 - mae: 1834.8745 - val_loss: 1834.1526 - val_mae: 1834.1527\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1833.7599 - mae: 1833.7599 - val_loss: 1833.0210 - val_mae: 1833.0212\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1832.6086 - mae: 1832.6088 - val_loss: 1831.8672 - val_mae: 1831.8671\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1831.4207 - mae: 1831.4205 - val_loss: 1830.6644 - val_mae: 1830.6646\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1830.1961 - mae: 1830.1963 - val_loss: 1829.4444 - val_mae: 1829.4443\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1828.9350 - mae: 1828.9351 - val_loss: 1828.1848 - val_mae: 1828.1849\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1827.6376 - mae: 1827.6377 - val_loss: 1826.8886 - val_mae: 1826.8887\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1826.3040 - mae: 1826.3040 - val_loss: 1825.5518 - val_mae: 1825.5518\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1824.9343 - mae: 1824.9342 - val_loss: 1824.1815 - val_mae: 1824.1814\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1823.5286 - mae: 1823.5288 - val_loss: 1822.7719 - val_mae: 1822.7720\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1822.0871 - mae: 1822.0870 - val_loss: 1821.3179 - val_mae: 1821.3180\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1820.6100 - mae: 1820.6100 - val_loss: 1819.8333 - val_mae: 1819.8334\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1819.0973 - mae: 1819.0973 - val_loss: 1818.2839 - val_mae: 1818.2839\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1817.5492 - mae: 1817.5493 - val_loss: 1816.6981 - val_mae: 1816.6980\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1815.9659 - mae: 1815.9659 - val_loss: 1815.1021 - val_mae: 1815.1021\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1814.3474 - mae: 1814.3477 - val_loss: 1813.4818 - val_mae: 1813.4818\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1812.6939 - mae: 1812.6936 - val_loss: 1811.8049 - val_mae: 1811.8049\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1811.0056 - mae: 1811.0056 - val_loss: 1810.1267 - val_mae: 1810.1266\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1809.3101 - mae: 1809.3103 - val_loss: 1807.5036 - val_mae: 1807.5034\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1807.5291 - mae: 1807.5291 - val_loss: 1805.3767 - val_mae: 1805.3768\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1805.7379 - mae: 1805.7378 - val_loss: 1803.5515 - val_mae: 1803.5515\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1803.9118 - mae: 1803.9120 - val_loss: 1801.7464 - val_mae: 1801.7465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1802.0514 - mae: 1802.0514 - val_loss: 1799.9310 - val_mae: 1799.9308\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1800.1570 - mae: 1800.1570 - val_loss: 1798.1074 - val_mae: 1798.1074\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1798.2287 - mae: 1798.2286 - val_loss: 1796.3230 - val_mae: 1796.3231\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1796.2667 - mae: 1796.2667 - val_loss: 1794.4091 - val_mae: 1794.4091\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1794.2712 - mae: 1794.2711 - val_loss: 1792.4394 - val_mae: 1792.4393\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1792.2422 - mae: 1792.2423 - val_loss: 1790.4917 - val_mae: 1790.4916\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1790.1799 - mae: 1790.1799 - val_loss: 1788.4503 - val_mae: 1788.4503\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1788.0845 - mae: 1788.0846 - val_loss: 1786.3466 - val_mae: 1786.3466\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1785.9561 - mae: 1785.9559 - val_loss: 1784.2317 - val_mae: 1784.2317\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1783.7948 - mae: 1783.7949 - val_loss: 1782.1133 - val_mae: 1782.1134\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1781.6023 - mae: 1781.6022 - val_loss: 1778.8894 - val_mae: 1778.8894\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1779.3780 - mae: 1779.3781 - val_loss: 1775.5578 - val_mae: 1775.5579\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1777.1205 - mae: 1777.1206 - val_loss: 1773.1113 - val_mae: 1773.1115\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1774.8294 - mae: 1774.8295 - val_loss: 1770.9533 - val_mae: 1770.9532\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1772.5057 - mae: 1772.5059 - val_loss: 1768.8290 - val_mae: 1768.8291\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1770.1499 - mae: 1770.1500 - val_loss: 1766.7125 - val_mae: 1766.7123\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1767.7623 - mae: 1767.7623 - val_loss: 1764.4432 - val_mae: 1764.4434\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1765.3427 - mae: 1765.3427 - val_loss: 1762.2618 - val_mae: 1762.2618\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1762.8915 - mae: 1762.8915 - val_loss: 1759.9476 - val_mae: 1759.9476\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1760.4087 - mae: 1760.4087 - val_loss: 1757.5676 - val_mae: 1757.5676\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1757.8946 - mae: 1757.8947 - val_loss: 1755.0712 - val_mae: 1755.0713\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1755.3491 - mae: 1755.3494 - val_loss: 1752.6673 - val_mae: 1752.6671\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1752.7725 - mae: 1752.7722 - val_loss: 1750.1216 - val_mae: 1750.1217\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1750.1649 - mae: 1750.1649 - val_loss: 1747.5839 - val_mae: 1747.5841\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1747.5264 - mae: 1747.5265 - val_loss: 1745.0158 - val_mae: 1745.0157\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1744.8570 - mae: 1744.8571 - val_loss: 1742.4820 - val_mae: 1742.4821\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1742.1571 - mae: 1742.1570 - val_loss: 1739.8317 - val_mae: 1739.8315\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1739.4265 - mae: 1739.4265 - val_loss: 1737.1548 - val_mae: 1737.1548\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1736.6656 - mae: 1736.6658 - val_loss: 1734.3980 - val_mae: 1734.3978\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1733.8744 - mae: 1733.8743 - val_loss: 1731.7021 - val_mae: 1731.7023\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1731.0530 - mae: 1731.0531 - val_loss: 1728.9926 - val_mae: 1728.9927\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1728.2016 - mae: 1728.2015 - val_loss: 1726.0950 - val_mae: 1726.0948\n",
      "Epoch 83/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1725.3700 - mae: 1725.3700 - val_loss: 1720.4168 - val_mae: 1720.4170\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1722.4149 - mae: 1722.4150 - val_loss: 1715.9786 - val_mae: 1715.9788\n",
      "Epoch 85/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1719.4756 - mae: 1719.4755 - val_loss: 1712.8796 - val_mae: 1712.8794\n",
      "Epoch 86/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1716.5054 - mae: 1716.5055 - val_loss: 1710.2015 - val_mae: 1710.2015\n",
      "Epoch 87/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1713.5054 - mae: 1713.5052 - val_loss: 1707.6173 - val_mae: 1707.6173\n",
      "Epoch 88/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1710.4759 - mae: 1710.4761 - val_loss: 1704.9380 - val_mae: 1704.9380\n",
      "Epoch 89/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1707.4173 - mae: 1707.4171 - val_loss: 1702.2639 - val_mae: 1702.2639\n",
      "Epoch 90/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1704.3294 - mae: 1704.3292 - val_loss: 1699.5408 - val_mae: 1699.5408\n",
      "Epoch 91/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1701.2728 - mae: 1701.2728 - val_loss: 1695.0152 - val_mae: 1695.0153\n",
      "Epoch 92/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1698.0725 - mae: 1698.0726 - val_loss: 1690.4163 - val_mae: 1690.4161\n",
      "Epoch 93/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1694.9000 - mae: 1694.8998 - val_loss: 1687.3904 - val_mae: 1687.3903\n",
      "Epoch 94/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1691.6972 - mae: 1691.6973 - val_loss: 1684.6560 - val_mae: 1684.6561\n",
      "Epoch 95/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1688.4654 - mae: 1688.4655 - val_loss: 1681.9052 - val_mae: 1681.9052\n",
      "Epoch 96/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1685.2049 - mae: 1685.2048 - val_loss: 1679.0824 - val_mae: 1679.0824\n",
      "Epoch 97/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1681.9160 - mae: 1681.9161 - val_loss: 1676.2627 - val_mae: 1676.2627\n",
      "Epoch 98/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1678.5989 - mae: 1678.5988 - val_loss: 1673.2402 - val_mae: 1673.2402\n",
      "Epoch 99/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1675.2535 - mae: 1675.2538 - val_loss: 1670.1534 - val_mae: 1670.1532\n",
      "Epoch 100/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1671.8801 - mae: 1671.8800 - val_loss: 1667.1318 - val_mae: 1667.1321\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 1667.1318 - mae: 1667.1321\n",
      "Val score is 1667.132080078125\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 55.5988 - mae: 55.5988 - val_loss: 55.5108 - val_mae: 55.5108\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 55.4770 - mae: 55.4770 - val_loss: 55.3346 - val_mae: 55.3346\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 55.3352 - mae: 55.3352 - val_loss: 55.1390 - val_mae: 55.1390\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 55.1688 - mae: 55.1688 - val_loss: 54.9196 - val_mae: 54.9196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 54.9745 - mae: 54.9745 - val_loss: 54.6699 - val_mae: 54.6699\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 54.7498 - mae: 54.7498 - val_loss: 54.3932 - val_mae: 54.3932\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 54.4930 - mae: 54.4930 - val_loss: 54.0882 - val_mae: 54.0882\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 54.2026 - mae: 54.2026 - val_loss: 53.7477 - val_mae: 53.7477\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 53.8777 - mae: 53.8777 - val_loss: 53.3759 - val_mae: 53.3759\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 53.5175 - mae: 53.5175 - val_loss: 52.9792 - val_mae: 52.9792\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 53.1213 - mae: 53.1213 - val_loss: 52.5366 - val_mae: 52.5366\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 52.6887 - mae: 52.6887 - val_loss: 52.0590 - val_mae: 52.0590\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 52.2193 - mae: 52.2193 - val_loss: 51.5456 - val_mae: 51.5456\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 51.7127 - mae: 51.7127 - val_loss: 51.0049 - val_mae: 51.0049\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 51.1688 - mae: 51.1688 - val_loss: 50.4363 - val_mae: 50.4363\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 50.5874 - mae: 50.5873 - val_loss: 49.8295 - val_mae: 49.8295\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 49.9682 - mae: 49.9682 - val_loss: 49.1904 - val_mae: 49.1904\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 49.3114 - mae: 49.3114 - val_loss: 48.5240 - val_mae: 48.5240\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 48.6168 - mae: 48.6168 - val_loss: 47.8150 - val_mae: 47.8150\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 47.8843 - mae: 47.8843 - val_loss: 47.0826 - val_mae: 47.0826\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 47.1141 - mae: 47.1141 - val_loss: 46.3004 - val_mae: 46.3004\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 46.3061 - mae: 46.3061 - val_loss: 45.4896 - val_mae: 45.4896\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 45.4603 - mae: 45.4603 - val_loss: 44.6508 - val_mae: 44.6508\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 44.5768 - mae: 44.5768 - val_loss: 43.7715 - val_mae: 43.7715\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 43.6558 - mae: 43.6558 - val_loss: 42.8440 - val_mae: 42.8440\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 42.6972 - mae: 42.6972 - val_loss: 41.8767 - val_mae: 41.8767\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 41.7012 - mae: 41.7012 - val_loss: 40.8816 - val_mae: 40.8816\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 40.6678 - mae: 40.6678 - val_loss: 39.8264 - val_mae: 39.8264\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 39.5972 - mae: 39.5972 - val_loss: 38.7688 - val_mae: 38.7688\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 38.4895 - mae: 38.4895 - val_loss: 37.6337 - val_mae: 37.6337\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 37.3448 - mae: 37.3448 - val_loss: 36.4599 - val_mae: 36.4599\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 36.1632 - mae: 36.1632 - val_loss: 35.2703 - val_mae: 35.2703\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 34.9449 - mae: 34.9449 - val_loss: 34.0386 - val_mae: 34.0386\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 33.6900 - mae: 33.6900 - val_loss: 32.7630 - val_mae: 32.7630\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 32.3986 - mae: 32.3986 - val_loss: 31.4465 - val_mae: 31.4465\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 31.0708 - mae: 31.0708 - val_loss: 30.1048 - val_mae: 30.1048\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 29.7068 - mae: 29.7068 - val_loss: 28.7083 - val_mae: 28.7083\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 28.3099 - mae: 28.3099 - val_loss: 27.0319 - val_mae: 27.0319\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 26.8752 - mae: 26.8752 - val_loss: 25.0806 - val_mae: 25.0806\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 25.4063 - mae: 25.4063 - val_loss: 23.5440 - val_mae: 23.5440\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 23.8995 - mae: 23.8995 - val_loss: 22.0844 - val_mae: 22.0844\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 22.3600 - mae: 22.3600 - val_loss: 20.4113 - val_mae: 20.4113\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 20.7839 - mae: 20.7839 - val_loss: 18.8058 - val_mae: 18.8058\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 19.2031 - mae: 19.2031 - val_loss: 17.0294 - val_mae: 17.0294\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 17.5861 - mae: 17.5861 - val_loss: 15.7562 - val_mae: 15.7562\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 15.9007 - mae: 15.9007 - val_loss: 14.1525 - val_mae: 14.1525\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 14.2899 - mae: 14.2899 - val_loss: 12.1807 - val_mae: 12.1807\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 12.5021 - mae: 12.5021 - val_loss: 10.5014 - val_mae: 10.5014\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 10.8202 - mae: 10.8202 - val_loss: 9.1738 - val_mae: 9.1738\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 9.0686 - mae: 9.0686 - val_loss: 6.8958 - val_mae: 6.8958\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 7.4296 - mae: 7.4296 - val_loss: 5.2576 - val_mae: 5.2576\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 6.0031 - mae: 6.0031 - val_loss: 3.8319 - val_mae: 3.8319\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.7446 - mae: 4.7446 - val_loss: 3.1224 - val_mae: 3.1224\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.6700 - mae: 3.6700 - val_loss: 2.5287 - val_mae: 2.5287\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.9628 - mae: 2.9628 - val_loss: 2.3487 - val_mae: 2.3487\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.4799 - mae: 2.4799 - val_loss: 1.9244 - val_mae: 1.9244\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 2.1673 - mae: 2.1673 - val_loss: 1.9606 - val_mae: 1.9606\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.0809 - mae: 2.0809 - val_loss: 1.6737 - val_mae: 1.6737\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.1936 - mae: 2.1936 - val_loss: 1.6552 - val_mae: 1.6552\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.8907 - mae: 1.8907 - val_loss: 1.5642 - val_mae: 1.5642\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.0702 - mae: 2.0702 - val_loss: 1.3196 - val_mae: 1.3196\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.8687 - mae: 1.8687 - val_loss: 1.3328 - val_mae: 1.3328\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 108us/sample - loss: 1.8869 - mae: 1.8870 - val_loss: 1.2641 - val_mae: 1.2641\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.6130 - mae: 1.6130 - val_loss: 1.0960 - val_mae: 1.0960\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.7656 - mae: 1.7656 - val_loss: 1.0551 - val_mae: 1.0551\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.4978 - mae: 1.4978 - val_loss: 1.0407 - val_mae: 1.0407\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.7745 - mae: 1.7745 - val_loss: 1.1301 - val_mae: 1.1301\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5219 - mae: 1.5219 - val_loss: 0.9387 - val_mae: 0.9387\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.8582 - mae: 1.8582 - val_loss: 1.2607 - val_mae: 1.2607\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.5846 - mae: 1.5846 - val_loss: 0.9840 - val_mae: 0.9840\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.9840 - mae: 0.9840\n",
      "Val score is 0.9839849472045898\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 38.5043 - mae: 38.5043 - val_loss: 37.5743 - val_mae: 37.5743\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 38.3712 - mae: 38.3712 - val_loss: 37.4880 - val_mae: 37.4880\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 38.2181 - mae: 38.2181 - val_loss: 37.3724 - val_mae: 37.3724\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 38.0405 - mae: 38.0405 - val_loss: 37.2300 - val_mae: 37.2300\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 37.8350 - mae: 37.8350 - val_loss: 37.0552 - val_mae: 37.0552\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 37.5993 - mae: 37.5993 - val_loss: 36.8559 - val_mae: 36.8559\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 37.3316 - mae: 37.3316 - val_loss: 36.6145 - val_mae: 36.6145\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 37.0306 - mae: 37.0306 - val_loss: 36.3460 - val_mae: 36.3460\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 36.6953 - mae: 36.6953 - val_loss: 36.0324 - val_mae: 36.0324\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 36.3249 - mae: 36.3249 - val_loss: 35.6880 - val_mae: 35.6880\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 35.9189 - mae: 35.9189 - val_loss: 35.3024 - val_mae: 35.3024\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 35.4768 - mae: 35.4768 - val_loss: 34.8824 - val_mae: 34.8824\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 34.9981 - mae: 34.9981 - val_loss: 34.4157 - val_mae: 34.4157\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 34.4826 - mae: 34.4826 - val_loss: 33.9119 - val_mae: 33.9119\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 33.9301 - mae: 33.9301 - val_loss: 33.3730 - val_mae: 33.3730\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 33.3403 - mae: 33.3403 - val_loss: 32.7877 - val_mae: 32.7877\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 32.7132 - mae: 32.7132 - val_loss: 32.1606 - val_mae: 32.1606\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 32.0486 - mae: 32.0486 - val_loss: 31.5001 - val_mae: 31.5001\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 31.3465 - mae: 31.3465 - val_loss: 30.8088 - val_mae: 30.8088\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 30.6068 - mae: 30.6068 - val_loss: 30.0787 - val_mae: 30.0787\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 29.8296 - mae: 29.8296 - val_loss: 29.3234 - val_mae: 29.3234\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 29.0213 - mae: 29.0213 - val_loss: 28.4219 - val_mae: 28.4219\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 28.1657 - mae: 28.1656 - val_loss: 27.2714 - val_mae: 27.2714\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 27.2780 - mae: 27.2780 - val_loss: 26.3062 - val_mae: 26.3062\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 26.3513 - mae: 26.3513 - val_loss: 25.3807 - val_mae: 25.3807\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 25.3869 - mae: 25.3869 - val_loss: 24.4370 - val_mae: 24.4370\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 24.3851 - mae: 24.3851 - val_loss: 23.4755 - val_mae: 23.4755\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 23.3461 - mae: 23.3461 - val_loss: 22.4667 - val_mae: 22.4667\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 22.2700 - mae: 22.2700 - val_loss: 21.4271 - val_mae: 21.4271\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 21.1570 - mae: 21.1570 - val_loss: 20.3310 - val_mae: 20.3310\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 20.0071 - mae: 20.0071 - val_loss: 19.2067 - val_mae: 19.2067\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 18.8204 - mae: 18.8204 - val_loss: 18.0518 - val_mae: 18.0518\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 17.5983 - mae: 17.5983 - val_loss: 15.8618 - val_mae: 15.8618\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 16.3729 - mae: 16.3729 - val_loss: 14.1010 - val_mae: 14.1010\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 15.0871 - mae: 15.0871 - val_loss: 12.7197 - val_mae: 12.7197\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 13.7637 - mae: 13.7637 - val_loss: 11.2151 - val_mae: 11.2151\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 12.4276 - mae: 12.4276 - val_loss: 9.8674 - val_mae: 9.8674\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 11.0457 - mae: 11.0457 - val_loss: 8.6445 - val_mae: 8.6445\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 9.6475 - mae: 9.6475 - val_loss: 7.2552 - val_mae: 7.2552\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 8.2765 - mae: 8.2765 - val_loss: 5.6339 - val_mae: 5.6339\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 7.0218 - mae: 7.0218 - val_loss: 4.2805 - val_mae: 4.2805\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 5.6028 - mae: 5.6028 - val_loss: 3.1726 - val_mae: 3.1726\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.4021 - mae: 4.4021 - val_loss: 2.6372 - val_mae: 2.6372\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.4373 - mae: 3.4373 - val_loss: 2.5289 - val_mae: 2.5289\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 2.7370 - mae: 2.7370 - val_loss: 2.3455 - val_mae: 2.3455\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.2913 - mae: 2.2913 - val_loss: 2.3247 - val_mae: 2.3247\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 2.0747 - mae: 2.0747 - val_loss: 2.1962 - val_mae: 2.1962\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.8279 - mae: 1.8279 - val_loss: 2.2446 - val_mae: 2.2446\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.9903 - mae: 1.9903 - val_loss: 1.9011 - val_mae: 1.9011\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 112us/sample - loss: 1.7981 - mae: 1.7981 - val_loss: 1.7561 - val_mae: 1.7561\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.7211 - mae: 1.7211 - val_loss: 1.6652 - val_mae: 1.6652\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.5818 - mae: 1.5818 - val_loss: 1.5146 - val_mae: 1.5146\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.5414 - mae: 1.5414 - val_loss: 1.3863 - val_mae: 1.3863\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4553 - mae: 1.4553 - val_loss: 1.3809 - val_mae: 1.3809\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.4863 - mae: 1.4863 - val_loss: 1.2821 - val_mae: 1.2821\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4337 - mae: 1.4337 - val_loss: 1.1559 - val_mae: 1.1559\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.4238 - mae: 1.4238 - val_loss: 1.4623 - val_mae: 1.4623\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4050 - mae: 1.4050 - val_loss: 1.0975 - val_mae: 1.0975\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.3465 - mae: 1.3465 - val_loss: 1.0589 - val_mae: 1.0589\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.3086 - mae: 1.3086 - val_loss: 1.0516 - val_mae: 1.0516\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.2788 - mae: 1.2788 - val_loss: 1.0008 - val_mae: 1.0008\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.3808 - mae: 1.3808 - val_loss: 1.0628 - val_mae: 1.0628\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.3459 - mae: 1.3459 - val_loss: 0.9608 - val_mae: 0.9608\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.2858 - mae: 1.2858 - val_loss: 0.9157 - val_mae: 0.9157\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.3580 - mae: 1.3580 - val_loss: 0.8713 - val_mae: 0.8713\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.1551 - mae: 1.1551 - val_loss: 0.8589 - val_mae: 0.8589\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.1859 - mae: 1.1859 - val_loss: 0.8706 - val_mae: 0.8706\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.2158 - mae: 1.2158 - val_loss: 0.7538 - val_mae: 0.7538\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.2547 - mae: 1.2547 - val_loss: 0.7433 - val_mae: 0.7433\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.2055 - mae: 1.2055 - val_loss: 0.7785 - val_mae: 0.7785\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.1661 - mae: 1.1661 - val_loss: 0.7779 - val_mae: 0.7779\n",
      "365/365 [==============================] - 0s 37us/sample - loss: 0.7779 - mae: 0.7779\n",
      "Val score is 0.7779451012611389\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 2.0975 - mae: 2.0975 - val_loss: 1.6084 - val_mae: 1.6084\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.8325 - mae: 1.8325 - val_loss: 1.5807 - val_mae: 1.5807\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.7194 - mae: 1.7194 - val_loss: 1.5878 - val_mae: 1.5878\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.6887 - mae: 1.6887 - val_loss: 1.6058 - val_mae: 1.6058\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 1.6058 - mae: 1.6058\n",
      "Val score is 1.6058334112167358\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 0.8933 - mae: 0.8933 - val_loss: 0.2016 - val_mae: 0.2016\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.6385 - mae: 0.6385 - val_loss: 0.1916 - val_mae: 0.1916\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.5454 - mae: 0.5454 - val_loss: 0.1785 - val_mae: 0.1785\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 153us/sample - loss: 0.4705 - mae: 0.4705 - val_loss: 0.1795 - val_mae: 0.1795\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 0.4088 - mae: 0.4088 - val_loss: 0.1653 - val_mae: 0.1653\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 0.3813 - mae: 0.3813 - val_loss: 0.1661 - val_mae: 0.1661\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.3703 - mae: 0.3703 - val_loss: 0.1686 - val_mae: 0.1686\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 0.1686 - mae: 0.1686\n",
      "Val score is 0.1685694307088852\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 2.3977 - mae: 2.3977 - val_loss: 2.4355 - val_mae: 2.4355\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.2699 - mae: 2.2699 - val_loss: 2.2775 - val_mae: 2.2775\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 2.1547 - mae: 2.1547 - val_loss: 2.1365 - val_mae: 2.1365\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.0415 - mae: 2.0415 - val_loss: 1.9759 - val_mae: 1.9759\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.9076 - mae: 1.9076 - val_loss: 1.7934 - val_mae: 1.7934\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.7762 - mae: 1.7762 - val_loss: 1.6171 - val_mae: 1.6171\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.6163 - mae: 1.6163 - val_loss: 1.4209 - val_mae: 1.4209\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4242 - mae: 1.4242 - val_loss: 1.2003 - val_mae: 1.2003\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.2307 - mae: 1.2307 - val_loss: 1.0009 - val_mae: 1.0009\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.0714 - mae: 1.0714 - val_loss: 0.8172 - val_mae: 0.8172\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 0.9028 - mae: 0.9028 - val_loss: 0.6273 - val_mae: 0.6273\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 0.7601 - mae: 0.7601 - val_loss: 0.4697 - val_mae: 0.4697\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 0.6490 - mae: 0.6490 - val_loss: 0.3830 - val_mae: 0.3830\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.5730 - mae: 0.5730 - val_loss: 0.3410 - val_mae: 0.3410\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 0.5056 - mae: 0.5056 - val_loss: 0.3327 - val_mae: 0.3327\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 0.4248 - mae: 0.4248 - val_loss: 0.3394 - val_mae: 0.3394\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 0.4203 - mae: 0.4203 - val_loss: 0.3388 - val_mae: 0.3388\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 0.3388 - mae: 0.3388\n",
      "Val score is 0.33877992630004883\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 1.4509 - mae: 1.4509 - val_loss: 0.7082 - val_mae: 0.7082\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1.0939 - mae: 1.0939 - val_loss: 0.7398 - val_mae: 0.7398\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 0.8804 - mae: 0.8804 - val_loss: 0.7255 - val_mae: 0.7255\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 0.7255 - mae: 0.7255\n",
      "Val score is 0.7254796028137207\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 34.4699 - mae: 34.4699 - val_loss: 34.5527 - val_mae: 34.5527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 34.3558 - mae: 34.3558 - val_loss: 34.3833 - val_mae: 34.3833\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 34.2215 - mae: 34.2215 - val_loss: 34.1905 - val_mae: 34.1906\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 34.0624 - mae: 34.0624 - val_loss: 33.9711 - val_mae: 33.9711\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 33.8750 - mae: 33.8750 - val_loss: 33.7177 - val_mae: 33.7177\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 33.6567 - mae: 33.6567 - val_loss: 33.4387 - val_mae: 33.4387\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 33.4058 - mae: 33.4058 - val_loss: 33.1303 - val_mae: 33.1303\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 33.1209 - mae: 33.1209 - val_loss: 32.7887 - val_mae: 32.7887\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 32.8009 - mae: 32.8009 - val_loss: 32.4156 - val_mae: 32.4156\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 32.4452 - mae: 32.4452 - val_loss: 32.0152 - val_mae: 32.0152\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 32.0530 - mae: 32.0530 - val_loss: 31.5772 - val_mae: 31.5772\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 31.6240 - mae: 31.6240 - val_loss: 31.1046 - val_mae: 31.1046\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 31.1578 - mae: 31.1578 - val_loss: 30.5965 - val_mae: 30.5965\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 30.6542 - mae: 30.6542 - val_loss: 30.0557 - val_mae: 30.0557\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 30.1128 - mae: 30.1128 - val_loss: 29.4820 - val_mae: 29.4820\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 29.5337 - mae: 29.5337 - val_loss: 28.8779 - val_mae: 28.8779\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 28.9166 - mae: 28.9166 - val_loss: 28.2356 - val_mae: 28.2356\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 28.2616 - mae: 28.2616 - val_loss: 27.5660 - val_mae: 27.5660\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 27.5686 - mae: 27.5686 - val_loss: 26.8649 - val_mae: 26.8649\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 26.8376 - mae: 26.8376 - val_loss: 26.1324 - val_mae: 26.1324\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 26.0686 - mae: 26.0686 - val_loss: 25.3604 - val_mae: 25.3604\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 25.2617 - mae: 25.2617 - val_loss: 24.5412 - val_mae: 24.5412\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 24.4169 - mae: 24.4169 - val_loss: 23.6798 - val_mae: 23.6798\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 23.5343 - mae: 23.5343 - val_loss: 22.7946 - val_mae: 22.7946\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 22.6140 - mae: 22.6140 - val_loss: 21.9053 - val_mae: 21.9053\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 21.6561 - mae: 21.6561 - val_loss: 20.9498 - val_mae: 20.9498\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 20.6607 - mae: 20.6607 - val_loss: 19.9636 - val_mae: 19.9636\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 19.6278 - mae: 19.6278 - val_loss: 18.9400 - val_mae: 18.9400\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 18.5577 - mae: 18.5577 - val_loss: 17.8492 - val_mae: 17.8492\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 17.4503 - mae: 17.4503 - val_loss: 16.7178 - val_mae: 16.7178\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 16.3059 - mae: 16.3059 - val_loss: 15.5509 - val_mae: 15.5509\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 15.1365 - mae: 15.1365 - val_loss: 14.2167 - val_mae: 14.2167\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 13.9295 - mae: 13.9295 - val_loss: 12.5175 - val_mae: 12.5175\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 12.7444 - mae: 12.7444 - val_loss: 10.8403 - val_mae: 10.8403\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 11.4526 - mae: 11.4526 - val_loss: 9.5255 - val_mae: 9.5255\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 10.1870 - mae: 10.1870 - val_loss: 8.5126 - val_mae: 8.5126\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 8.8263 - mae: 8.8263 - val_loss: 7.2331 - val_mae: 7.2331\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 7.5260 - mae: 7.5260 - val_loss: 6.0788 - val_mae: 6.0788\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 6.2654 - mae: 6.2654 - val_loss: 5.0116 - val_mae: 5.0116\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 5.1279 - mae: 5.1279 - val_loss: 4.3967 - val_mae: 4.3967\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 4.2466 - mae: 4.2466 - val_loss: 3.5409 - val_mae: 3.5409\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 3.4518 - mae: 3.4518 - val_loss: 2.9056 - val_mae: 2.9056\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.8981 - mae: 2.8981 - val_loss: 2.5154 - val_mae: 2.5154\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 2.6059 - mae: 2.6059 - val_loss: 2.2360 - val_mae: 2.2360\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.2710 - mae: 2.2710 - val_loss: 1.9784 - val_mae: 1.9784\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 2.3115 - mae: 2.3115 - val_loss: 1.9071 - val_mae: 1.9071\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 2.1233 - mae: 2.1233 - val_loss: 1.7071 - val_mae: 1.7071\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.9292 - mae: 1.9292 - val_loss: 1.6234 - val_mae: 1.6234\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 1.9810 - mae: 1.9810 - val_loss: 1.4934 - val_mae: 1.4934\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.7973 - mae: 1.7973 - val_loss: 1.3763 - val_mae: 1.3763\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.7961 - mae: 1.7961 - val_loss: 1.4813 - val_mae: 1.4813\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.8072 - mae: 1.8072 - val_loss: 1.3127 - val_mae: 1.3127\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.8310 - mae: 1.8310 - val_loss: 1.3076 - val_mae: 1.3076\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.6945 - mae: 1.6945 - val_loss: 1.2155 - val_mae: 1.2155\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 1.7045 - mae: 1.7045 - val_loss: 1.3038 - val_mae: 1.3038\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.7869 - mae: 1.7869 - val_loss: 1.2729 - val_mae: 1.2729\n",
      "365/365 [==============================] - 0s 36us/sample - loss: 1.2729 - mae: 1.2729\n",
      "Val score is 1.2728583812713623\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 38.9734 - mae: 38.9734 - val_loss: 38.7186 - val_mae: 38.7186\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 38.8640 - mae: 38.8640 - val_loss: 38.6329 - val_mae: 38.6329\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 120us/sample - loss: 38.7344 - mae: 38.7344 - val_loss: 38.5237 - val_mae: 38.5237\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 38.5801 - mae: 38.5801 - val_loss: 38.3872 - val_mae: 38.3872\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 38.3975 - mae: 38.3975 - val_loss: 38.2198 - val_mae: 38.2198\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 38.1840 - mae: 38.1840 - val_loss: 38.0214 - val_mae: 38.0214\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 37.9377 - mae: 37.9377 - val_loss: 37.7841 - val_mae: 37.7841\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 37.6574 - mae: 37.6574 - val_loss: 37.5091 - val_mae: 37.5091\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 37.3418 - mae: 37.3418 - val_loss: 37.1942 - val_mae: 37.1942\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 36.9903 - mae: 36.9903 - val_loss: 36.8431 - val_mae: 36.8431\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 36.6023 - mae: 36.6023 - val_loss: 36.4604 - val_mae: 36.4604\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 36.1772 - mae: 36.1772 - val_loss: 36.0311 - val_mae: 36.0311\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 35.7148 - mae: 35.7148 - val_loss: 35.5649 - val_mae: 35.5649\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 35.2147 - mae: 35.2147 - val_loss: 35.0642 - val_mae: 35.0642\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 34.6768 - mae: 34.6768 - val_loss: 34.5159 - val_mae: 34.5159\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 34.1009 - mae: 34.1009 - val_loss: 33.9353 - val_mae: 33.9352\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 33.4870 - mae: 33.4870 - val_loss: 33.3027 - val_mae: 33.3027\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 32.8349 - mae: 32.8349 - val_loss: 32.6377 - val_mae: 32.6377\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 32.1448 - mae: 32.1448 - val_loss: 31.9190 - val_mae: 31.9190\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 31.4165 - mae: 31.4165 - val_loss: 31.1584 - val_mae: 31.1584\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 30.6501 - mae: 30.6501 - val_loss: 30.3620 - val_mae: 30.3620\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 29.8456 - mae: 29.8456 - val_loss: 29.5450 - val_mae: 29.5450\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 29.0032 - mae: 29.0032 - val_loss: 28.6550 - val_mae: 28.6550\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 28.1229 - mae: 28.1229 - val_loss: 27.7240 - val_mae: 27.7240\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 27.2048 - mae: 27.2048 - val_loss: 26.7536 - val_mae: 26.7536\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 26.2490 - mae: 26.2490 - val_loss: 25.7637 - val_mae: 25.7637\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 25.2555 - mae: 25.2555 - val_loss: 24.7260 - val_mae: 24.7260\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 24.2246 - mae: 24.2246 - val_loss: 23.6807 - val_mae: 23.6807\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 23.1563 - mae: 23.1563 - val_loss: 22.6163 - val_mae: 22.6163\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 22.0507 - mae: 22.0507 - val_loss: 21.5068 - val_mae: 21.5068\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 20.9080 - mae: 20.9080 - val_loss: 20.3347 - val_mae: 20.3347\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 19.7284 - mae: 19.7284 - val_loss: 19.1284 - val_mae: 19.1284\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 18.5147 - mae: 18.5147 - val_loss: 17.3092 - val_mae: 17.3092\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 17.2651 - mae: 17.2651 - val_loss: 15.8945 - val_mae: 15.8945\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 15.9945 - mae: 15.9945 - val_loss: 14.5193 - val_mae: 14.5193\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 14.6660 - mae: 14.6660 - val_loss: 13.1499 - val_mae: 13.1499\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 13.3157 - mae: 13.3157 - val_loss: 11.8263 - val_mae: 11.8263\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 11.9542 - mae: 11.9542 - val_loss: 10.2333 - val_mae: 10.2333\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 10.6136 - mae: 10.6136 - val_loss: 8.4846 - val_mae: 8.4846\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 9.1565 - mae: 9.1565 - val_loss: 7.0487 - val_mae: 7.0487\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 7.7353 - mae: 7.7353 - val_loss: 5.7715 - val_mae: 5.7715\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 6.2703 - mae: 6.2703 - val_loss: 4.7753 - val_mae: 4.7753\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.9503 - mae: 4.9503 - val_loss: 3.6428 - val_mae: 3.6428\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 3.6832 - mae: 3.6832 - val_loss: 2.5826 - val_mae: 2.5826\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.7964 - mae: 2.7964 - val_loss: 2.1887 - val_mae: 2.1887\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 2.2656 - mae: 2.2656 - val_loss: 1.6952 - val_mae: 1.6952\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.9764 - mae: 1.9764 - val_loss: 1.7842 - val_mae: 1.7842\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 1.7086 - mae: 1.7086 - val_loss: 1.6439 - val_mae: 1.6439\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 1.6272 - mae: 1.6272 - val_loss: 1.5806 - val_mae: 1.5806\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5906 - mae: 1.5906 - val_loss: 1.5028 - val_mae: 1.5028\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.5924 - mae: 1.5924 - val_loss: 1.2133 - val_mae: 1.2133\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.4894 - mae: 1.4894 - val_loss: 1.1371 - val_mae: 1.1371\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 1.4586 - mae: 1.4586 - val_loss: 1.0583 - val_mae: 1.0583\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.3001 - mae: 1.3001 - val_loss: 1.0579 - val_mae: 1.0579\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.3867 - mae: 1.3867 - val_loss: 1.0073 - val_mae: 1.0073\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.3316 - mae: 1.3316 - val_loss: 1.1290 - val_mae: 1.1290\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.3539 - mae: 1.3539 - val_loss: 1.0269 - val_mae: 1.0269\n",
      "365/365 [==============================] - 0s 35us/sample - loss: 1.0269 - mae: 1.0269\n",
      "Val score is 1.0269224643707275\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 3ms/sample - loss: 102.8496 - mae: 102.8496 - val_loss: 102.7941 - val_mae: 102.7941\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 102.6920 - mae: 102.6920 - val_loss: 102.6141 - val_mae: 102.6141\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 117us/sample - loss: 102.5144 - mae: 102.5144 - val_loss: 102.4130 - val_mae: 102.4130\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 102.3127 - mae: 102.3127 - val_loss: 102.1886 - val_mae: 102.1886\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 102.0836 - mae: 102.0836 - val_loss: 101.9353 - val_mae: 101.9353\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 101.8249 - mae: 101.8249 - val_loss: 101.6529 - val_mae: 101.6529\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 101.5350 - mae: 101.5350 - val_loss: 101.3386 - val_mae: 101.3386\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 101.2127 - mae: 101.2127 - val_loss: 100.9950 - val_mae: 100.9950\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 100.8569 - mae: 100.8569 - val_loss: 100.6156 - val_mae: 100.6156\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 100.4671 - mae: 100.4671 - val_loss: 100.2077 - val_mae: 100.2076\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 100.0424 - mae: 100.0424 - val_loss: 99.7656 - val_mae: 99.7656\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 99.5825 - mae: 99.5825 - val_loss: 99.2820 - val_mae: 99.2820\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 99.0869 - mae: 99.0869 - val_loss: 98.7529 - val_mae: 98.7529\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 98.5553 - mae: 98.5553 - val_loss: 98.2002 - val_mae: 98.2002\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 97.9874 - mae: 97.9874 - val_loss: 97.6101 - val_mae: 97.6101\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 97.3829 - mae: 97.3829 - val_loss: 96.9907 - val_mae: 96.9907\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 96.7418 - mae: 96.7418 - val_loss: 96.3418 - val_mae: 96.3418\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 96.0639 - mae: 96.0639 - val_loss: 95.6503 - val_mae: 95.6503\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 95.3490 - mae: 95.3490 - val_loss: 94.9169 - val_mae: 94.9168\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 94.5972 - mae: 94.5972 - val_loss: 94.1482 - val_mae: 94.1482\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 93.8083 - mae: 93.8083 - val_loss: 93.3302 - val_mae: 93.3302\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 92.9824 - mae: 92.9824 - val_loss: 92.4771 - val_mae: 92.4771\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 92.1194 - mae: 92.1194 - val_loss: 91.5960 - val_mae: 91.5960\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 91.2194 - mae: 91.2194 - val_loss: 90.6782 - val_mae: 90.6782\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 90.2824 - mae: 90.2824 - val_loss: 89.7243 - val_mae: 89.7243\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 89.3085 - mae: 89.3085 - val_loss: 88.7281 - val_mae: 88.7281\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 88.2976 - mae: 88.2976 - val_loss: 87.6797 - val_mae: 87.6797\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 87.2500 - mae: 87.2500 - val_loss: 86.5985 - val_mae: 86.5985\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 86.1655 - mae: 86.1655 - val_loss: 85.4729 - val_mae: 85.4729\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 85.0444 - mae: 85.0444 - val_loss: 84.3317 - val_mae: 84.3317\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 83.8868 - mae: 83.8868 - val_loss: 83.1452 - val_mae: 83.1452\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 82.6926 - mae: 82.6926 - val_loss: 81.9390 - val_mae: 81.9390\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 81.4621 - mae: 81.4621 - val_loss: 80.6950 - val_mae: 80.6950\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 80.1953 - mae: 80.1953 - val_loss: 79.4255 - val_mae: 79.4255\n",
      "Epoch 35/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 78.8924 - mae: 78.8924 - val_loss: 78.0819 - val_mae: 78.0819\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 77.5534 - mae: 77.5534 - val_loss: 76.7155 - val_mae: 76.7155\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 76.1785 - mae: 76.1785 - val_loss: 75.3314 - val_mae: 75.3314\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 74.7678 - mae: 74.7678 - val_loss: 73.8840 - val_mae: 73.8840\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 73.3214 - mae: 73.3214 - val_loss: 72.4349 - val_mae: 72.4349\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 71.8395 - mae: 71.8395 - val_loss: 70.9119 - val_mae: 70.9119\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 70.3222 - mae: 70.3222 - val_loss: 69.3731 - val_mae: 69.3731\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 68.7696 - mae: 68.7696 - val_loss: 67.8077 - val_mae: 67.8077\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 67.1818 - mae: 67.1818 - val_loss: 66.2323 - val_mae: 66.2323\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 65.5590 - mae: 65.5590 - val_loss: 64.6009 - val_mae: 64.6009\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 63.9014 - mae: 63.9014 - val_loss: 62.8857 - val_mae: 62.8857\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 62.2090 - mae: 62.2090 - val_loss: 61.1911 - val_mae: 61.1911\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 60.4819 - mae: 60.4819 - val_loss: 59.4403 - val_mae: 59.4403\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 58.7204 - mae: 58.7204 - val_loss: 57.6540 - val_mae: 57.6540\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 56.9246 - mae: 56.9245 - val_loss: 55.8309 - val_mae: 55.8309\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 55.0945 - mae: 55.0945 - val_loss: 53.9991 - val_mae: 53.9991\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 53.2304 - mae: 53.2304 - val_loss: 52.1265 - val_mae: 52.1265\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 51.3323 - mae: 51.3323 - val_loss: 50.2357 - val_mae: 50.2357\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 49.4004 - mae: 49.4004 - val_loss: 48.2390 - val_mae: 48.2390\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 47.4349 - mae: 47.4349 - val_loss: 46.2437 - val_mae: 46.2437\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 45.4358 - mae: 45.4358 - val_loss: 44.2338 - val_mae: 44.2338\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 43.4034 - mae: 43.4034 - val_loss: 42.2142 - val_mae: 42.2142\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 41.3377 - mae: 41.3377 - val_loss: 40.1435 - val_mae: 40.1435\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 39.2389 - mae: 39.2389 - val_loss: 38.0586 - val_mae: 38.0586\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 37.1071 - mae: 37.1071 - val_loss: 35.8952 - val_mae: 35.8952\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 117us/sample - loss: 34.9425 - mae: 34.9425 - val_loss: 33.6875 - val_mae: 33.6875\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 32.7451 - mae: 32.7451 - val_loss: 31.5076 - val_mae: 31.5076\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 30.5152 - mae: 30.5152 - val_loss: 29.2146 - val_mae: 29.2146\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 126us/sample - loss: 28.2528 - mae: 28.2528 - val_loss: 26.8837 - val_mae: 26.8837\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 25.9581 - mae: 25.9581 - val_loss: 24.5728 - val_mae: 24.5728\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 23.6312 - mae: 23.6312 - val_loss: 22.2093 - val_mae: 22.2093\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 21.3615 - mae: 21.3615 - val_loss: 19.5385 - val_mae: 19.5385\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 19.2561 - mae: 19.2561 - val_loss: 15.8937 - val_mae: 15.8937\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 16.7978 - mae: 16.7978 - val_loss: 13.6284 - val_mae: 13.6284\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 14.4265 - mae: 14.4265 - val_loss: 11.2886 - val_mae: 11.2886\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 12.2415 - mae: 12.2415 - val_loss: 8.6039 - val_mae: 8.6039\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 124us/sample - loss: 9.8195 - mae: 9.8195 - val_loss: 6.6697 - val_mae: 6.6697\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 7.6989 - mae: 7.6989 - val_loss: 5.3816 - val_mae: 5.3816\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 5.9297 - mae: 5.9297 - val_loss: 4.1365 - val_mae: 4.1365\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 4.7793 - mae: 4.7793 - val_loss: 3.6762 - val_mae: 3.6762\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 4.0177 - mae: 4.0177 - val_loss: 3.1260 - val_mae: 3.1260\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 3.3031 - mae: 3.3031 - val_loss: 3.0513 - val_mae: 3.0513\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 3.2491 - mae: 3.2491 - val_loss: 2.7538 - val_mae: 2.7538\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 3.0161 - mae: 3.0161 - val_loss: 2.7768 - val_mae: 2.7768\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 3.0726 - mae: 3.0726 - val_loss: 2.5344 - val_mae: 2.5344\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 2.7349 - mae: 2.7349 - val_loss: 2.1330 - val_mae: 2.1330\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 2.7259 - mae: 2.7259 - val_loss: 2.2077 - val_mae: 2.2077\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 2.4681 - mae: 2.4681 - val_loss: 2.1839 - val_mae: 2.1839\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 2.1839 - mae: 2.1839\n",
      "Val score is 2.1839025020599365\n",
      "Train on 365 samples, validate on 365 samples\n",
      "Epoch 1/100\n",
      "365/365 [==============================] - 1s 2ms/sample - loss: 99.5649 - mae: 99.5649 - val_loss: 99.6737 - val_mae: 99.6737\n",
      "Epoch 2/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 99.4360 - mae: 99.4360 - val_loss: 99.4926 - val_mae: 99.4926\n",
      "Epoch 3/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 99.2870 - mae: 99.2870 - val_loss: 99.2882 - val_mae: 99.2882\n",
      "Epoch 4/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 99.1135 - mae: 99.1135 - val_loss: 99.0554 - val_mae: 99.0554\n",
      "Epoch 5/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 98.9120 - mae: 98.9120 - val_loss: 98.7955 - val_mae: 98.7955\n",
      "Epoch 6/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 98.6802 - mae: 98.6802 - val_loss: 98.5017 - val_mae: 98.5017\n",
      "Epoch 7/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 98.4162 - mae: 98.4162 - val_loss: 98.1819 - val_mae: 98.1819\n",
      "Epoch 8/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 98.1187 - mae: 98.1187 - val_loss: 97.8319 - val_mae: 97.8319\n",
      "Epoch 9/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 97.7867 - mae: 97.7867 - val_loss: 97.4452 - val_mae: 97.4452\n",
      "Epoch 10/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 97.4195 - mae: 97.4195 - val_loss: 97.0320 - val_mae: 97.0321\n",
      "Epoch 11/100\n",
      "365/365 [==============================] - 0s 119us/sample - loss: 97.0164 - mae: 97.0163 - val_loss: 96.5803 - val_mae: 96.5803\n",
      "Epoch 12/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 96.5769 - mae: 96.5769 - val_loss: 96.0905 - val_mae: 96.0905\n",
      "Epoch 13/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 96.1008 - mae: 96.1007 - val_loss: 95.5787 - val_mae: 95.5787\n",
      "Epoch 14/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 95.5876 - mae: 95.5876 - val_loss: 95.0332 - val_mae: 95.0332\n",
      "Epoch 15/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 95.0372 - mae: 95.0372 - val_loss: 94.4497 - val_mae: 94.4497\n",
      "Epoch 16/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 94.4494 - mae: 94.4494 - val_loss: 93.8261 - val_mae: 93.8261\n",
      "Epoch 17/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 93.8240 - mae: 93.8241 - val_loss: 93.1698 - val_mae: 93.1698\n",
      "Epoch 18/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 93.1611 - mae: 93.1611 - val_loss: 92.4879 - val_mae: 92.4879\n",
      "Epoch 19/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 92.4605 - mae: 92.4606 - val_loss: 91.7615 - val_mae: 91.7615\n",
      "Epoch 20/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 91.7223 - mae: 91.7223 - val_loss: 91.0144 - val_mae: 91.0144\n",
      "Epoch 21/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 90.9464 - mae: 90.9464 - val_loss: 90.2201 - val_mae: 90.2202\n",
      "Epoch 22/100\n",
      "365/365 [==============================] - 0s 123us/sample - loss: 90.1328 - mae: 90.1328 - val_loss: 89.4003 - val_mae: 89.4003\n",
      "Epoch 23/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 89.2816 - mae: 89.2816 - val_loss: 88.5272 - val_mae: 88.5272\n",
      "Epoch 24/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 88.3929 - mae: 88.3929 - val_loss: 87.6097 - val_mae: 87.6097\n",
      "Epoch 25/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 87.4667 - mae: 87.4667 - val_loss: 86.6559 - val_mae: 86.6559\n",
      "Epoch 26/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 86.5030 - mae: 86.5030 - val_loss: 85.6746 - val_mae: 85.6746\n",
      "Epoch 27/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 85.5021 - mae: 85.5021 - val_loss: 84.6492 - val_mae: 84.6492\n",
      "Epoch 28/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 84.4639 - mae: 84.4639 - val_loss: 83.5951 - val_mae: 83.5951\n",
      "Epoch 29/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 83.3886 - mae: 83.3886 - val_loss: 82.5371 - val_mae: 82.5371\n",
      "Epoch 30/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 82.2763 - mae: 82.2763 - val_loss: 81.4182 - val_mae: 81.4182\n",
      "Epoch 31/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 81.1271 - mae: 81.1271 - val_loss: 80.2914 - val_mae: 80.2914\n",
      "Epoch 32/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 79.9411 - mae: 79.9411 - val_loss: 79.1518 - val_mae: 79.1518\n",
      "Epoch 33/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 78.7185 - mae: 78.7185 - val_loss: 77.9273 - val_mae: 77.9273\n",
      "Epoch 34/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 77.4593 - mae: 77.4593 - val_loss: 76.6826 - val_mae: 76.6827\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 114us/sample - loss: 76.1638 - mae: 76.1638 - val_loss: 75.3608 - val_mae: 75.3608\n",
      "Epoch 36/100\n",
      "365/365 [==============================] - 0s 108us/sample - loss: 74.8319 - mae: 74.8319 - val_loss: 74.0274 - val_mae: 74.0274\n",
      "Epoch 37/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 73.4640 - mae: 73.4640 - val_loss: 72.6628 - val_mae: 72.6627\n",
      "Epoch 38/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 72.0601 - mae: 72.0601 - val_loss: 71.2382 - val_mae: 71.2382\n",
      "Epoch 39/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 70.6203 - mae: 70.6203 - val_loss: 69.7776 - val_mae: 69.7776\n",
      "Epoch 40/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 69.1448 - mae: 69.1448 - val_loss: 68.2660 - val_mae: 68.2660\n",
      "Epoch 41/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 67.6337 - mae: 67.6337 - val_loss: 66.7209 - val_mae: 66.7209\n",
      "Epoch 42/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 66.0872 - mae: 66.0872 - val_loss: 65.2091 - val_mae: 65.2091\n",
      "Epoch 43/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 64.5054 - mae: 64.5054 - val_loss: 63.6197 - val_mae: 63.6197\n",
      "Epoch 44/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 62.8885 - mae: 62.8885 - val_loss: 61.9963 - val_mae: 61.9963\n",
      "Epoch 45/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 61.2366 - mae: 61.2366 - val_loss: 60.3265 - val_mae: 60.3265\n",
      "Epoch 46/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 59.5498 - mae: 59.5498 - val_loss: 58.6601 - val_mae: 58.6601\n",
      "Epoch 47/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 57.8283 - mae: 57.8283 - val_loss: 56.9576 - val_mae: 56.9576\n",
      "Epoch 48/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 56.0722 - mae: 56.0722 - val_loss: 55.1544 - val_mae: 55.1544\n",
      "Epoch 49/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 54.2817 - mae: 54.2817 - val_loss: 53.3481 - val_mae: 53.3481\n",
      "Epoch 50/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 52.4569 - mae: 52.4569 - val_loss: 51.5260 - val_mae: 51.5260\n",
      "Epoch 51/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 50.5979 - mae: 50.5979 - val_loss: 49.5964 - val_mae: 49.5964\n",
      "Epoch 52/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 48.7050 - mae: 48.7050 - val_loss: 47.6739 - val_mae: 47.6739\n",
      "Epoch 53/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 46.7782 - mae: 46.7782 - val_loss: 45.7647 - val_mae: 45.7647\n",
      "Epoch 54/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 44.8177 - mae: 44.8177 - val_loss: 43.7937 - val_mae: 43.7937\n",
      "Epoch 55/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 42.8236 - mae: 42.8236 - val_loss: 41.8429 - val_mae: 41.8429\n",
      "Epoch 56/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 40.7960 - mae: 40.7960 - val_loss: 39.8021 - val_mae: 39.8020\n",
      "Epoch 57/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 38.7352 - mae: 38.7352 - val_loss: 37.7631 - val_mae: 37.7631\n",
      "Epoch 58/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 36.6412 - mae: 36.6412 - val_loss: 35.6535 - val_mae: 35.6535\n",
      "Epoch 59/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 34.5142 - mae: 34.5142 - val_loss: 33.5240 - val_mae: 33.5240\n",
      "Epoch 60/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 32.3544 - mae: 32.3544 - val_loss: 31.2892 - val_mae: 31.2892\n",
      "Epoch 61/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 30.1617 - mae: 30.1617 - val_loss: 29.1282 - val_mae: 29.1282\n",
      "Epoch 62/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 27.9365 - mae: 27.9365 - val_loss: 26.9134 - val_mae: 26.9134\n",
      "Epoch 63/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 25.6861 - mae: 25.6861 - val_loss: 24.1519 - val_mae: 24.1519\n",
      "Epoch 64/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 23.4510 - mae: 23.4510 - val_loss: 20.6744 - val_mae: 20.6744\n",
      "Epoch 65/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 21.1488 - mae: 21.1488 - val_loss: 18.3893 - val_mae: 18.3893\n",
      "Epoch 66/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 18.8482 - mae: 18.8482 - val_loss: 15.1606 - val_mae: 15.1606\n",
      "Epoch 67/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 16.4954 - mae: 16.4954 - val_loss: 11.9014 - val_mae: 11.9014\n",
      "Epoch 68/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 14.1285 - mae: 14.1285 - val_loss: 9.5394 - val_mae: 9.5394\n",
      "Epoch 69/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 11.7494 - mae: 11.7494 - val_loss: 7.7630 - val_mae: 7.7630\n",
      "Epoch 70/100\n",
      "365/365 [==============================] - 0s 117us/sample - loss: 9.4606 - mae: 9.4606 - val_loss: 6.8918 - val_mae: 6.8918\n",
      "Epoch 71/100\n",
      "365/365 [==============================] - 0s 121us/sample - loss: 7.3630 - mae: 7.3630 - val_loss: 4.9688 - val_mae: 4.9688\n",
      "Epoch 72/100\n",
      "365/365 [==============================] - 0s 120us/sample - loss: 5.3006 - mae: 5.3006 - val_loss: 3.9339 - val_mae: 3.9339\n",
      "Epoch 73/100\n",
      "365/365 [==============================] - 0s 116us/sample - loss: 3.7887 - mae: 3.7887 - val_loss: 2.9231 - val_mae: 2.9231\n",
      "Epoch 74/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 3.0513 - mae: 3.0513 - val_loss: 2.2252 - val_mae: 2.2252\n",
      "Epoch 75/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 2.2746 - mae: 2.2746 - val_loss: 2.0468 - val_mae: 2.0468\n",
      "Epoch 76/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 2.1725 - mae: 2.1725 - val_loss: 2.1481 - val_mae: 2.1481\n",
      "Epoch 77/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 2.1464 - mae: 2.1464 - val_loss: 1.9324 - val_mae: 1.9324\n",
      "Epoch 78/100\n",
      "365/365 [==============================] - 0s 118us/sample - loss: 2.0545 - mae: 2.0545 - val_loss: 1.7018 - val_mae: 1.7018\n",
      "Epoch 79/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.8226 - mae: 1.8226 - val_loss: 1.5261 - val_mae: 1.5261\n",
      "Epoch 80/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.5541 - mae: 1.5541 - val_loss: 1.3095 - val_mae: 1.3095\n",
      "Epoch 81/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.6827 - mae: 1.6827 - val_loss: 1.3680 - val_mae: 1.3680\n",
      "Epoch 82/100\n",
      "365/365 [==============================] - 0s 113us/sample - loss: 1.6010 - mae: 1.6010 - val_loss: 1.2517 - val_mae: 1.2517\n",
      "Epoch 83/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.6041 - mae: 1.6041 - val_loss: 1.2268 - val_mae: 1.2268\n",
      "Epoch 84/100\n",
      "365/365 [==============================] - 0s 110us/sample - loss: 1.5489 - mae: 1.5489 - val_loss: 1.1452 - val_mae: 1.1452\n",
      "Epoch 85/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.4145 - mae: 1.4145 - val_loss: 1.1432 - val_mae: 1.1432\n",
      "Epoch 86/100\n",
      "365/365 [==============================] - 0s 109us/sample - loss: 1.4248 - mae: 1.4248 - val_loss: 1.0562 - val_mae: 1.0562\n",
      "Epoch 87/100\n",
      "365/365 [==============================] - 0s 115us/sample - loss: 1.5938 - mae: 1.5938 - val_loss: 1.1122 - val_mae: 1.1122\n",
      "Epoch 88/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.3205 - mae: 1.3205 - val_loss: 1.0526 - val_mae: 1.0526\n",
      "Epoch 89/100\n",
      "365/365 [==============================] - 0s 112us/sample - loss: 1.4165 - mae: 1.4165 - val_loss: 0.9874 - val_mae: 0.9874\n",
      "Epoch 90/100\n",
      "365/365 [==============================] - 0s 107us/sample - loss: 1.2689 - mae: 1.2689 - val_loss: 0.9311 - val_mae: 0.9311\n",
      "Epoch 91/100\n",
      "365/365 [==============================] - 0s 111us/sample - loss: 1.2875 - mae: 1.2875 - val_loss: 0.9651 - val_mae: 0.9651\n",
      "Epoch 92/100\n",
      "365/365 [==============================] - 0s 106us/sample - loss: 1.3557 - mae: 1.3557 - val_loss: 0.8407 - val_mae: 0.8407\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365/365 [==============================] - 0s 115us/sample - loss: 1.1798 - mae: 1.1798 - val_loss: 0.8551 - val_mae: 0.8551\n",
      "Epoch 94/100\n",
      "365/365 [==============================] - 0s 114us/sample - loss: 1.1996 - mae: 1.1996 - val_loss: 0.8812 - val_mae: 0.8812\n",
      "365/365 [==============================] - 0s 38us/sample - loss: 0.8812 - mae: 0.8812\n",
      "Val score is 0.8812200427055359\n",
      "DataFrame: bands imput_method :MLP model :KNN score:  0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: bands imput_method :MLP model :XGB score:  0.74\n",
      "DataFrame: bands imput_method :MLP model :MLP score:  0.65\n",
      "DataFrame: breast imput_method :LOCF model :KNN score:  0.7\n",
      "DataFrame: breast imput_method :LOCF model :XGB score:  0.72\n",
      "DataFrame: breast imput_method :LOCF model :MLP score:  0.68\n",
      "DataFrame: breast imput_method :mean_mode model :KNN score:  0.7\n",
      "DataFrame: breast imput_method :mean_mode model :XGB score:  0.73\n",
      "DataFrame: breast imput_method :mean_mode model :MLP score:  0.66\n",
      "DataFrame: breast imput_method :knn model :KNN score:  0.7\n",
      "DataFrame: breast imput_method :knn model :XGB score:  0.72\n",
      "DataFrame: breast imput_method :knn model :MLP score:  0.65\n",
      "Best score is 0.8274115630356865\n",
      "Best score is 0.4280701754385965\n",
      "DataFrame: breast imput_method :trees model :KNN score:  0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: breast imput_method :trees model :XGB score:  0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: breast imput_method :trees model :MLP score:  0.65\n",
      "Train on 277 samples, validate on 277 samples\n",
      "Epoch 1/100\n",
      "277/277 [==============================] - 1s 3ms/sample - loss: 0.9038 - accuracy: 0.5126 - val_loss: 0.7329 - val_accuracy: 0.4801\n",
      "Epoch 2/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 0.6954 - accuracy: 0.6029 - val_loss: 0.6832 - val_accuracy: 0.6209\n",
      "Epoch 3/100\n",
      "277/277 [==============================] - 0s 125us/sample - loss: 0.5695 - accuracy: 0.7437 - val_loss: 0.6330 - val_accuracy: 0.7256\n",
      "Epoch 4/100\n",
      "277/277 [==============================] - 0s 131us/sample - loss: 0.5108 - accuracy: 0.7798 - val_loss: 0.5825 - val_accuracy: 0.8123\n",
      "Epoch 5/100\n",
      "277/277 [==============================] - 0s 155us/sample - loss: 0.4403 - accuracy: 0.8484 - val_loss: 0.5412 - val_accuracy: 0.8556\n",
      "Epoch 6/100\n",
      "277/277 [==============================] - 0s 130us/sample - loss: 0.4133 - accuracy: 0.8736 - val_loss: 0.5001 - val_accuracy: 0.8881\n",
      "Epoch 7/100\n",
      "277/277 [==============================] - 0s 127us/sample - loss: 0.3758 - accuracy: 0.9025 - val_loss: 0.4608 - val_accuracy: 0.9134\n",
      "Epoch 8/100\n",
      "277/277 [==============================] - 0s 123us/sample - loss: 0.3672 - accuracy: 0.8736 - val_loss: 0.4277 - val_accuracy: 0.9061\n",
      "Epoch 9/100\n",
      "277/277 [==============================] - 0s 122us/sample - loss: 0.3178 - accuracy: 0.9170 - val_loss: 0.3969 - val_accuracy: 0.9170\n",
      "Epoch 10/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 0.2983 - accuracy: 0.9097 - val_loss: 0.3667 - val_accuracy: 0.9242\n",
      "Epoch 11/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 0.2721 - accuracy: 0.9314 - val_loss: 0.3398 - val_accuracy: 0.9242\n",
      "Epoch 12/100\n",
      "277/277 [==============================] - 0s 133us/sample - loss: 0.2635 - accuracy: 0.9314 - val_loss: 0.3129 - val_accuracy: 0.9278\n",
      "Epoch 13/100\n",
      "277/277 [==============================] - 0s 120us/sample - loss: 0.2437 - accuracy: 0.9495 - val_loss: 0.2905 - val_accuracy: 0.9386\n",
      "Epoch 14/100\n",
      "277/277 [==============================] - 0s 137us/sample - loss: 0.2413 - accuracy: 0.9350 - val_loss: 0.2691 - val_accuracy: 0.9495\n",
      "Epoch 15/100\n",
      "277/277 [==============================] - 0s 136us/sample - loss: 0.2116 - accuracy: 0.9531 - val_loss: 0.2494 - val_accuracy: 0.9495\n",
      "Epoch 16/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 0.2021 - accuracy: 0.9458 - val_loss: 0.2316 - val_accuracy: 0.9495\n",
      "277/277 [==============================] - 0s 41us/sample - loss: 0.2316 - accuracy: 0.9495\n",
      "Val score is 0.9494584798812866\n",
      "Train on 277 samples, validate on 277 samples\n",
      "Epoch 1/100\n",
      "277/277 [==============================] - 1s 3ms/sample - loss: 1.9704 - accuracy: 0.2238 - val_loss: 1.6615 - val_accuracy: 0.2274\n",
      "Epoch 2/100\n",
      "277/277 [==============================] - 0s 125us/sample - loss: 1.7150 - accuracy: 0.2780 - val_loss: 1.5935 - val_accuracy: 0.2671\n",
      "Epoch 3/100\n",
      "277/277 [==============================] - 0s 123us/sample - loss: 1.5616 - accuracy: 0.3141 - val_loss: 1.5383 - val_accuracy: 0.3032\n",
      "Epoch 4/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 1.5101 - accuracy: 0.3466 - val_loss: 1.4849 - val_accuracy: 0.3502\n",
      "Epoch 5/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 1.3889 - accuracy: 0.4188 - val_loss: 1.4358 - val_accuracy: 0.3827\n",
      "Epoch 6/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 1.3369 - accuracy: 0.4513 - val_loss: 1.3919 - val_accuracy: 0.4332\n",
      "Epoch 7/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 1.2670 - accuracy: 0.4946 - val_loss: 1.3490 - val_accuracy: 0.4801\n",
      "Epoch 8/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 1.2403 - accuracy: 0.5379 - val_loss: 1.3097 - val_accuracy: 0.5235\n",
      "Epoch 9/100\n",
      "277/277 [==============================] - 0s 124us/sample - loss: 1.1862 - accuracy: 0.5415 - val_loss: 1.2680 - val_accuracy: 0.5487\n",
      "Epoch 10/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 1.1314 - accuracy: 0.5776 - val_loss: 1.2304 - val_accuracy: 0.5704\n",
      "Epoch 11/100\n",
      "277/277 [==============================] - 0s 122us/sample - loss: 1.1029 - accuracy: 0.6173 - val_loss: 1.1958 - val_accuracy: 0.5993\n",
      "Epoch 12/100\n",
      "277/277 [==============================] - 0s 125us/sample - loss: 1.1095 - accuracy: 0.5993 - val_loss: 1.1606 - val_accuracy: 0.6390\n",
      "Epoch 13/100\n",
      "277/277 [==============================] - 0s 123us/sample - loss: 1.0433 - accuracy: 0.6643 - val_loss: 1.1262 - val_accuracy: 0.6606\n",
      "Epoch 14/100\n",
      "277/277 [==============================] - 0s 125us/sample - loss: 1.0299 - accuracy: 0.6570 - val_loss: 1.0945 - val_accuracy: 0.6823\n",
      "Epoch 15/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 1.0108 - accuracy: 0.6137 - val_loss: 1.0611 - val_accuracy: 0.6823\n",
      "Epoch 16/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 0.9690 - accuracy: 0.6679 - val_loss: 1.0320 - val_accuracy: 0.6968\n",
      "Epoch 17/100\n",
      "277/277 [==============================] - 0s 120us/sample - loss: 0.9315 - accuracy: 0.6895 - val_loss: 1.0034 - val_accuracy: 0.6968\n",
      "Epoch 18/100\n",
      "277/277 [==============================] - 0s 123us/sample - loss: 0.9540 - accuracy: 0.6498 - val_loss: 0.9748 - val_accuracy: 0.7076\n",
      "Epoch 19/100\n",
      "277/277 [==============================] - 0s 118us/sample - loss: 0.8982 - accuracy: 0.6895 - val_loss: 0.9489 - val_accuracy: 0.7148\n",
      "Epoch 20/100\n",
      "277/277 [==============================] - 0s 119us/sample - loss: 0.8864 - accuracy: 0.7076 - val_loss: 0.9233 - val_accuracy: 0.7292\n",
      "Epoch 21/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 0.8661 - accuracy: 0.6895 - val_loss: 0.8976 - val_accuracy: 0.7292\n",
      "Epoch 22/100\n",
      "277/277 [==============================] - 0s 116us/sample - loss: 0.8346 - accuracy: 0.7184 - val_loss: 0.8724 - val_accuracy: 0.7329\n",
      "Epoch 23/100\n",
      "277/277 [==============================] - 0s 118us/sample - loss: 0.8138 - accuracy: 0.7004 - val_loss: 0.8472 - val_accuracy: 0.7401\n",
      "Epoch 24/100\n",
      "277/277 [==============================] - 0s 119us/sample - loss: 0.7674 - accuracy: 0.7617 - val_loss: 0.8247 - val_accuracy: 0.7509\n",
      "Epoch 25/100\n",
      "277/277 [==============================] - 0s 124us/sample - loss: 0.7907 - accuracy: 0.7148 - val_loss: 0.8022 - val_accuracy: 0.7617\n",
      "Epoch 26/100\n",
      "277/277 [==============================] - 0s 121us/sample - loss: 0.7758 - accuracy: 0.7365 - val_loss: 0.7800 - val_accuracy: 0.7653\n",
      "Epoch 27/100\n",
      "277/277 [==============================] - 0s 120us/sample - loss: 0.7327 - accuracy: 0.7329 - val_loss: 0.7589 - val_accuracy: 0.7653\n",
      "Epoch 28/100\n",
      "277/277 [==============================] - 0s 123us/sample - loss: 0.7246 - accuracy: 0.7617 - val_loss: 0.7380 - val_accuracy: 0.7870\n",
      "Epoch 29/100\n",
      "277/277 [==============================] - 0s 123us/sample - loss: 0.7080 - accuracy: 0.7509 - val_loss: 0.7189 - val_accuracy: 0.7834\n",
      "Epoch 30/100\n",
      "277/277 [==============================] - 0s 126us/sample - loss: 0.7055 - accuracy: 0.7653 - val_loss: 0.7013 - val_accuracy: 0.7798\n",
      "277/277 [==============================] - 0s 43us/sample - loss: 0.7013 - accuracy: 0.7798\n",
      "Val score is 0.7797833681106567\n",
      "DataFrame: breast imput_method :MLP model :KNN score:  0.7\n",
      "DataFrame: breast imput_method :MLP model :XGB score:  0.73\n",
      "DataFrame: breast imput_method :MLP model :MLP score:  0.65\n",
      "DataFrame: cleveland imput_method :LOCF model :KNN score:  0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: cleveland imput_method :LOCF model :XGB score:  0.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: cleveland imput_method :LOCF model :MLP score:  0.54\n",
      "DataFrame: cleveland imput_method :mean_mode model :KNN score:  0.58\n",
      "DataFrame: cleveland imput_method :mean_mode model :XGB score:  0.57\n",
      "DataFrame: cleveland imput_method :mean_mode model :MLP score:  0.55\n",
      "DataFrame: cleveland imput_method :knn model :KNN score:  0.58\n",
      "DataFrame: cleveland imput_method :knn model :XGB score:  0.59\n",
      "DataFrame: cleveland imput_method :knn model :MLP score:  0.53\n",
      "Best score is -0.5966476260411618\n",
      "Best score is -1.3123218816577797\n",
      "DataFrame: cleveland imput_method :trees model :KNN score:  0.58\n",
      "DataFrame: cleveland imput_method :trees model :XGB score:  0.58\n",
      "DataFrame: cleveland imput_method :trees model :MLP score:  0.55\n",
      "Train on 297 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "297/297 [==============================] - 1s 4ms/sample - loss: 1.2453 - mae: 1.2453 - val_loss: 0.7297 - val_mae: 0.7297\n",
      "Epoch 2/100\n",
      "297/297 [==============================] - 0s 152us/sample - loss: 0.9221 - mae: 0.9221 - val_loss: 0.7086 - val_mae: 0.7086\n",
      "Epoch 3/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.8035 - mae: 0.8035 - val_loss: 0.7044 - val_mae: 0.7044\n",
      "Epoch 4/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.7389 - mae: 0.7389 - val_loss: 0.7012 - val_mae: 0.7012\n",
      "Epoch 5/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.6774 - mae: 0.6774 - val_loss: 0.6994 - val_mae: 0.6994\n",
      "Epoch 6/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.6313 - mae: 0.6313 - val_loss: 0.6943 - val_mae: 0.6943\n",
      "Epoch 7/100\n",
      "297/297 [==============================] - 0s 157us/sample - loss: 0.6019 - mae: 0.6019 - val_loss: 0.6847 - val_mae: 0.6847\n",
      "Epoch 8/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.5882 - mae: 0.5882 - val_loss: 0.6710 - val_mae: 0.6710\n",
      "Epoch 9/100\n",
      "297/297 [==============================] - 0s 161us/sample - loss: 0.5812 - mae: 0.5812 - val_loss: 0.6584 - val_mae: 0.6584\n",
      "Epoch 10/100\n",
      "297/297 [==============================] - 0s 160us/sample - loss: 0.5489 - mae: 0.5489 - val_loss: 0.6594 - val_mae: 0.6594\n",
      "Epoch 11/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.5215 - mae: 0.5215 - val_loss: 0.6335 - val_mae: 0.6335\n",
      "Epoch 12/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.5237 - mae: 0.5237 - val_loss: 0.6192 - val_mae: 0.6192\n",
      "Epoch 13/100\n",
      "297/297 [==============================] - 0s 157us/sample - loss: 0.5235 - mae: 0.5235 - val_loss: 0.5907 - val_mae: 0.5907\n",
      "Epoch 14/100\n",
      "297/297 [==============================] - 0s 161us/sample - loss: 0.4853 - mae: 0.4853 - val_loss: 0.5660 - val_mae: 0.5660\n",
      "Epoch 15/100\n",
      "297/297 [==============================] - 0s 166us/sample - loss: 0.4876 - mae: 0.4876 - val_loss: 0.5411 - val_mae: 0.5411\n",
      "Epoch 16/100\n",
      "297/297 [==============================] - 0s 154us/sample - loss: 0.4702 - mae: 0.4702 - val_loss: 0.5655 - val_mae: 0.5655\n",
      "Epoch 17/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.4647 - mae: 0.4647 - val_loss: 0.5384 - val_mae: 0.5384\n",
      "Epoch 18/100\n",
      "297/297 [==============================] - 0s 154us/sample - loss: 0.4810 - mae: 0.4810 - val_loss: 0.5160 - val_mae: 0.5160\n",
      "Epoch 19/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.4159 - mae: 0.4159 - val_loss: 0.4970 - val_mae: 0.4970\n",
      "Epoch 20/100\n",
      "297/297 [==============================] - 0s 156us/sample - loss: 0.4561 - mae: 0.4561 - val_loss: 0.4987 - val_mae: 0.4987\n",
      "Epoch 21/100\n",
      "297/297 [==============================] - 0s 161us/sample - loss: 0.4519 - mae: 0.4519 - val_loss: 0.4660 - val_mae: 0.4660\n",
      "Epoch 22/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.4457 - mae: 0.4457 - val_loss: 0.4352 - val_mae: 0.4352\n",
      "Epoch 23/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.4284 - mae: 0.4284 - val_loss: 0.4416 - val_mae: 0.4416\n",
      "Epoch 24/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.4365 - mae: 0.4365 - val_loss: 0.4373 - val_mae: 0.4373\n",
      "297/297 [==============================] - 0s 54us/sample - loss: 0.4373 - mae: 0.4373\n",
      "Val score is 0.4372875988483429\n",
      "Train on 297 samples, validate on 297 samples\n",
      "Epoch 1/100\n",
      "297/297 [==============================] - 1s 5ms/sample - loss: 4.6917 - mae: 4.6917 - val_loss: 4.9910 - val_mae: 4.9910\n",
      "Epoch 2/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 4.5832 - mae: 4.5832 - val_loss: 4.8214 - val_mae: 4.8214\n",
      "Epoch 3/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 4.4684 - mae: 4.4684 - val_loss: 4.6495 - val_mae: 4.6495\n",
      "Epoch 4/100\n",
      "297/297 [==============================] - 0s 157us/sample - loss: 4.3405 - mae: 4.3405 - val_loss: 4.4344 - val_mae: 4.4344\n",
      "Epoch 5/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 4.2047 - mae: 4.2047 - val_loss: 4.1969 - val_mae: 4.1969\n",
      "Epoch 6/100\n",
      "297/297 [==============================] - 0s 165us/sample - loss: 4.0434 - mae: 4.0434 - val_loss: 3.9552 - val_mae: 3.9552\n",
      "Epoch 7/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 3.8645 - mae: 3.8645 - val_loss: 3.7417 - val_mae: 3.7417\n",
      "Epoch 8/100\n",
      "297/297 [==============================] - 0s 152us/sample - loss: 3.6683 - mae: 3.6683 - val_loss: 3.4902 - val_mae: 3.4902\n",
      "Epoch 9/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 3.4566 - mae: 3.4566 - val_loss: 3.1632 - val_mae: 3.1632\n",
      "Epoch 10/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 3.2189 - mae: 3.2189 - val_loss: 2.8131 - val_mae: 2.8131\n",
      "Epoch 11/100\n",
      "297/297 [==============================] - 0s 153us/sample - loss: 2.9808 - mae: 2.9808 - val_loss: 2.4594 - val_mae: 2.4594\n",
      "Epoch 12/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 2.7301 - mae: 2.7301 - val_loss: 2.1370 - val_mae: 2.1370\n",
      "Epoch 13/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 2.4538 - mae: 2.4538 - val_loss: 1.8418 - val_mae: 1.8418\n",
      "Epoch 14/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 2.2167 - mae: 2.2167 - val_loss: 1.5948 - val_mae: 1.5948\n",
      "Epoch 15/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 1.9780 - mae: 1.9780 - val_loss: 1.4850 - val_mae: 1.4850\n",
      "Epoch 16/100\n",
      "297/297 [==============================] - 0s 148us/sample - loss: 1.7984 - mae: 1.7984 - val_loss: 1.2948 - val_mae: 1.2948\n",
      "Epoch 17/100\n",
      "297/297 [==============================] - 0s 159us/sample - loss: 1.5396 - mae: 1.5396 - val_loss: 1.1894 - val_mae: 1.1894\n",
      "Epoch 18/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 1.3287 - mae: 1.3287 - val_loss: 1.1397 - val_mae: 1.1397\n",
      "Epoch 19/100\n",
      "297/297 [==============================] - 0s 148us/sample - loss: 1.1977 - mae: 1.1977 - val_loss: 1.0604 - val_mae: 1.0604\n",
      "Epoch 20/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 1.1014 - mae: 1.1014 - val_loss: 1.0696 - val_mae: 1.0696\n",
      "Epoch 21/100\n",
      "297/297 [==============================] - 0s 148us/sample - loss: 1.0425 - mae: 1.0425 - val_loss: 1.0438 - val_mae: 1.0438\n",
      "Epoch 22/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 1.0213 - mae: 1.0213 - val_loss: 1.0081 - val_mae: 1.0081\n",
      "Epoch 23/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.9391 - mae: 0.9391 - val_loss: 0.9732 - val_mae: 0.9732\n",
      "Epoch 24/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.9794 - mae: 0.9794 - val_loss: 0.9159 - val_mae: 0.9159\n",
      "Epoch 25/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.8814 - mae: 0.8814 - val_loss: 0.8859 - val_mae: 0.8859\n",
      "Epoch 26/100\n",
      "297/297 [==============================] - 0s 157us/sample - loss: 0.9427 - mae: 0.9427 - val_loss: 0.8224 - val_mae: 0.8224\n",
      "Epoch 27/100\n",
      "297/297 [==============================] - 0s 155us/sample - loss: 0.8659 - mae: 0.8659 - val_loss: 0.8062 - val_mae: 0.8062\n",
      "Epoch 28/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.8732 - mae: 0.8732 - val_loss: 0.7462 - val_mae: 0.7462\n",
      "Epoch 29/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.8013 - mae: 0.8013 - val_loss: 0.7234 - val_mae: 0.7234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.7854 - mae: 0.7854 - val_loss: 0.6975 - val_mae: 0.6975\n",
      "Epoch 31/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.8208 - mae: 0.8208 - val_loss: 0.7665 - val_mae: 0.7665\n",
      "Epoch 32/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.8050 - mae: 0.8050 - val_loss: 0.6589 - val_mae: 0.6589\n",
      "Epoch 33/100\n",
      "297/297 [==============================] - 0s 158us/sample - loss: 0.8355 - mae: 0.8355 - val_loss: 0.6364 - val_mae: 0.6364\n",
      "Epoch 34/100\n",
      "297/297 [==============================] - 0s 162us/sample - loss: 0.7766 - mae: 0.7766 - val_loss: 0.7120 - val_mae: 0.7120\n",
      "Epoch 35/100\n",
      "297/297 [==============================] - 0s 145us/sample - loss: 0.8012 - mae: 0.8012 - val_loss: 0.6233 - val_mae: 0.6233\n",
      "Epoch 36/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.7335 - mae: 0.7335 - val_loss: 0.5728 - val_mae: 0.5728\n",
      "Epoch 37/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.8472 - mae: 0.8472 - val_loss: 0.6450 - val_mae: 0.6450\n",
      "Epoch 38/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.7045 - mae: 0.7045 - val_loss: 0.5472 - val_mae: 0.5472\n",
      "Epoch 39/100\n",
      "297/297 [==============================] - 0s 151us/sample - loss: 0.8426 - mae: 0.8426 - val_loss: 0.5567 - val_mae: 0.5567\n",
      "Epoch 40/100\n",
      "297/297 [==============================] - 0s 145us/sample - loss: 0.8987 - mae: 0.8987 - val_loss: 0.5636 - val_mae: 0.5636\n",
      "297/297 [==============================] - 0s 50us/sample - loss: 0.5636 - mae: 0.5636\n",
      "Val score is 0.5635855197906494\n",
      "DataFrame: cleveland imput_method :MLP model :KNN score:  0.59\n",
      "DataFrame: cleveland imput_method :MLP model :XGB score:  0.55\n",
      "DataFrame: cleveland imput_method :MLP model :MLP score:  0.55\n",
      "DataFrame: crx imput_method :LOCF model :KNN score:  0.8\n",
      "DataFrame: crx imput_method :LOCF model :XGB score:  0.86\n",
      "DataFrame: crx imput_method :LOCF model :MLP score:  0.82\n",
      "DataFrame: crx imput_method :mean_mode model :KNN score:  0.81\n",
      "DataFrame: crx imput_method :mean_mode model :XGB score:  0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: crx imput_method :mean_mode model :MLP score:  0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n",
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: crx imput_method :knn model :KNN score:  0.81\n",
      "DataFrame: crx imput_method :knn model :XGB score:  0.87\n",
      "DataFrame: crx imput_method :knn model :MLP score:  0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is 0.6902654867256638\n",
      "Best score is -1201.8481636047363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is 0.9970760233918128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is 0.9985380116959064\n",
      "Best score is 0.28487518355359764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is 0.6607929515418502\n",
      "Best score is -119.63088750064746\n",
      "DataFrame: crx imput_method :trees model :KNN score:  0.8\n",
      "DataFrame: crx imput_method :trees model :XGB score:  0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: crx imput_method :trees model :MLP score:  0.82\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 1s 2ms/sample - loss: 0.7808 - accuracy: 0.5544 - val_loss: 0.6903 - val_accuracy: 0.5819\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 0.6362 - accuracy: 0.6386 - val_loss: 0.6141 - val_accuracy: 0.6953\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 139us/sample - loss: 0.5854 - accuracy: 0.7044 - val_loss: 0.5813 - val_accuracy: 0.7075\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 136us/sample - loss: 0.5273 - accuracy: 0.7489 - val_loss: 0.5556 - val_accuracy: 0.7152\n",
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 0.5221 - accuracy: 0.7504 - val_loss: 0.5350 - val_accuracy: 0.7228\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 150us/sample - loss: 0.4984 - accuracy: 0.7596 - val_loss: 0.5142 - val_accuracy: 0.7443\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 147us/sample - loss: 0.4849 - accuracy: 0.7902 - val_loss: 0.4956 - val_accuracy: 0.7473\n",
      "Epoch 8/100\n",
      "653/653 [==============================] - 0s 136us/sample - loss: 0.4786 - accuracy: 0.7642 - val_loss: 0.4826 - val_accuracy: 0.7473\n",
      "Epoch 9/100\n",
      "653/653 [==============================] - 0s 150us/sample - loss: 0.4495 - accuracy: 0.7841 - val_loss: 0.4671 - val_accuracy: 0.7611\n",
      "Epoch 10/100\n",
      "653/653 [==============================] - 0s 153us/sample - loss: 0.4451 - accuracy: 0.7948 - val_loss: 0.4506 - val_accuracy: 0.7841\n",
      "Epoch 11/100\n",
      "653/653 [==============================] - 0s 145us/sample - loss: 0.4312 - accuracy: 0.8055 - val_loss: 0.4352 - val_accuracy: 0.7917\n",
      "Epoch 12/100\n",
      "653/653 [==============================] - 0s 150us/sample - loss: 0.4222 - accuracy: 0.7979 - val_loss: 0.4218 - val_accuracy: 0.8086\n",
      "Epoch 13/100\n",
      "653/653 [==============================] - 0s 136us/sample - loss: 0.4252 - accuracy: 0.8009 - val_loss: 0.4110 - val_accuracy: 0.7948\n",
      "Epoch 14/100\n",
      "653/653 [==============================] - 0s 158us/sample - loss: 0.4270 - accuracy: 0.8025 - val_loss: 0.4012 - val_accuracy: 0.8086\n",
      "653/653 [==============================] - 0s 41us/sample - loss: 0.4012 - accuracy: 0.8086\n",
      "Val score is 0.8085758090019226\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 1s 2ms/sample - loss: 2679.1364 - mae: 2679.1362 - val_loss: 2678.5927 - val_mae: 2678.5930\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 131us/sample - loss: 2678.8769 - mae: 2678.8770 - val_loss: 2678.2823 - val_mae: 2678.2825\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2678.5417 - mae: 2678.5413 - val_loss: 2677.8984 - val_mae: 2677.8982\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2678.1153 - mae: 2678.1152 - val_loss: 2677.4078 - val_mae: 2677.4080\n",
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 134us/sample - loss: 2677.5887 - mae: 2677.5884 - val_loss: 2676.8266 - val_mae: 2676.8267\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 138us/sample - loss: 2676.9563 - mae: 2676.9563 - val_loss: 2676.1406 - val_mae: 2676.1406\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 153us/sample - loss: 2676.2147 - mae: 2676.2148 - val_loss: 2675.3481 - val_mae: 2675.3481\n",
      "Epoch 8/100\n",
      "653/653 [==============================] - 0s 153us/sample - loss: 2675.3613 - mae: 2675.3613 - val_loss: 2674.4594 - val_mae: 2674.4597\n",
      "Epoch 9/100\n",
      "653/653 [==============================] - 0s 148us/sample - loss: 2674.3948 - mae: 2674.3948 - val_loss: 2673.4538 - val_mae: 2673.4541\n",
      "Epoch 10/100\n",
      "653/653 [==============================] - 0s 125us/sample - loss: 2673.3142 - mae: 2673.3142 - val_loss: 2672.3424 - val_mae: 2672.3425\n",
      "Epoch 11/100\n",
      "653/653 [==============================] - 0s 125us/sample - loss: 2672.1188 - mae: 2672.1189 - val_loss: 2671.1021 - val_mae: 2671.1021\n",
      "Epoch 12/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2670.8084 - mae: 2670.8086 - val_loss: 2669.7660 - val_mae: 2669.7656\n",
      "Epoch 13/100\n",
      "653/653 [==============================] - 0s 155us/sample - loss: 2669.3830 - mae: 2669.3831 - val_loss: 2668.3117 - val_mae: 2668.3120\n",
      "Epoch 14/100\n",
      "653/653 [==============================] - 0s 155us/sample - loss: 2667.8427 - mae: 2667.8428 - val_loss: 2666.7165 - val_mae: 2666.7168\n",
      "Epoch 15/100\n",
      "653/653 [==============================] - 0s 155us/sample - loss: 2666.1931 - mae: 2666.1931 - val_loss: 2664.4560 - val_mae: 2664.4561\n",
      "Epoch 16/100\n",
      "653/653 [==============================] - 0s 122us/sample - loss: 2664.4879 - mae: 2664.4880 - val_loss: 2662.3623 - val_mae: 2662.3621\n",
      "Epoch 17/100\n",
      "653/653 [==============================] - 0s 126us/sample - loss: 2662.5738 - mae: 2662.5740 - val_loss: 2660.0224 - val_mae: 2660.0225\n",
      "Epoch 18/100\n",
      "653/653 [==============================] - 0s 137us/sample - loss: 2660.6332 - mae: 2660.6331 - val_loss: 2658.0218 - val_mae: 2658.0217\n",
      "Epoch 19/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 2658.5495 - mae: 2658.5493 - val_loss: 2655.8859 - val_mae: 2655.8860\n",
      "Epoch 20/100\n",
      "653/653 [==============================] - 0s 134us/sample - loss: 2656.3564 - mae: 2656.3564 - val_loss: 2654.2017 - val_mae: 2654.2017\n",
      "Epoch 21/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2654.0683 - mae: 2654.0686 - val_loss: 2651.6628 - val_mae: 2651.6631\n",
      "Epoch 22/100\n",
      "653/653 [==============================] - 0s 128us/sample - loss: 2651.6772 - mae: 2651.6772 - val_loss: 2649.1018 - val_mae: 2649.1021\n",
      "Epoch 23/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2649.2423 - mae: 2649.2424 - val_loss: 2646.2726 - val_mae: 2646.2727\n",
      "Epoch 24/100\n",
      "653/653 [==============================] - 0s 126us/sample - loss: 2646.7167 - mae: 2646.7168 - val_loss: 2643.8072 - val_mae: 2643.8071\n",
      "Epoch 25/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2644.0384 - mae: 2644.0386 - val_loss: 2640.8564 - val_mae: 2640.8562\n",
      "Epoch 26/100\n",
      "653/653 [==============================] - 0s 131us/sample - loss: 2641.3072 - mae: 2641.3074 - val_loss: 2637.4377 - val_mae: 2637.4375\n",
      "Epoch 27/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 2638.5928 - mae: 2638.5925 - val_loss: 2634.2738 - val_mae: 2634.2734\n",
      "Epoch 28/100\n",
      "653/653 [==============================] - 0s 135us/sample - loss: 2635.6541 - mae: 2635.6538 - val_loss: 2630.9271 - val_mae: 2630.9272\n",
      "Epoch 29/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2632.6497 - mae: 2632.6494 - val_loss: 2628.3758 - val_mae: 2628.3760\n",
      "Epoch 30/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2629.5873 - mae: 2629.5874 - val_loss: 2625.4578 - val_mae: 2625.4580\n",
      "Epoch 31/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2626.4225 - mae: 2626.4224 - val_loss: 2622.0431 - val_mae: 2622.0430\n",
      "Epoch 32/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2623.0804 - mae: 2623.0806 - val_loss: 2618.3343 - val_mae: 2618.3345\n",
      "Epoch 33/100\n",
      "653/653 [==============================] - 0s 128us/sample - loss: 2619.6587 - mae: 2619.6589 - val_loss: 2614.2627 - val_mae: 2614.2629\n",
      "Epoch 34/100\n",
      "653/653 [==============================] - 0s 134us/sample - loss: 2616.1963 - mae: 2616.1963 - val_loss: 2610.2871 - val_mae: 2610.2874\n",
      "Epoch 35/100\n",
      "653/653 [==============================] - 0s 122us/sample - loss: 2612.6233 - mae: 2612.6235 - val_loss: 2606.5961 - val_mae: 2606.5962\n",
      "Epoch 36/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2608.9776 - mae: 2608.9775 - val_loss: 2601.6993 - val_mae: 2601.6992\n",
      "Epoch 37/100\n",
      "653/653 [==============================] - 0s 125us/sample - loss: 2605.4546 - mae: 2605.4546 - val_loss: 2595.9782 - val_mae: 2595.9783\n",
      "Epoch 38/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2601.2143 - mae: 2601.2141 - val_loss: 2591.8719 - val_mae: 2591.8721\n",
      "Epoch 39/100\n",
      "653/653 [==============================] - 0s 125us/sample - loss: 2597.1328 - mae: 2597.1328 - val_loss: 2587.2060 - val_mae: 2587.2061\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 113us/sample - loss: 2593.0823 - mae: 2593.0823 - val_loss: 2583.2482 - val_mae: 2583.2480\n",
      "Epoch 41/100\n",
      "653/653 [==============================] - 0s 151us/sample - loss: 2588.7552 - mae: 2588.7551 - val_loss: 2577.3514 - val_mae: 2577.3513\n",
      "Epoch 42/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2584.2197 - mae: 2584.2200 - val_loss: 2571.8332 - val_mae: 2571.8333\n",
      "Epoch 43/100\n",
      "653/653 [==============================] - 0s 128us/sample - loss: 2579.7402 - mae: 2579.7400 - val_loss: 2566.9619 - val_mae: 2566.9617\n",
      "Epoch 44/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2575.4892 - mae: 2575.4893 - val_loss: 2563.7049 - val_mae: 2563.7048\n",
      "Epoch 45/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2571.2797 - mae: 2571.2798 - val_loss: 2560.0340 - val_mae: 2560.0342\n",
      "Epoch 46/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2566.0922 - mae: 2566.0925 - val_loss: 2554.3304 - val_mae: 2554.3303\n",
      "Epoch 47/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2560.9345 - mae: 2560.9346 - val_loss: 2549.2956 - val_mae: 2549.2957\n",
      "Epoch 48/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2555.9851 - mae: 2555.9851 - val_loss: 2544.6197 - val_mae: 2544.6196\n",
      "Epoch 49/100\n",
      "653/653 [==============================] - 0s 135us/sample - loss: 2551.3096 - mae: 2551.3098 - val_loss: 2540.9013 - val_mae: 2540.9014\n",
      "Epoch 50/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2546.2773 - mae: 2546.2776 - val_loss: 2532.9060 - val_mae: 2532.9060\n",
      "Epoch 51/100\n",
      "653/653 [==============================] - 0s 128us/sample - loss: 2540.9150 - mae: 2540.9150 - val_loss: 2527.4552 - val_mae: 2527.4553\n",
      "Epoch 52/100\n",
      "653/653 [==============================] - 0s 131us/sample - loss: 2536.4160 - mae: 2536.4160 - val_loss: 2521.3072 - val_mae: 2521.3071\n",
      "Epoch 53/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2530.9399 - mae: 2530.9399 - val_loss: 2519.4568 - val_mae: 2519.4568\n",
      "Epoch 54/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2525.0195 - mae: 2525.0193 - val_loss: 2511.0875 - val_mae: 2511.0874\n",
      "Epoch 55/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2519.6873 - mae: 2519.6875 - val_loss: 2503.1371 - val_mae: 2503.1367\n",
      "Epoch 56/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2514.1205 - mae: 2514.1206 - val_loss: 2499.2315 - val_mae: 2499.2310\n",
      "Epoch 57/100\n",
      "653/653 [==============================] - 0s 126us/sample - loss: 2508.5105 - mae: 2508.5105 - val_loss: 2497.8972 - val_mae: 2497.8977\n",
      "Epoch 58/100\n",
      "653/653 [==============================] - 0s 131us/sample - loss: 2503.0515 - mae: 2503.0515 - val_loss: 2487.5083 - val_mae: 2487.5083\n",
      "Epoch 59/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2497.3454 - mae: 2497.3459 - val_loss: 2486.0089 - val_mae: 2486.0090\n",
      "Epoch 60/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2490.8398 - mae: 2490.8396 - val_loss: 2478.7892 - val_mae: 2478.7891\n",
      "Epoch 61/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2486.3681 - mae: 2486.3679 - val_loss: 2469.1352 - val_mae: 2469.1355\n",
      "Epoch 62/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2480.8196 - mae: 2480.8196 - val_loss: 2462.4313 - val_mae: 2462.4316\n",
      "Epoch 63/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 2473.6548 - mae: 2473.6548 - val_loss: 2465.4224 - val_mae: 2465.4224\n",
      "Epoch 64/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2468.1963 - mae: 2468.1960 - val_loss: 2456.7870 - val_mae: 2456.7869\n",
      "Epoch 65/100\n",
      "653/653 [==============================] - 0s 136us/sample - loss: 2462.7322 - mae: 2462.7322 - val_loss: 2446.3806 - val_mae: 2446.3809\n",
      "Epoch 66/100\n",
      "653/653 [==============================] - 0s 142us/sample - loss: 2456.8571 - mae: 2456.8569 - val_loss: 2446.8451 - val_mae: 2446.8455\n",
      "Epoch 67/100\n",
      "653/653 [==============================] - 0s 147us/sample - loss: 2448.9498 - mae: 2448.9500 - val_loss: 2436.9311 - val_mae: 2436.9314\n",
      "Epoch 68/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2444.1107 - mae: 2444.1104 - val_loss: 2426.7429 - val_mae: 2426.7427\n",
      "Epoch 69/100\n",
      "653/653 [==============================] - 0s 123us/sample - loss: 2436.2269 - mae: 2436.2271 - val_loss: 2415.4766 - val_mae: 2415.4766\n",
      "Epoch 70/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2431.5979 - mae: 2431.5981 - val_loss: 2413.6188 - val_mae: 2413.6189\n",
      "Epoch 71/100\n",
      "653/653 [==============================] - 0s 148us/sample - loss: 2424.6280 - mae: 2424.6282 - val_loss: 2407.6376 - val_mae: 2407.6379\n",
      "Epoch 72/100\n",
      "653/653 [==============================] - 0s 153us/sample - loss: 2418.6751 - mae: 2418.6748 - val_loss: 2405.5335 - val_mae: 2405.5332\n",
      "Epoch 73/100\n",
      "653/653 [==============================] - 0s 160us/sample - loss: 2409.4497 - mae: 2409.4497 - val_loss: 2396.1067 - val_mae: 2396.1069\n",
      "Epoch 74/100\n",
      "653/653 [==============================] - 0s 135us/sample - loss: 2404.3342 - mae: 2404.3337 - val_loss: 2391.7068 - val_mae: 2391.7068\n",
      "Epoch 75/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 2396.3483 - mae: 2396.3484 - val_loss: 2386.1994 - val_mae: 2386.1992\n",
      "Epoch 76/100\n",
      "653/653 [==============================] - 0s 129us/sample - loss: 2391.6357 - mae: 2391.6357 - val_loss: 2383.1606 - val_mae: 2383.1606\n",
      "Epoch 77/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2383.2738 - mae: 2383.2742 - val_loss: 2379.6366 - val_mae: 2379.6362\n",
      "Epoch 78/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2376.2433 - mae: 2376.2437 - val_loss: 2365.6799 - val_mae: 2365.6797\n",
      "Epoch 79/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2368.8628 - mae: 2368.8630 - val_loss: 2372.3777 - val_mae: 2372.3779\n",
      "Epoch 80/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2360.0010 - mae: 2360.0007 - val_loss: 2364.1137 - val_mae: 2364.1138\n",
      "Epoch 81/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2354.3477 - mae: 2354.3479 - val_loss: 2347.6698 - val_mae: 2347.6697\n",
      "Epoch 82/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 2346.8768 - mae: 2346.8767 - val_loss: 2342.1896 - val_mae: 2342.1895\n",
      "Epoch 83/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2340.6829 - mae: 2340.6829 - val_loss: 2344.2977 - val_mae: 2344.2979\n",
      "Epoch 84/100\n",
      "653/653 [==============================] - 0s 128us/sample - loss: 2331.8019 - mae: 2331.8018 - val_loss: 2327.1333 - val_mae: 2327.1333\n",
      "Epoch 85/100\n",
      "653/653 [==============================] - 0s 136us/sample - loss: 2324.7005 - mae: 2324.7004 - val_loss: 2303.7972 - val_mae: 2303.7971\n",
      "Epoch 86/100\n",
      "653/653 [==============================] - 0s 127us/sample - loss: 2315.7656 - mae: 2315.7656 - val_loss: 2300.5500 - val_mae: 2300.5498\n",
      "Epoch 87/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 2308.6101 - mae: 2308.6099 - val_loss: 2290.0898 - val_mae: 2290.0898\n",
      "Epoch 88/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 2302.1440 - mae: 2302.1438 - val_loss: 2300.7540 - val_mae: 2300.7539\n",
      "Epoch 89/100\n",
      "653/653 [==============================] - 0s 131us/sample - loss: 2294.8032 - mae: 2294.8035 - val_loss: 2291.9561 - val_mae: 2291.9558\n",
      "653/653 [==============================] - 0s 40us/sample - loss: 2291.9561 - mae: 2291.9558\n",
      "Val score is 2291.955810546875\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 2s 2ms/sample - loss: 1.2288 - accuracy: 0.4303 - val_loss: 0.9962 - val_accuracy: 0.5100\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 0.7229 - accuracy: 0.7014 - val_loss: 0.7908 - val_accuracy: 0.8254\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 0.4944 - accuracy: 0.8698 - val_loss: 0.5807 - val_accuracy: 0.9709\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 135us/sample - loss: 0.3167 - accuracy: 0.9541 - val_loss: 0.4125 - val_accuracy: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 134us/sample - loss: 0.2114 - accuracy: 0.9816 - val_loss: 0.2795 - val_accuracy: 0.9954\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 0.1514 - accuracy: 0.9923 - val_loss: 0.1867 - val_accuracy: 0.9969\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 0.0912 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9969\n",
      "Epoch 8/100\n",
      "653/653 [==============================] - 0s 124us/sample - loss: 0.0850 - accuracy: 0.9939 - val_loss: 0.0869 - val_accuracy: 0.9985\n",
      "Epoch 9/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 0.0638 - accuracy: 0.9954 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "653/653 [==============================] - 0s 128us/sample - loss: 0.0491 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "653/653 [==============================] - 0s 132us/sample - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "653/653 [==============================] - 0s 41us/sample - loss: 0.0318 - accuracy: 1.0000\n",
      "Val score is 1.0\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 1s 2ms/sample - loss: 1.2332 - accuracy: 0.4257 - val_loss: 0.9292 - val_accuracy: 0.6554\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 133us/sample - loss: 0.7167 - accuracy: 0.7044 - val_loss: 0.6756 - val_accuracy: 0.9296\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 131us/sample - loss: 0.4468 - accuracy: 0.8760 - val_loss: 0.4805 - val_accuracy: 0.9740\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 130us/sample - loss: 0.2932 - accuracy: 0.9541 - val_loss: 0.3132 - val_accuracy: 0.9877\n",
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 114us/sample - loss: 0.1739 - accuracy: 0.9877 - val_loss: 0.1995 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 0.1196 - accuracy: 0.9939 - val_loss: 0.1280 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 0.0804 - accuracy: 0.9969 - val_loss: 0.0868 - val_accuracy: 1.0000\n",
      "653/653 [==============================] - 0s 33us/sample - loss: 0.0868 - accuracy: 1.0000\n",
      "Val score is 1.0\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 1s 1ms/sample - loss: 3.1214 - accuracy: 0.0904 - val_loss: 2.6621 - val_accuracy: 0.0904\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 2.6022 - accuracy: 0.2006 - val_loss: 2.4679 - val_accuracy: 0.2221\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 2.3292 - accuracy: 0.2527 - val_loss: 2.3152 - val_accuracy: 0.2894\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 2.1499 - accuracy: 0.3047 - val_loss: 2.1848 - val_accuracy: 0.3247\n",
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 2.0667 - accuracy: 0.3369 - val_loss: 2.0774 - val_accuracy: 0.3461\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.9359 - accuracy: 0.3737 - val_loss: 1.9857 - val_accuracy: 0.3828\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.8913 - accuracy: 0.3767 - val_loss: 1.9084 - val_accuracy: 0.3920\n",
      "Epoch 8/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.8195 - accuracy: 0.3874 - val_loss: 1.8392 - val_accuracy: 0.4089\n",
      "Epoch 9/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 1.7778 - accuracy: 0.3905 - val_loss: 1.7822 - val_accuracy: 0.4165\n",
      "Epoch 10/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.7478 - accuracy: 0.4227 - val_loss: 1.7270 - val_accuracy: 0.4334\n",
      "Epoch 11/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 1.7066 - accuracy: 0.4211 - val_loss: 1.6810 - val_accuracy: 0.4426\n",
      "Epoch 12/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.6658 - accuracy: 0.4456 - val_loss: 1.6382 - val_accuracy: 0.4594\n",
      "Epoch 13/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.6569 - accuracy: 0.4472 - val_loss: 1.6047 - val_accuracy: 0.4564\n",
      "Epoch 14/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.6103 - accuracy: 0.4655 - val_loss: 1.5704 - val_accuracy: 0.4778\n",
      "Epoch 15/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.5860 - accuracy: 0.4732 - val_loss: 1.5398 - val_accuracy: 0.4870\n",
      "Epoch 16/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.5562 - accuracy: 0.4793 - val_loss: 1.5121 - val_accuracy: 0.4931\n",
      "Epoch 17/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.5609 - accuracy: 0.4548 - val_loss: 1.4863 - val_accuracy: 0.5054\n",
      "Epoch 18/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 1.5030 - accuracy: 0.4747 - val_loss: 1.4627 - val_accuracy: 0.5115\n",
      "Epoch 19/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.5124 - accuracy: 0.4946 - val_loss: 1.4421 - val_accuracy: 0.5176\n",
      "Epoch 20/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.4880 - accuracy: 0.4916 - val_loss: 1.4213 - val_accuracy: 0.5191\n",
      "Epoch 21/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.5087 - accuracy: 0.4916 - val_loss: 1.4020 - val_accuracy: 0.5222\n",
      "Epoch 22/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.4637 - accuracy: 0.4839 - val_loss: 1.3808 - val_accuracy: 0.5436\n",
      "Epoch 23/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.4437 - accuracy: 0.5084 - val_loss: 1.3619 - val_accuracy: 0.5452\n",
      "Epoch 24/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 1.4512 - accuracy: 0.5008 - val_loss: 1.3473 - val_accuracy: 0.5391\n",
      "Epoch 25/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.4330 - accuracy: 0.4900 - val_loss: 1.3283 - val_accuracy: 0.5498\n",
      "Epoch 26/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.3961 - accuracy: 0.5191 - val_loss: 1.3129 - val_accuracy: 0.5636\n",
      "Epoch 27/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.4062 - accuracy: 0.5084 - val_loss: 1.2990 - val_accuracy: 0.5651\n",
      "Epoch 28/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.3829 - accuracy: 0.5436 - val_loss: 1.2788 - val_accuracy: 0.5681\n",
      "Epoch 29/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.3395 - accuracy: 0.5452 - val_loss: 1.2638 - val_accuracy: 0.5850\n",
      "Epoch 30/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.3593 - accuracy: 0.5299 - val_loss: 1.2456 - val_accuracy: 0.5804\n",
      "Epoch 31/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.3315 - accuracy: 0.5406 - val_loss: 1.2320 - val_accuracy: 0.5972\n",
      "Epoch 32/100\n",
      "653/653 [==============================] - 0s 104us/sample - loss: 1.3154 - accuracy: 0.5773 - val_loss: 1.2168 - val_accuracy: 0.5957\n",
      "Epoch 33/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.3208 - accuracy: 0.5482 - val_loss: 1.2010 - val_accuracy: 0.6064\n",
      "Epoch 34/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.2737 - accuracy: 0.5804 - val_loss: 1.1888 - val_accuracy: 0.6049\n",
      "Epoch 35/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.2720 - accuracy: 0.5681 - val_loss: 1.1728 - val_accuracy: 0.6141\n",
      "Epoch 36/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.2679 - accuracy: 0.5819 - val_loss: 1.1601 - val_accuracy: 0.6187\n",
      "Epoch 37/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.2316 - accuracy: 0.6003 - val_loss: 1.1461 - val_accuracy: 0.6248\n",
      "Epoch 38/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.2470 - accuracy: 0.5835 - val_loss: 1.1306 - val_accuracy: 0.6294\n",
      "Epoch 39/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.2081 - accuracy: 0.5896 - val_loss: 1.1138 - val_accuracy: 0.6401\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 99us/sample - loss: 1.2228 - accuracy: 0.5850 - val_loss: 1.1003 - val_accuracy: 0.6417\n",
      "Epoch 41/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 1.1804 - accuracy: 0.6172 - val_loss: 1.0875 - val_accuracy: 0.6462\n",
      "Epoch 42/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.1965 - accuracy: 0.5957 - val_loss: 1.0760 - val_accuracy: 0.6462\n",
      "Epoch 43/100\n",
      "653/653 [==============================] - 0s 105us/sample - loss: 1.1874 - accuracy: 0.5881 - val_loss: 1.0617 - val_accuracy: 0.6600\n",
      "Epoch 44/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.1580 - accuracy: 0.6156 - val_loss: 1.0495 - val_accuracy: 0.6616\n",
      "Epoch 45/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 1.1515 - accuracy: 0.6217 - val_loss: 1.0368 - val_accuracy: 0.6646\n",
      "Epoch 46/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.1518 - accuracy: 0.6126 - val_loss: 1.0304 - val_accuracy: 0.6662\n",
      "Epoch 47/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 1.1461 - accuracy: 0.6141 - val_loss: 1.0133 - val_accuracy: 0.6784\n",
      "Epoch 48/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.1190 - accuracy: 0.6080 - val_loss: 1.0009 - val_accuracy: 0.6692\n",
      "Epoch 49/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 1.1187 - accuracy: 0.6080 - val_loss: 0.9874 - val_accuracy: 0.6830\n",
      "Epoch 50/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.1206 - accuracy: 0.6279 - val_loss: 0.9816 - val_accuracy: 0.6723\n",
      "Epoch 51/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 1.0716 - accuracy: 0.6401 - val_loss: 0.9709 - val_accuracy: 0.6891\n",
      "Epoch 52/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.1073 - accuracy: 0.6095 - val_loss: 0.9563 - val_accuracy: 0.6907\n",
      "Epoch 53/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 1.0790 - accuracy: 0.6447 - val_loss: 0.9455 - val_accuracy: 0.6953\n",
      "Epoch 54/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.0426 - accuracy: 0.6462 - val_loss: 0.9375 - val_accuracy: 0.7090\n",
      "Epoch 55/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.0764 - accuracy: 0.6478 - val_loss: 0.9264 - val_accuracy: 0.7136\n",
      "Epoch 56/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.0437 - accuracy: 0.6554 - val_loss: 0.9119 - val_accuracy: 0.7213\n",
      "Epoch 57/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.0294 - accuracy: 0.6447 - val_loss: 0.9015 - val_accuracy: 0.7136\n",
      "Epoch 58/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.0463 - accuracy: 0.6493 - val_loss: 0.8938 - val_accuracy: 0.7228\n",
      "Epoch 59/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.0545 - accuracy: 0.6309 - val_loss: 0.8893 - val_accuracy: 0.7167\n",
      "Epoch 60/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.0146 - accuracy: 0.6585 - val_loss: 0.8749 - val_accuracy: 0.7228\n",
      "653/653 [==============================] - 0s 32us/sample - loss: 0.8749 - accuracy: 0.7228\n",
      "Val score is 0.7228177785873413\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 1s 2ms/sample - loss: 2.4815 - accuracy: 0.1838 - val_loss: 2.0737 - val_accuracy: 0.2251\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 1.9289 - accuracy: 0.3691 - val_loss: 1.8784 - val_accuracy: 0.4839\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 1.6202 - accuracy: 0.5391 - val_loss: 1.6772 - val_accuracy: 0.6080\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 1.4145 - accuracy: 0.6064 - val_loss: 1.4692 - val_accuracy: 0.6723\n",
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 1.2325 - accuracy: 0.6662 - val_loss: 1.2843 - val_accuracy: 0.7014\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 1.0914 - accuracy: 0.6861 - val_loss: 1.1133 - val_accuracy: 0.7243\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 0.9820 - accuracy: 0.7136 - val_loss: 0.9830 - val_accuracy: 0.7381\n",
      "Epoch 8/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 0.9057 - accuracy: 0.7198 - val_loss: 0.8836 - val_accuracy: 0.7427\n",
      "Epoch 9/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.8204 - accuracy: 0.7427 - val_loss: 0.8076 - val_accuracy: 0.7596\n",
      "Epoch 10/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 0.7724 - accuracy: 0.7534 - val_loss: 0.7477 - val_accuracy: 0.7642\n",
      "Epoch 11/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.7388 - accuracy: 0.7672 - val_loss: 0.7003 - val_accuracy: 0.7825\n",
      "Epoch 12/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 0.7410 - accuracy: 0.7489 - val_loss: 0.6617 - val_accuracy: 0.7871\n",
      "Epoch 13/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.6684 - accuracy: 0.7810 - val_loss: 0.6307 - val_accuracy: 0.7902\n",
      "Epoch 14/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 0.6239 - accuracy: 0.7979 - val_loss: 0.6032 - val_accuracy: 0.7994\n",
      "Epoch 15/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.6222 - accuracy: 0.7779 - val_loss: 0.5797 - val_accuracy: 0.8040\n",
      "Epoch 16/100\n",
      "653/653 [==============================] - 0s 104us/sample - loss: 0.6043 - accuracy: 0.7933 - val_loss: 0.5587 - val_accuracy: 0.8070\n",
      "Epoch 17/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 0.5800 - accuracy: 0.7963 - val_loss: 0.5389 - val_accuracy: 0.8162\n",
      "Epoch 18/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 0.5497 - accuracy: 0.8101 - val_loss: 0.5176 - val_accuracy: 0.8193\n",
      "Epoch 19/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 0.5652 - accuracy: 0.7994 - val_loss: 0.5008 - val_accuracy: 0.8300\n",
      "Epoch 20/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 0.5252 - accuracy: 0.8132 - val_loss: 0.4865 - val_accuracy: 0.8285\n",
      "Epoch 21/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 0.5269 - accuracy: 0.8116 - val_loss: 0.4673 - val_accuracy: 0.8361\n",
      "Epoch 22/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.5014 - accuracy: 0.8300 - val_loss: 0.4564 - val_accuracy: 0.8346\n",
      "Epoch 23/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 0.4834 - accuracy: 0.8331 - val_loss: 0.4425 - val_accuracy: 0.8438\n",
      "Epoch 24/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 0.4519 - accuracy: 0.8315 - val_loss: 0.4251 - val_accuracy: 0.8453\n",
      "Epoch 25/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 0.4924 - accuracy: 0.8224 - val_loss: 0.4146 - val_accuracy: 0.8453\n",
      "Epoch 26/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 0.4679 - accuracy: 0.8438 - val_loss: 0.4090 - val_accuracy: 0.8530\n",
      "Epoch 27/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 0.4522 - accuracy: 0.8392 - val_loss: 0.3954 - val_accuracy: 0.8606\n",
      "Epoch 28/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 0.4366 - accuracy: 0.8315 - val_loss: 0.3830 - val_accuracy: 0.8622\n",
      "Epoch 29/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.4192 - accuracy: 0.8469 - val_loss: 0.3738 - val_accuracy: 0.8637\n",
      "Epoch 30/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 0.4009 - accuracy: 0.8591 - val_loss: 0.3663 - val_accuracy: 0.8683\n",
      "Epoch 31/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 0.4046 - accuracy: 0.8622 - val_loss: 0.3555 - val_accuracy: 0.8683\n",
      "Epoch 32/100\n",
      "653/653 [==============================] - 0s 96us/sample - loss: 0.3849 - accuracy: 0.8560 - val_loss: 0.3403 - val_accuracy: 0.8851\n",
      "Epoch 33/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 0.4166 - accuracy: 0.8346 - val_loss: 0.3320 - val_accuracy: 0.8882\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 102us/sample - loss: 0.3690 - accuracy: 0.8729 - val_loss: 0.3238 - val_accuracy: 0.8928\n",
      "Epoch 35/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.3764 - accuracy: 0.8545 - val_loss: 0.3148 - val_accuracy: 0.8897\n",
      "Epoch 36/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.3602 - accuracy: 0.8668 - val_loss: 0.3062 - val_accuracy: 0.8989\n",
      "Epoch 37/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 0.3600 - accuracy: 0.8729 - val_loss: 0.2966 - val_accuracy: 0.9035\n",
      "Epoch 38/100\n",
      "653/653 [==============================] - 0s 96us/sample - loss: 0.3386 - accuracy: 0.8851 - val_loss: 0.2944 - val_accuracy: 0.9035\n",
      "Epoch 39/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 0.3418 - accuracy: 0.8790 - val_loss: 0.2840 - val_accuracy: 0.8928\n",
      "653/653 [==============================] - 0s 32us/sample - loss: 0.2840 - accuracy: 0.8928\n",
      "Val score is 0.8928024768829346\n",
      "Train on 653 samples, validate on 653 samples\n",
      "Epoch 1/100\n",
      "653/653 [==============================] - 1s 1ms/sample - loss: 180.4367 - mae: 180.4367 - val_loss: 180.2769 - val_mae: 180.2769\n",
      "Epoch 2/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 180.1759 - mae: 180.1759 - val_loss: 180.0648 - val_mae: 180.0648\n",
      "Epoch 3/100\n",
      "653/653 [==============================] - 0s 96us/sample - loss: 179.9378 - mae: 179.9378 - val_loss: 179.8704 - val_mae: 179.8704\n",
      "Epoch 4/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 179.7072 - mae: 179.7072 - val_loss: 179.6472 - val_mae: 179.6472\n",
      "Epoch 5/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 179.4516 - mae: 179.4516 - val_loss: 179.3837 - val_mae: 179.3837\n",
      "Epoch 6/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 179.1605 - mae: 179.1605 - val_loss: 179.0783 - val_mae: 179.0783\n",
      "Epoch 7/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 178.7753 - mae: 178.7753 - val_loss: 178.6694 - val_mae: 178.6693\n",
      "Epoch 8/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 178.3602 - mae: 178.3602 - val_loss: 178.1811 - val_mae: 178.1811\n",
      "Epoch 9/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 177.8535 - mae: 177.8535 - val_loss: 177.6387 - val_mae: 177.6387\n",
      "Epoch 10/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 177.2342 - mae: 177.2342 - val_loss: 176.9865 - val_mae: 176.9865\n",
      "Epoch 11/100\n",
      "653/653 [==============================] - 0s 95us/sample - loss: 176.5603 - mae: 176.5603 - val_loss: 176.2732 - val_mae: 176.2732\n",
      "Epoch 12/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 175.8885 - mae: 175.8885 - val_loss: 175.4324 - val_mae: 175.4324\n",
      "Epoch 13/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 174.9361 - mae: 174.9361 - val_loss: 174.3631 - val_mae: 174.3631\n",
      "Epoch 14/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 173.9585 - mae: 173.9585 - val_loss: 173.2821 - val_mae: 173.2821\n",
      "Epoch 15/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 172.8951 - mae: 172.8951 - val_loss: 172.0162 - val_mae: 172.0162\n",
      "Epoch 16/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 171.6706 - mae: 171.6706 - val_loss: 170.8638 - val_mae: 170.8638\n",
      "Epoch 17/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 170.2488 - mae: 170.2488 - val_loss: 169.4265 - val_mae: 169.4265\n",
      "Epoch 18/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 168.8314 - mae: 168.8314 - val_loss: 167.9143 - val_mae: 167.9143\n",
      "Epoch 19/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 167.4892 - mae: 167.4892 - val_loss: 166.3080 - val_mae: 166.3080\n",
      "Epoch 20/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 165.5283 - mae: 165.5283 - val_loss: 164.4640 - val_mae: 164.4639\n",
      "Epoch 21/100\n",
      "653/653 [==============================] - 0s 105us/sample - loss: 164.0199 - mae: 164.0200 - val_loss: 162.7117 - val_mae: 162.7117\n",
      "Epoch 22/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 162.1196 - mae: 162.1196 - val_loss: 160.8782 - val_mae: 160.8782\n",
      "Epoch 23/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 159.9287 - mae: 159.9287 - val_loss: 158.8369 - val_mae: 158.8369\n",
      "Epoch 24/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 158.3699 - mae: 158.3699 - val_loss: 156.8471 - val_mae: 156.8471\n",
      "Epoch 25/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 156.3003 - mae: 156.3003 - val_loss: 154.6788 - val_mae: 154.6788\n",
      "Epoch 26/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 154.2198 - mae: 154.2198 - val_loss: 152.9017 - val_mae: 152.9017\n",
      "Epoch 27/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 151.8498 - mae: 151.8498 - val_loss: 150.5867 - val_mae: 150.5867\n",
      "Epoch 28/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 149.6109 - mae: 149.6109 - val_loss: 147.3503 - val_mae: 147.3504\n",
      "Epoch 29/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 147.0687 - mae: 147.0687 - val_loss: 144.2595 - val_mae: 144.2595\n",
      "Epoch 30/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 144.9513 - mae: 144.9513 - val_loss: 141.6844 - val_mae: 141.6844\n",
      "Epoch 31/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 142.3320 - mae: 142.3320 - val_loss: 139.0685 - val_mae: 139.0685\n",
      "Epoch 32/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 139.8510 - mae: 139.8510 - val_loss: 135.0825 - val_mae: 135.0825\n",
      "Epoch 33/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 137.5107 - mae: 137.5107 - val_loss: 131.9350 - val_mae: 131.9350\n",
      "Epoch 34/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 134.7370 - mae: 134.7370 - val_loss: 129.2898 - val_mae: 129.2898\n",
      "Epoch 35/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 132.2487 - mae: 132.2487 - val_loss: 126.7179 - val_mae: 126.7179\n",
      "Epoch 36/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 129.9758 - mae: 129.9758 - val_loss: 124.4801 - val_mae: 124.4801\n",
      "Epoch 37/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 127.7080 - mae: 127.7080 - val_loss: 122.2078 - val_mae: 122.2078\n",
      "Epoch 38/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 125.4057 - mae: 125.4057 - val_loss: 119.6950 - val_mae: 119.6950\n",
      "Epoch 39/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 123.3959 - mae: 123.3960 - val_loss: 118.2686 - val_mae: 118.2686\n",
      "Epoch 40/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 121.1824 - mae: 121.1824 - val_loss: 115.6103 - val_mae: 115.6103\n",
      "Epoch 41/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 117.7358 - mae: 117.7358 - val_loss: 113.3542 - val_mae: 113.3542\n",
      "Epoch 42/100\n",
      "653/653 [==============================] - 0s 95us/sample - loss: 115.8348 - mae: 115.8348 - val_loss: 110.4888 - val_mae: 110.4888\n",
      "Epoch 43/100\n",
      "653/653 [==============================] - 0s 96us/sample - loss: 113.6633 - mae: 113.6633 - val_loss: 107.9548 - val_mae: 107.9548\n",
      "Epoch 44/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 111.3099 - mae: 111.3099 - val_loss: 107.1286 - val_mae: 107.1286\n",
      "Epoch 45/100\n",
      "653/653 [==============================] - 0s 95us/sample - loss: 109.5949 - mae: 109.5949 - val_loss: 105.1477 - val_mae: 105.1477\n",
      "Epoch 46/100\n",
      "653/653 [==============================] - 0s 107us/sample - loss: 107.6822 - mae: 107.6822 - val_loss: 103.0408 - val_mae: 103.0408\n",
      "Epoch 47/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 106.4380 - mae: 106.4380 - val_loss: 101.3355 - val_mae: 101.3355\n",
      "Epoch 48/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 104.6319 - mae: 104.6319 - val_loss: 99.8453 - val_mae: 99.8453\n",
      "Epoch 49/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 102.9118 - mae: 102.9118 - val_loss: 98.5569 - val_mae: 98.5569\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 96us/sample - loss: 100.9514 - mae: 100.9514 - val_loss: 96.2141 - val_mae: 96.2141\n",
      "Epoch 51/100\n",
      "653/653 [==============================] - 0s 98us/sample - loss: 99.9383 - mae: 99.9383 - val_loss: 94.9488 - val_mae: 94.9488\n",
      "Epoch 52/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 98.5676 - mae: 98.5676 - val_loss: 93.8956 - val_mae: 93.8956\n",
      "Epoch 53/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 95.6666 - mae: 95.6666 - val_loss: 93.0546 - val_mae: 93.0546\n",
      "Epoch 54/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 96.3855 - mae: 96.3855 - val_loss: 91.6342 - val_mae: 91.6342\n",
      "Epoch 55/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 94.0785 - mae: 94.0785 - val_loss: 90.0472 - val_mae: 90.0472\n",
      "Epoch 56/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 92.0550 - mae: 92.0550 - val_loss: 89.0396 - val_mae: 89.0396\n",
      "Epoch 57/100\n",
      "653/653 [==============================] - 0s 103us/sample - loss: 90.5377 - mae: 90.5377 - val_loss: 86.9554 - val_mae: 86.9554\n",
      "Epoch 58/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 89.4178 - mae: 89.4178 - val_loss: 86.7641 - val_mae: 86.7641\n",
      "Epoch 59/100\n",
      "653/653 [==============================] - 0s 97us/sample - loss: 89.4693 - mae: 89.4693 - val_loss: 86.7684 - val_mae: 86.7684\n",
      "Epoch 60/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 88.0377 - mae: 88.0377 - val_loss: 85.4185 - val_mae: 85.4185\n",
      "Epoch 61/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 88.1653 - mae: 88.1653 - val_loss: 82.0749 - val_mae: 82.0749\n",
      "Epoch 62/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 85.9332 - mae: 85.9332 - val_loss: 81.4797 - val_mae: 81.4797\n",
      "Epoch 63/100\n",
      "653/653 [==============================] - 0s 100us/sample - loss: 83.2509 - mae: 83.2509 - val_loss: 80.8147 - val_mae: 80.8148\n",
      "Epoch 64/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 82.4171 - mae: 82.4171 - val_loss: 80.2797 - val_mae: 80.2797\n",
      "Epoch 65/100\n",
      "653/653 [==============================] - 0s 99us/sample - loss: 82.9660 - mae: 82.9660 - val_loss: 79.4199 - val_mae: 79.4199\n",
      "Epoch 66/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 80.8719 - mae: 80.8719 - val_loss: 77.7190 - val_mae: 77.7190\n",
      "Epoch 67/100\n",
      "653/653 [==============================] - 0s 101us/sample - loss: 80.6933 - mae: 80.6933 - val_loss: 77.8314 - val_mae: 77.8314\n",
      "Epoch 68/100\n",
      "653/653 [==============================] - 0s 102us/sample - loss: 81.0246 - mae: 81.0246 - val_loss: 77.9122 - val_mae: 77.9121\n",
      "653/653 [==============================] - 0s 32us/sample - loss: 77.9122 - mae: 77.9121\n",
      "Val score is 77.91213989257812\n",
      "DataFrame: crx imput_method :MLP model :KNN score:  0.8\n",
      "DataFrame: crx imput_method :MLP model :XGB score:  0.87\n",
      "DataFrame: crx imput_method :MLP model :MLP score:  0.82\n",
      "DataFrame: dermatology imput_method :LOCF model :KNN score:  0.96\n",
      "DataFrame: dermatology imput_method :LOCF model :XGB score:  0.97\n",
      "DataFrame: dermatology imput_method :LOCF model :MLP score:  0.96\n",
      "DataFrame: dermatology imput_method :mean_mode model :KNN score:  0.96\n",
      "DataFrame: dermatology imput_method :mean_mode model :XGB score:  0.96\n",
      "DataFrame: dermatology imput_method :mean_mode model :MLP score:  0.96\n",
      "DataFrame: dermatology imput_method :knn model :KNN score:  0.95\n",
      "DataFrame: dermatology imput_method :knn model :XGB score:  0.96\n",
      "DataFrame: dermatology imput_method :knn model :MLP score:  0.96\n",
      "Best score is -12.389819600001104\n",
      "DataFrame: dermatology imput_method :trees model :KNN score:  0.95\n",
      "DataFrame: dermatology imput_method :trees model :XGB score:  0.96\n",
      "DataFrame: dermatology imput_method :trees model :MLP score:  0.96\n",
      "Train on 358 samples, validate on 358 samples\n",
      "Epoch 1/100\n",
      "358/358 [==============================] - 1s 3ms/sample - loss: 36.2459 - mae: 36.2459 - val_loss: 36.7806 - val_mae: 36.7806\n",
      "Epoch 2/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 36.1178 - mae: 36.1178 - val_loss: 36.5248 - val_mae: 36.5248\n",
      "Epoch 3/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 35.9696 - mae: 35.9696 - val_loss: 36.2578 - val_mae: 36.2578\n",
      "Epoch 4/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 35.7970 - mae: 35.7970 - val_loss: 35.9675 - val_mae: 35.9675\n",
      "Epoch 5/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 35.5965 - mae: 35.5965 - val_loss: 35.6415 - val_mae: 35.6415\n",
      "Epoch 6/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 35.3656 - mae: 35.3656 - val_loss: 35.3055 - val_mae: 35.3055\n",
      "Epoch 7/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 35.1027 - mae: 35.1027 - val_loss: 34.9328 - val_mae: 34.9328\n",
      "Epoch 8/100\n",
      "358/358 [==============================] - 0s 142us/sample - loss: 34.8064 - mae: 34.8064 - val_loss: 34.5229 - val_mae: 34.5229\n",
      "Epoch 9/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 34.4757 - mae: 34.4757 - val_loss: 34.0843 - val_mae: 34.0843\n",
      "Epoch 10/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 34.1126 - mae: 34.1126 - val_loss: 33.5443 - val_mae: 33.5443\n",
      "Epoch 11/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 33.7091 - mae: 33.7091 - val_loss: 32.9261 - val_mae: 32.9261\n",
      "Epoch 12/100\n",
      "358/358 [==============================] - 0s 150us/sample - loss: 33.2719 - mae: 33.2719 - val_loss: 32.3745 - val_mae: 32.3745\n",
      "Epoch 13/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 32.7977 - mae: 32.7977 - val_loss: 31.8421 - val_mae: 31.8421\n",
      "Epoch 14/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 32.2886 - mae: 32.2886 - val_loss: 31.2787 - val_mae: 31.2787\n",
      "Epoch 15/100\n",
      "358/358 [==============================] - 0s 149us/sample - loss: 31.7508 - mae: 31.7508 - val_loss: 30.6702 - val_mae: 30.6702\n",
      "Epoch 16/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 31.1696 - mae: 31.1696 - val_loss: 29.8954 - val_mae: 29.8954\n",
      "Epoch 17/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 30.5444 - mae: 30.5444 - val_loss: 29.2033 - val_mae: 29.2033\n",
      "Epoch 18/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 29.8968 - mae: 29.8967 - val_loss: 28.5517 - val_mae: 28.5517\n",
      "Epoch 19/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 29.1942 - mae: 29.1942 - val_loss: 27.9137 - val_mae: 27.9137\n",
      "Epoch 20/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 28.4638 - mae: 28.4638 - val_loss: 27.4075 - val_mae: 27.4075\n",
      "Epoch 21/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 27.6899 - mae: 27.6899 - val_loss: 26.7435 - val_mae: 26.7435\n",
      "Epoch 22/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 26.8889 - mae: 26.8889 - val_loss: 26.1811 - val_mae: 26.1811\n",
      "Epoch 23/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 26.0516 - mae: 26.0516 - val_loss: 25.2124 - val_mae: 25.2124\n",
      "Epoch 24/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 25.1610 - mae: 25.1610 - val_loss: 24.2414 - val_mae: 24.2414\n",
      "Epoch 25/100\n",
      "358/358 [==============================] - 0s 149us/sample - loss: 24.2408 - mae: 24.2408 - val_loss: 23.4275 - val_mae: 23.4275\n",
      "Epoch 26/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 23.3059 - mae: 23.3059 - val_loss: 22.6224 - val_mae: 22.6224\n",
      "Epoch 27/100\n",
      "358/358 [==============================] - 0s 154us/sample - loss: 22.3757 - mae: 22.3757 - val_loss: 21.2968 - val_mae: 21.2968\n",
      "Epoch 28/100\n",
      "358/358 [==============================] - 0s 150us/sample - loss: 21.3481 - mae: 21.3481 - val_loss: 20.2980 - val_mae: 20.2980\n",
      "Epoch 29/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 20.3171 - mae: 20.3171 - val_loss: 19.1677 - val_mae: 19.1677\n",
      "Epoch 30/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 19.3257 - mae: 19.3257 - val_loss: 18.1842 - val_mae: 18.1842\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358/358 [==============================] - 0s 148us/sample - loss: 18.2649 - mae: 18.2649 - val_loss: 17.2027 - val_mae: 17.2027\n",
      "Epoch 32/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 17.3483 - mae: 17.3483 - val_loss: 15.9826 - val_mae: 15.9826\n",
      "Epoch 33/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 16.3522 - mae: 16.3522 - val_loss: 15.1484 - val_mae: 15.1484\n",
      "Epoch 34/100\n",
      "358/358 [==============================] - 0s 140us/sample - loss: 15.4820 - mae: 15.4820 - val_loss: 14.1873 - val_mae: 14.1873\n",
      "Epoch 35/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 14.6008 - mae: 14.6008 - val_loss: 13.6774 - val_mae: 13.6774\n",
      "Epoch 36/100\n",
      "358/358 [==============================] - 0s 151us/sample - loss: 13.6978 - mae: 13.6978 - val_loss: 12.8265 - val_mae: 12.8265\n",
      "Epoch 37/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 13.1116 - mae: 13.1116 - val_loss: 11.9777 - val_mae: 11.9777\n",
      "Epoch 38/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 12.2334 - mae: 12.2334 - val_loss: 11.2551 - val_mae: 11.2551\n",
      "Epoch 39/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 11.6378 - mae: 11.6378 - val_loss: 10.6857 - val_mae: 10.6857\n",
      "Epoch 40/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 11.1594 - mae: 11.1594 - val_loss: 10.1658 - val_mae: 10.1658\n",
      "Epoch 41/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 10.7322 - mae: 10.7322 - val_loss: 9.7025 - val_mae: 9.7025\n",
      "Epoch 42/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 10.0987 - mae: 10.0987 - val_loss: 9.4160 - val_mae: 9.4160\n",
      "Epoch 43/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 10.0120 - mae: 10.0120 - val_loss: 9.2358 - val_mae: 9.2358\n",
      "Epoch 44/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 9.5249 - mae: 9.5249 - val_loss: 8.8178 - val_mae: 8.8178\n",
      "Epoch 45/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 9.2912 - mae: 9.2912 - val_loss: 8.5103 - val_mae: 8.5103\n",
      "Epoch 46/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 8.8819 - mae: 8.8819 - val_loss: 8.2543 - val_mae: 8.2543\n",
      "Epoch 47/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 8.6060 - mae: 8.6060 - val_loss: 8.0074 - val_mae: 8.0074\n",
      "Epoch 48/100\n",
      "358/358 [==============================] - 0s 143us/sample - loss: 8.4590 - mae: 8.4590 - val_loss: 7.8568 - val_mae: 7.8568\n",
      "Epoch 49/100\n",
      "358/358 [==============================] - 0s 162us/sample - loss: 8.0478 - mae: 8.0478 - val_loss: 7.6687 - val_mae: 7.6687\n",
      "Epoch 50/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 7.8234 - mae: 7.8234 - val_loss: 7.4899 - val_mae: 7.4899\n",
      "Epoch 51/100\n",
      "358/358 [==============================] - 0s 144us/sample - loss: 7.9650 - mae: 7.9650 - val_loss: 7.2249 - val_mae: 7.2249\n",
      "Epoch 52/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 7.9843 - mae: 7.9843 - val_loss: 7.1170 - val_mae: 7.1170\n",
      "Epoch 53/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 7.7095 - mae: 7.7095 - val_loss: 7.0935 - val_mae: 7.0935\n",
      "Epoch 54/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 7.5930 - mae: 7.5930 - val_loss: 6.9989 - val_mae: 6.9989\n",
      "Epoch 55/100\n",
      "358/358 [==============================] - 0s 149us/sample - loss: 7.3125 - mae: 7.3125 - val_loss: 6.9066 - val_mae: 6.9066\n",
      "Epoch 56/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 7.2740 - mae: 7.2740 - val_loss: 6.7001 - val_mae: 6.7001\n",
      "Epoch 57/100\n",
      "358/358 [==============================] - 0s 141us/sample - loss: 7.0087 - mae: 7.0087 - val_loss: 6.7792 - val_mae: 6.7792\n",
      "Epoch 58/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 6.8698 - mae: 6.8698 - val_loss: 6.3998 - val_mae: 6.3998\n",
      "Epoch 59/100\n",
      "358/358 [==============================] - 0s 147us/sample - loss: 6.8456 - mae: 6.8456 - val_loss: 6.3329 - val_mae: 6.3329\n",
      "Epoch 60/100\n",
      "358/358 [==============================] - 0s 145us/sample - loss: 6.8998 - mae: 6.8998 - val_loss: 6.1387 - val_mae: 6.1387\n",
      "Epoch 61/100\n",
      "358/358 [==============================] - 0s 148us/sample - loss: 6.8689 - mae: 6.8689 - val_loss: 6.1884 - val_mae: 6.1884\n",
      "Epoch 62/100\n",
      "358/358 [==============================] - 0s 146us/sample - loss: 6.9889 - mae: 6.9889 - val_loss: 6.3283 - val_mae: 6.3283\n",
      "358/358 [==============================] - 0s 46us/sample - loss: 6.3283 - mae: 6.3283\n",
      "Val score is 6.328266143798828\n",
      "DataFrame: dermatology imput_method :MLP model :KNN score:  0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: dermatology imput_method :MLP model :XGB score:  0.96\n",
      "DataFrame: dermatology imput_method :MLP model :MLP score:  0.96\n",
      "DataFrame: hepatitis imput_method :LOCF model :KNN score:  0.84\n",
      "DataFrame: hepatitis imput_method :LOCF model :XGB score:  0.82\n",
      "DataFrame: hepatitis imput_method :LOCF model :MLP score:  0.84\n",
      "DataFrame: hepatitis imput_method :mean_mode model :KNN score:  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: hepatitis imput_method :mean_mode model :XGB score:  0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: hepatitis imput_method :mean_mode model :MLP score:  0.82\n",
      "DataFrame: hepatitis imput_method :knn model :KNN score:  0.86\n",
      "DataFrame: hepatitis imput_method :knn model :XGB score:  0.86\n",
      "DataFrame: hepatitis imput_method :knn model :MLP score:  0.83\n",
      "Best score is -0.48225771493801467\n",
      "Best score is -0.28622311439089826\n",
      "Best score is -0.2806745073479465\n",
      "Best score is -0.23251727585998716\n",
      "Best score is -0.26032337652785437\n",
      "Best score is -0.35544353723526\n",
      "Best score is -0.3190394679705302\n",
      "Best score is -0.34928334871927896\n",
      "Best score is -0.1530724314848582\n",
      "Best score is -0.17245520035425824\n",
      "Best score is -0.649642250495703\n",
      "Best score is -33.89738421969943\n",
      "Best score is -53.86636786541908\n",
      "Best score is -0.37936451948979294\n",
      "Best score is -17.386079247427165\n",
      "DataFrame: hepatitis imput_method :trees model :KNN score:  0.86\n",
      "DataFrame: hepatitis imput_method :trees model :XGB score:  0.84\n",
      "DataFrame: hepatitis imput_method :trees model :MLP score:  0.83\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 19ms/sample - loss: 1.7354 - mae: 1.7354 - val_loss: 1.8242 - val_mae: 1.8242\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 55us/sample - loss: 1.6867 - mae: 1.6867 - val_loss: 1.7780 - val_mae: 1.7780\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5517 - mae: 1.5517 - val_loss: 1.7303 - val_mae: 1.7303\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4942 - mae: 1.4942 - val_loss: 1.6925 - val_mae: 1.6925\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.4306 - mae: 1.4306 - val_loss: 1.6523 - val_mae: 1.6523\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3899 - mae: 1.3899 - val_loss: 1.6186 - val_mae: 1.6186\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.3735 - mae: 1.3735 - val_loss: 1.5845 - val_mae: 1.5845\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3463 - mae: 1.3463 - val_loss: 1.5477 - val_mae: 1.5477\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3216 - mae: 1.3216 - val_loss: 1.5115 - val_mae: 1.5115\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.2826 - mae: 1.2826 - val_loss: 1.4728 - val_mae: 1.4728\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2598 - mae: 1.2598 - val_loss: 1.4350 - val_mae: 1.4350\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.2367 - mae: 1.2367 - val_loss: 1.4032 - val_mae: 1.4032\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 336us/sample - loss: 1.2032 - mae: 1.2032 - val_loss: 1.3688 - val_mae: 1.3688\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.1760 - mae: 1.1760 - val_loss: 1.3362 - val_mae: 1.3362\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.1507 - mae: 1.1507 - val_loss: 1.3043 - val_mae: 1.3043\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1136 - mae: 1.1136 - val_loss: 1.2708 - val_mae: 1.2708\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0956 - mae: 1.0956 - val_loss: 1.2345 - val_mae: 1.2345\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0712 - mae: 1.0712 - val_loss: 1.1994 - val_mae: 1.1994\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0315 - mae: 1.0315 - val_loss: 1.1643 - val_mae: 1.1643\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0050 - mae: 1.0050 - val_loss: 1.1327 - val_mae: 1.1327\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9693 - mae: 0.9693 - val_loss: 1.1019 - val_mae: 1.1019\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9288 - mae: 0.9288 - val_loss: 1.0674 - val_mae: 1.0674\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9298 - mae: 0.9298 - val_loss: 1.0321 - val_mae: 1.0321\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 202us/sample - loss: 0.8808 - mae: 0.8808 - val_loss: 0.9913 - val_mae: 0.9913\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 432us/sample - loss: 0.8700 - mae: 0.8700 - val_loss: 0.9460 - val_mae: 0.9460\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8152 - mae: 0.8152 - val_loss: 0.8983 - val_mae: 0.8983\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7711 - mae: 0.7711 - val_loss: 0.8535 - val_mae: 0.8535\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.7463 - mae: 0.7463 - val_loss: 0.8087 - val_mae: 0.8087\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.7086 - mae: 0.7086 - val_loss: 0.7608 - val_mae: 0.7608\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.6973 - mae: 0.6973 - val_loss: 0.7192 - val_mae: 0.7192\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6618 - mae: 0.6618 - val_loss: 0.6817 - val_mae: 0.6817\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6048 - mae: 0.6048 - val_loss: 0.6440 - val_mae: 0.6440\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5902 - mae: 0.5902 - val_loss: 0.6114 - val_mae: 0.6114\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.5446 - mae: 0.5446 - val_loss: 0.5878 - val_mae: 0.5878\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5262 - mae: 0.5262 - val_loss: 0.5691 - val_mae: 0.5691\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.4968 - mae: 0.4968 - val_loss: 0.5402 - val_mae: 0.5402\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.4469 - mae: 0.4469 - val_loss: 0.5015 - val_mae: 0.5015\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4617 - mae: 0.4617 - val_loss: 0.4610 - val_mae: 0.4610\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4409 - mae: 0.4409 - val_loss: 0.4268 - val_mae: 0.4268\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.3998 - mae: 0.3998 - val_loss: 0.4036 - val_mae: 0.4036\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3817 - mae: 0.3817 - val_loss: 0.3996 - val_mae: 0.3996\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.3726 - mae: 0.3726 - val_loss: 0.3883 - val_mae: 0.3883\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4050 - mae: 0.4050 - val_loss: 0.3883 - val_mae: 0.3883\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3298 - mae: 0.3298 - val_loss: 0.3798 - val_mae: 0.3798\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.3163 - mae: 0.3163 - val_loss: 0.3413 - val_mae: 0.3413\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3554 - mae: 0.3554 - val_loss: 0.3162 - val_mae: 0.3162\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2774 - mae: 0.2774 - val_loss: 0.3045 - val_mae: 0.3045\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.2946 - mae: 0.2946 - val_loss: 0.3165 - val_mae: 0.3165\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3376 - mae: 0.3376 - val_loss: 0.3295 - val_mae: 0.3295\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.3295 - mae: 0.3295\n",
      "Val score is 0.3295357823371887\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 15ms/sample - loss: 1.5316 - mae: 1.5316 - val_loss: 1.2158 - val_mae: 1.2158\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 1.3722 - mae: 1.3722 - val_loss: 1.1888 - val_mae: 1.1888\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 259us/sample - loss: 1.3646 - mae: 1.3646 - val_loss: 1.1671 - val_mae: 1.1671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 1.3123 - mae: 1.3123 - val_loss: 1.1455 - val_mae: 1.1455\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 1.2525 - mae: 1.2525 - val_loss: 1.1240 - val_mae: 1.1240\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 256us/sample - loss: 1.2165 - mae: 1.2165 - val_loss: 1.0953 - val_mae: 1.0953\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 1.2030 - mae: 1.2030 - val_loss: 1.0705 - val_mae: 1.0705\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1530 - mae: 1.1530 - val_loss: 1.0504 - val_mae: 1.0504\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.1308 - mae: 1.1308 - val_loss: 1.0317 - val_mae: 1.0317\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1008 - mae: 1.1008 - val_loss: 1.0173 - val_mae: 1.0173\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0680 - mae: 1.0680 - val_loss: 1.0038 - val_mae: 1.0038\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0592 - mae: 1.0592 - val_loss: 0.9894 - val_mae: 0.9894\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 298us/sample - loss: 1.0073 - mae: 1.0073 - val_loss: 0.9707 - val_mae: 0.9707\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 198us/sample - loss: 0.9938 - mae: 0.9938 - val_loss: 0.9484 - val_mae: 0.9484\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9690 - mae: 0.9690 - val_loss: 0.9251 - val_mae: 0.9251\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.9296 - mae: 0.9296 - val_loss: 0.9017 - val_mae: 0.9017\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9030 - mae: 0.9030 - val_loss: 0.8795 - val_mae: 0.8795\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9044 - mae: 0.9044 - val_loss: 0.8587 - val_mae: 0.8587\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.8917 - mae: 0.8917 - val_loss: 0.8398 - val_mae: 0.8398\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8501 - mae: 0.8501 - val_loss: 0.8254 - val_mae: 0.8254\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8281 - mae: 0.8281 - val_loss: 0.8097 - val_mae: 0.8097\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8089 - mae: 0.8089 - val_loss: 0.7900 - val_mae: 0.7900\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7508 - mae: 0.7508 - val_loss: 0.7725 - val_mae: 0.7725\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7405 - mae: 0.7405 - val_loss: 0.7519 - val_mae: 0.7519\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 290us/sample - loss: 0.7001 - mae: 0.7001 - val_loss: 0.7309 - val_mae: 0.7309\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6601 - mae: 0.6601 - val_loss: 0.7079 - val_mae: 0.7079\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6493 - mae: 0.6493 - val_loss: 0.6854 - val_mae: 0.6854\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.6062 - mae: 0.6062 - val_loss: 0.6611 - val_mae: 0.6611\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 270us/sample - loss: 0.6017 - mae: 0.6017 - val_loss: 0.6336 - val_mae: 0.6336\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.5756 - mae: 0.5756 - val_loss: 0.6054 - val_mae: 0.6054\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.5496 - mae: 0.5496 - val_loss: 0.5781 - val_mae: 0.5781\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 254us/sample - loss: 0.5457 - mae: 0.5457 - val_loss: 0.5491 - val_mae: 0.5491\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 255us/sample - loss: 0.4882 - mae: 0.4882 - val_loss: 0.5208 - val_mae: 0.5208\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4431 - mae: 0.4431 - val_loss: 0.4989 - val_mae: 0.4989\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4354 - mae: 0.4354 - val_loss: 0.4802 - val_mae: 0.4802\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.4394 - mae: 0.4394 - val_loss: 0.4702 - val_mae: 0.4702\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4254 - mae: 0.4254 - val_loss: 0.4568 - val_mae: 0.4568\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 263us/sample - loss: 0.3744 - mae: 0.3744 - val_loss: 0.4377 - val_mae: 0.4377\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3861 - mae: 0.3861 - val_loss: 0.4146 - val_mae: 0.4146\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3159 - mae: 0.3159 - val_loss: 0.3901 - val_mae: 0.3901\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 0.3250 - mae: 0.3250 - val_loss: 0.3724 - val_mae: 0.3724\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.3135 - mae: 0.3135 - val_loss: 0.3649 - val_mae: 0.3649\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.3370 - mae: 0.3370 - val_loss: 0.3650 - val_mae: 0.3650\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.3090 - mae: 0.3090 - val_loss: 0.3550 - val_mae: 0.3550\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 237us/sample - loss: 0.3056 - mae: 0.3056 - val_loss: 0.3326 - val_mae: 0.3326\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 253us/sample - loss: 0.2720 - mae: 0.2720 - val_loss: 0.3081 - val_mae: 0.3081\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 56us/sample - loss: 0.2635 - mae: 0.2635 - val_loss: 0.2995 - val_mae: 0.2995\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3219 - mae: 0.3219 - val_loss: 0.2948 - val_mae: 0.2948\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2672 - mae: 0.2672 - val_loss: 0.2975 - val_mae: 0.2975\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 398us/sample - loss: 0.2664 - mae: 0.2664 - val_loss: 0.2957 - val_mae: 0.2957\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 0.2957 - mae: 0.2957\n",
      "Val score is 0.2956923842430115\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 1.7389 - mae: 1.7389 - val_loss: 2.0456 - val_mae: 2.0456\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 61us/sample - loss: 1.6771 - mae: 1.6771 - val_loss: 1.9457 - val_mae: 1.9457\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5931 - mae: 1.5931 - val_loss: 1.8471 - val_mae: 1.8471\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5965 - mae: 1.5965 - val_loss: 1.7624 - val_mae: 1.7624\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5083 - mae: 1.5083 - val_loss: 1.6874 - val_mae: 1.6874\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4712 - mae: 1.4712 - val_loss: 1.6168 - val_mae: 1.6168\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4224 - mae: 1.4224 - val_loss: 1.5516 - val_mae: 1.5516\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3999 - mae: 1.3999 - val_loss: 1.4927 - val_mae: 1.4927\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.3662 - mae: 1.3662 - val_loss: 1.4433 - val_mae: 1.4433\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3433 - mae: 1.3433 - val_loss: 1.3941 - val_mae: 1.3941\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2964 - mae: 1.2964 - val_loss: 1.3407 - val_mae: 1.3407\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.2757 - mae: 1.2757 - val_loss: 1.2886 - val_mae: 1.2886\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2454 - mae: 1.2454 - val_loss: 1.2348 - val_mae: 1.2348\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2111 - mae: 1.2111 - val_loss: 1.1801 - val_mae: 1.1801\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.1825 - mae: 1.1825 - val_loss: 1.1324 - val_mae: 1.1324\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1403 - mae: 1.1403 - val_loss: 1.0852 - val_mae: 1.0852\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1125 - mae: 1.1125 - val_loss: 1.0371 - val_mae: 1.0371\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0672 - mae: 1.0672 - val_loss: 0.9881 - val_mae: 0.9881\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0484 - mae: 1.0484 - val_loss: 0.9371 - val_mae: 0.9371\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 0.9997 - mae: 0.9997 - val_loss: 0.8865 - val_mae: 0.8865\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.9743 - mae: 0.9743 - val_loss: 0.8384 - val_mae: 0.8384\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 259us/sample - loss: 0.9353 - mae: 0.9353 - val_loss: 0.7983 - val_mae: 0.7983\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 271us/sample - loss: 0.8862 - mae: 0.8862 - val_loss: 0.7609 - val_mae: 0.7609\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.8614 - mae: 0.8614 - val_loss: 0.7155 - val_mae: 0.7155\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.8185 - mae: 0.8185 - val_loss: 0.6680 - val_mae: 0.6680\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.7727 - mae: 0.7727 - val_loss: 0.6250 - val_mae: 0.6250\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.7491 - mae: 0.7491 - val_loss: 0.5793 - val_mae: 0.5793\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.7095 - mae: 0.7095 - val_loss: 0.5351 - val_mae: 0.5351\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6999 - mae: 0.6999 - val_loss: 0.4932 - val_mae: 0.4932\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.6843 - mae: 0.6843 - val_loss: 0.4556 - val_mae: 0.4556\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.6168 - mae: 0.6168 - val_loss: 0.4182 - val_mae: 0.4182\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.5826 - mae: 0.5826 - val_loss: 0.3826 - val_mae: 0.3826\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 269us/sample - loss: 0.5715 - mae: 0.5715 - val_loss: 0.3524 - val_mae: 0.3524\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 250us/sample - loss: 0.4801 - mae: 0.4801 - val_loss: 0.3287 - val_mae: 0.3287\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 3ms/sample - loss: 0.4984 - mae: 0.4984 - val_loss: 0.3068 - val_mae: 0.3068\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 297us/sample - loss: 0.4450 - mae: 0.4450 - val_loss: 0.2925 - val_mae: 0.2925\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 262us/sample - loss: 0.4530 - mae: 0.4530 - val_loss: 0.2766 - val_mae: 0.2766\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 0.3786 - mae: 0.3786 - val_loss: 0.2633 - val_mae: 0.2633\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 261us/sample - loss: 0.4041 - mae: 0.4041 - val_loss: 0.2530 - val_mae: 0.2530\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 275us/sample - loss: 0.4144 - mae: 0.4144 - val_loss: 0.2553 - val_mae: 0.2553\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 268us/sample - loss: 0.3923 - mae: 0.3923 - val_loss: 0.2564 - val_mae: 0.2564\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2564 - mae: 0.2564\n",
      "Val score is 0.25640252232551575\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 15ms/sample - loss: 2.2721 - mae: 2.2721 - val_loss: 2.0866 - val_mae: 2.0866\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 249us/sample - loss: 1.9354 - mae: 1.9354 - val_loss: 2.0491 - val_mae: 2.0491\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.8551 - mae: 1.8551 - val_loss: 2.0088 - val_mae: 2.0088\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7665 - mae: 1.7665 - val_loss: 1.9669 - val_mae: 1.9669\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.7266 - mae: 1.7266 - val_loss: 1.9324 - val_mae: 1.9324\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6881 - mae: 1.6881 - val_loss: 1.9037 - val_mae: 1.9037\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6918 - mae: 1.6918 - val_loss: 1.8712 - val_mae: 1.8712\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6253 - mae: 1.6253 - val_loss: 1.8384 - val_mae: 1.8384\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5877 - mae: 1.5877 - val_loss: 1.8061 - val_mae: 1.8061\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5529 - mae: 1.5529 - val_loss: 1.7732 - val_mae: 1.7732\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5350 - mae: 1.5350 - val_loss: 1.7384 - val_mae: 1.7384\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.4882 - mae: 1.4882 - val_loss: 1.7018 - val_mae: 1.7018\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4526 - mae: 1.4526 - val_loss: 1.6668 - val_mae: 1.6668\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4341 - mae: 1.4341 - val_loss: 1.6305 - val_mae: 1.6305\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3809 - mae: 1.3809 - val_loss: 1.5925 - val_mae: 1.5925\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3617 - mae: 1.3617 - val_loss: 1.5475 - val_mae: 1.5475\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.3093 - mae: 1.3093 - val_loss: 1.5032 - val_mae: 1.5032\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2810 - mae: 1.2810 - val_loss: 1.4567 - val_mae: 1.4567\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2387 - mae: 1.2387 - val_loss: 1.4072 - val_mae: 1.4072\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.2018 - mae: 1.2018 - val_loss: 1.3593 - val_mae: 1.3593\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1578 - mae: 1.1578 - val_loss: 1.3128 - val_mae: 1.3128\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1368 - mae: 1.1368 - val_loss: 1.2636 - val_mae: 1.2636\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0787 - mae: 1.0787 - val_loss: 1.2140 - val_mae: 1.2140\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0770 - mae: 1.0770 - val_loss: 1.1614 - val_mae: 1.1614\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9936 - mae: 0.9936 - val_loss: 1.1060 - val_mae: 1.1060\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.9655 - mae: 0.9655 - val_loss: 1.0569 - val_mae: 1.0569\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9397 - mae: 0.9397 - val_loss: 1.0098 - val_mae: 1.0098\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8979 - mae: 0.8979 - val_loss: 0.9580 - val_mae: 0.9580\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.8420 - mae: 0.8420 - val_loss: 0.9152 - val_mae: 0.9152\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7894 - mae: 0.7894 - val_loss: 0.8694 - val_mae: 0.8694\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 391us/sample - loss: 0.7445 - mae: 0.7445 - val_loss: 0.8267 - val_mae: 0.8267\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 123us/sample - loss: 0.7730 - mae: 0.7730 - val_loss: 0.7846 - val_mae: 0.7846\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.7032 - mae: 0.7032 - val_loss: 0.7380 - val_mae: 0.7380\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6426 - mae: 0.6426 - val_loss: 0.6848 - val_mae: 0.6848\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5928 - mae: 0.5928 - val_loss: 0.6249 - val_mae: 0.6249\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5583 - mae: 0.5583 - val_loss: 0.5670 - val_mae: 0.5670\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5183 - mae: 0.5183 - val_loss: 0.5251 - val_mae: 0.5251\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.4983 - mae: 0.4983 - val_loss: 0.4909 - val_mae: 0.4909\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 0.5107 - mae: 0.5107 - val_loss: 0.4491 - val_mae: 0.4491\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4609 - mae: 0.4609 - val_loss: 0.4279 - val_mae: 0.4279\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.3830 - mae: 0.3830 - val_loss: 0.4072 - val_mae: 0.4072\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4120 - mae: 0.4120 - val_loss: 0.3752 - val_mae: 0.3752\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3783 - mae: 0.3783 - val_loss: 0.3443 - val_mae: 0.3443\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.3606 - mae: 0.3606 - val_loss: 0.3236 - val_mae: 0.3236\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2954 - mae: 0.2954 - val_loss: 0.2963 - val_mae: 0.2963\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3861 - mae: 0.3861 - val_loss: 0.2785 - val_mae: 0.2785\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3709 - mae: 0.3709 - val_loss: 0.2710 - val_mae: 0.2710\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3543 - mae: 0.3543 - val_loss: 0.2494 - val_mae: 0.2494\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3274 - mae: 0.3274 - val_loss: 0.2557 - val_mae: 0.2557\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3046 - mae: 0.3046 - val_loss: 0.2624 - val_mae: 0.2624\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.2624 - mae: 0.2624\n",
      "Val score is 0.26244157552719116\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 2.0556 - mae: 2.0556 - val_loss: 1.7809 - val_mae: 1.7809\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.8666 - mae: 1.8666 - val_loss: 1.7260 - val_mae: 1.7260\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7792 - mae: 1.7792 - val_loss: 1.6723 - val_mae: 1.6723\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 168us/sample - loss: 1.7490 - mae: 1.7490 - val_loss: 1.6191 - val_mae: 1.6191\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.7069 - mae: 1.7069 - val_loss: 1.5660 - val_mae: 1.5660\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 113us/sample - loss: 1.6759 - mae: 1.6759 - val_loss: 1.5155 - val_mae: 1.5155\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.6376 - mae: 1.6376 - val_loss: 1.4716 - val_mae: 1.4716\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6016 - mae: 1.6016 - val_loss: 1.4284 - val_mae: 1.4284\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5681 - mae: 1.5681 - val_loss: 1.3862 - val_mae: 1.3862\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.5340 - mae: 1.5340 - val_loss: 1.3467 - val_mae: 1.3467\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 81us/sample - loss: 1.5063 - mae: 1.5063 - val_loss: 1.3063 - val_mae: 1.3063\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 1.4646 - mae: 1.4646 - val_loss: 1.2652 - val_mae: 1.2652\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4292 - mae: 1.4292 - val_loss: 1.2246 - val_mae: 1.2246\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4020 - mae: 1.4020 - val_loss: 1.1835 - val_mae: 1.1835\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3610 - mae: 1.3610 - val_loss: 1.1423 - val_mae: 1.1423\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3318 - mae: 1.3318 - val_loss: 1.1007 - val_mae: 1.1007\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 69us/sample - loss: 1.3096 - mae: 1.3096 - val_loss: 1.0629 - val_mae: 1.0629\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2715 - mae: 1.2715 - val_loss: 1.0269 - val_mae: 1.0269\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2122 - mae: 1.2122 - val_loss: 0.9836 - val_mae: 0.9836\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1796 - mae: 1.1796 - val_loss: 0.9299 - val_mae: 0.9299\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1359 - mae: 1.1359 - val_loss: 0.8771 - val_mae: 0.8771\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0934 - mae: 1.0934 - val_loss: 0.8316 - val_mae: 0.8316\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 1.0633 - mae: 1.0633 - val_loss: 0.7845 - val_mae: 0.7845\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0432 - mae: 1.0432 - val_loss: 0.7366 - val_mae: 0.7366\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9656 - mae: 0.9656 - val_loss: 0.6886 - val_mae: 0.6886\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9287 - mae: 0.9287 - val_loss: 0.6462 - val_mae: 0.6462\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8920 - mae: 0.8920 - val_loss: 0.6136 - val_mae: 0.6136\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8563 - mae: 0.8563 - val_loss: 0.5809 - val_mae: 0.5809\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8207 - mae: 0.8207 - val_loss: 0.5461 - val_mae: 0.5461\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8158 - mae: 0.8158 - val_loss: 0.5054 - val_mae: 0.5054\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7116 - mae: 0.7116 - val_loss: 0.4680 - val_mae: 0.4680\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6812 - mae: 0.6812 - val_loss: 0.4469 - val_mae: 0.4469\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6825 - mae: 0.6825 - val_loss: 0.4302 - val_mae: 0.4302\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6142 - mae: 0.6142 - val_loss: 0.4129 - val_mae: 0.4129\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6198 - mae: 0.6198 - val_loss: 0.3978 - val_mae: 0.3978\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6301 - mae: 0.6301 - val_loss: 0.3794 - val_mae: 0.3794\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6182 - mae: 0.6182 - val_loss: 0.3538 - val_mae: 0.3538\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5012 - mae: 0.5012 - val_loss: 0.3274 - val_mae: 0.3274\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.4777 - mae: 0.4777 - val_loss: 0.3086 - val_mae: 0.3086\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5353 - mae: 0.5353 - val_loss: 0.2951 - val_mae: 0.2951\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4635 - mae: 0.4635 - val_loss: 0.2680 - val_mae: 0.2680\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4258 - mae: 0.4258 - val_loss: 0.2534 - val_mae: 0.2534\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4168 - mae: 0.4168 - val_loss: 0.2307 - val_mae: 0.2307\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4237 - mae: 0.4237 - val_loss: 0.2151 - val_mae: 0.2151\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3693 - mae: 0.3693 - val_loss: 0.2289 - val_mae: 0.2289\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3738 - mae: 0.3738 - val_loss: 0.2409 - val_mae: 0.2409\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.2409 - mae: 0.2409\n",
      "Val score is 0.24087801575660706\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 1.8308 - mae: 1.8308 - val_loss: 1.6790 - val_mae: 1.6790\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7847 - mae: 1.7847 - val_loss: 1.6397 - val_mae: 1.6397\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6253 - mae: 1.6253 - val_loss: 1.6124 - val_mae: 1.6124\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5247 - mae: 1.5247 - val_loss: 1.5822 - val_mae: 1.5822\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4141 - mae: 1.4141 - val_loss: 1.5508 - val_mae: 1.5508\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3936 - mae: 1.3936 - val_loss: 1.5226 - val_mae: 1.5226\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3661 - mae: 1.3661 - val_loss: 1.4949 - val_mae: 1.4949\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3247 - mae: 1.3247 - val_loss: 1.4664 - val_mae: 1.4664\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 140us/sample - loss: 1.2952 - mae: 1.2952 - val_loss: 1.4367 - val_mae: 1.4367\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 203us/sample - loss: 1.2622 - mae: 1.2622 - val_loss: 1.4039 - val_mae: 1.4039\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2382 - mae: 1.2382 - val_loss: 1.3678 - val_mae: 1.3678\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2116 - mae: 1.2116 - val_loss: 1.3341 - val_mae: 1.3341\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1993 - mae: 1.1993 - val_loss: 1.3049 - val_mae: 1.3049\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1399 - mae: 1.1399 - val_loss: 1.2750 - val_mae: 1.2750\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1345 - mae: 1.1345 - val_loss: 1.2414 - val_mae: 1.2414\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0824 - mae: 1.0824 - val_loss: 1.2067 - val_mae: 1.2067\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0554 - mae: 1.0554 - val_loss: 1.1731 - val_mae: 1.1731\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0197 - mae: 1.0197 - val_loss: 1.1374 - val_mae: 1.1374\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.9854 - mae: 0.9854 - val_loss: 1.1008 - val_mae: 1.1008\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9585 - mae: 0.9585 - val_loss: 1.0638 - val_mae: 1.0638\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.9689 - mae: 0.968 - 0s 195us/sample - loss: 0.9237 - mae: 0.9237 - val_loss: 1.0279 - val_mae: 1.0279\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8906 - mae: 0.8906 - val_loss: 0.9907 - val_mae: 0.9907\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8609 - mae: 0.8609 - val_loss: 0.9545 - val_mae: 0.9545\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8290 - mae: 0.8290 - val_loss: 0.9190 - val_mae: 0.9190\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7872 - mae: 0.7872 - val_loss: 0.8820 - val_mae: 0.8820\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7660 - mae: 0.7660 - val_loss: 0.8437 - val_mae: 0.8437\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7296 - mae: 0.7296 - val_loss: 0.8048 - val_mae: 0.8048\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6817 - mae: 0.6817 - val_loss: 0.7655 - val_mae: 0.7655\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6632 - mae: 0.6632 - val_loss: 0.7217 - val_mae: 0.7217\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5951 - mae: 0.5951 - val_loss: 0.6821 - val_mae: 0.6821\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6106 - mae: 0.6106 - val_loss: 0.6389 - val_mae: 0.6389\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5295 - mae: 0.5295 - val_loss: 0.6005 - val_mae: 0.6005\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4919 - mae: 0.4919 - val_loss: 0.5604 - val_mae: 0.5604\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4914 - mae: 0.4914 - val_loss: 0.5263 - val_mae: 0.5263\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4420 - mae: 0.4420 - val_loss: 0.5016 - val_mae: 0.5016\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4359 - mae: 0.4359 - val_loss: 0.4858 - val_mae: 0.4858\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4203 - mae: 0.4203 - val_loss: 0.4746 - val_mae: 0.4746\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4037 - mae: 0.4037 - val_loss: 0.4554 - val_mae: 0.4554\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3757 - mae: 0.3757 - val_loss: 0.4270 - val_mae: 0.4270\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3207 - mae: 0.3207 - val_loss: 0.3955 - val_mae: 0.3955\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3062 - mae: 0.3062 - val_loss: 0.3628 - val_mae: 0.3628\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3734 - mae: 0.3734 - val_loss: 0.3361 - val_mae: 0.3361\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3376 - mae: 0.3376 - val_loss: 0.3142 - val_mae: 0.3142\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3411 - mae: 0.3411 - val_loss: 0.3077 - val_mae: 0.3077\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2870 - mae: 0.2870 - val_loss: 0.3145 - val_mae: 0.3145\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2598 - mae: 0.2598 - val_loss: 0.3241 - val_mae: 0.3241\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.3241 - mae: 0.3241\n",
      "Val score is 0.32407790422439575\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 1.9028 - mae: 1.9028 - val_loss: 1.6614 - val_mae: 1.6614\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 100us/sample - loss: 1.7931 - mae: 1.7931 - val_loss: 1.6290 - val_mae: 1.6290\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7551 - mae: 1.7551 - val_loss: 1.5967 - val_mae: 1.5967\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7236 - mae: 1.7236 - val_loss: 1.5747 - val_mae: 1.5747\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6971 - mae: 1.6971 - val_loss: 1.5542 - val_mae: 1.5542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6727 - mae: 1.6727 - val_loss: 1.5322 - val_mae: 1.5322\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6449 - mae: 1.6449 - val_loss: 1.5114 - val_mae: 1.5114\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6141 - mae: 1.6141 - val_loss: 1.4913 - val_mae: 1.4913\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5871 - mae: 1.5871 - val_loss: 1.4690 - val_mae: 1.4690\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 1.5619 - mae: 1.5619 - val_loss: 1.4422 - val_mae: 1.4422\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5259 - mae: 1.5259 - val_loss: 1.4185 - val_mae: 1.4185\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4967 - mae: 1.4967 - val_loss: 1.3936 - val_mae: 1.3936\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 196us/sample - loss: 1.4721 - mae: 1.4721 - val_loss: 1.3705 - val_mae: 1.3705\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 191us/sample - loss: 1.4633 - mae: 1.4633 - val_loss: 1.3446 - val_mae: 1.3446\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 38us/sample - loss: 1.4071 - mae: 1.4071 - val_loss: 1.3166 - val_mae: 1.3166\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3752 - mae: 1.3752 - val_loss: 1.2875 - val_mae: 1.2875\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3503 - mae: 1.3503 - val_loss: 1.2590 - val_mae: 1.2590\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3059 - mae: 1.3059 - val_loss: 1.2302 - val_mae: 1.2302\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2791 - mae: 1.2791 - val_loss: 1.1910 - val_mae: 1.1910\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2360 - mae: 1.2360 - val_loss: 1.1469 - val_mae: 1.1469\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2025 - mae: 1.2025 - val_loss: 1.1030 - val_mae: 1.1030\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1613 - mae: 1.1613 - val_loss: 1.0573 - val_mae: 1.0573\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1344 - mae: 1.1344 - val_loss: 1.0097 - val_mae: 1.0097\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0852 - mae: 1.0852 - val_loss: 0.9544 - val_mae: 0.9544\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0458 - mae: 1.0458 - val_loss: 0.9057 - val_mae: 0.9057\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0275 - mae: 1.0275 - val_loss: 0.8693 - val_mae: 0.8693\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9708 - mae: 0.9708 - val_loss: 0.8433 - val_mae: 0.8433\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9563 - mae: 0.9563 - val_loss: 0.8113 - val_mae: 0.8113\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8953 - mae: 0.8953 - val_loss: 0.7759 - val_mae: 0.7759\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8799 - mae: 0.8799 - val_loss: 0.7356 - val_mae: 0.7356\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8136 - mae: 0.8136 - val_loss: 0.6873 - val_mae: 0.6873\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7662 - mae: 0.7662 - val_loss: 0.6369 - val_mae: 0.6369\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7401 - mae: 0.7401 - val_loss: 0.5936 - val_mae: 0.5936\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.6997 - mae: 0.6997 - val_loss: 0.5605 - val_mae: 0.5605\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6535 - mae: 0.6535 - val_loss: 0.5294 - val_mae: 0.5294\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5957 - mae: 0.5957 - val_loss: 0.4994 - val_mae: 0.4994\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5701 - mae: 0.5701 - val_loss: 0.4619 - val_mae: 0.4619\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5464 - mae: 0.5464 - val_loss: 0.4116 - val_mae: 0.4116\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4718 - mae: 0.4718 - val_loss: 0.3683 - val_mae: 0.3683\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5117 - mae: 0.5117 - val_loss: 0.3437 - val_mae: 0.3437\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4507 - mae: 0.4507 - val_loss: 0.3261 - val_mae: 0.3261\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3756 - mae: 0.3756 - val_loss: 0.2967 - val_mae: 0.2967\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4167 - mae: 0.4167 - val_loss: 0.2678 - val_mae: 0.2678\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4029 - mae: 0.4029 - val_loss: 0.2478 - val_mae: 0.2478\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3319 - mae: 0.3319 - val_loss: 0.2335 - val_mae: 0.2335\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3114 - mae: 0.3114 - val_loss: 0.2164 - val_mae: 0.2164\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3009 - mae: 0.3009 - val_loss: 0.1940 - val_mae: 0.1940\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3701 - mae: 0.3701 - val_loss: 0.1932 - val_mae: 0.1932\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2821 - mae: 0.2821 - val_loss: 0.1914 - val_mae: 0.1914\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3471 - mae: 0.3471 - val_loss: 0.1804 - val_mae: 0.1804\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3239 - mae: 0.3239 - val_loss: 0.1972 - val_mae: 0.1972\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2935 - mae: 0.2935 - val_loss: 0.1966 - val_mae: 0.1966\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.1966 - mae: 0.1966\n",
      "Val score is 0.19660359621047974\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 2.0637 - mae: 2.0637 - val_loss: 1.5973 - val_mae: 1.5973\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 115us/sample - loss: 1.8895 - mae: 1.8895 - val_loss: 1.5621 - val_mae: 1.5621\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7962 - mae: 1.7962 - val_loss: 1.5344 - val_mae: 1.5344\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7497 - mae: 1.7497 - val_loss: 1.5092 - val_mae: 1.5092\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6311 - mae: 1.6311 - val_loss: 1.4867 - val_mae: 1.4867\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5749 - mae: 1.5749 - val_loss: 1.4665 - val_mae: 1.4665\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5275 - mae: 1.5275 - val_loss: 1.4498 - val_mae: 1.4498\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5056 - mae: 1.5056 - val_loss: 1.4330 - val_mae: 1.4330\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4680 - mae: 1.4680 - val_loss: 1.4147 - val_mae: 1.4147\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4314 - mae: 1.4314 - val_loss: 1.3935 - val_mae: 1.3935\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 210us/sample - loss: 1.4180 - mae: 1.4180 - val_loss: 1.3710 - val_mae: 1.3710\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3715 - mae: 1.3715 - val_loss: 1.3495 - val_mae: 1.3495\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3345 - mae: 1.3345 - val_loss: 1.3273 - val_mae: 1.3273\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3048 - mae: 1.3048 - val_loss: 1.3033 - val_mae: 1.3033\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2746 - mae: 1.2746 - val_loss: 1.2808 - val_mae: 1.2808\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2464 - mae: 1.2464 - val_loss: 1.2576 - val_mae: 1.2576\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2336 - mae: 1.2336 - val_loss: 1.2305 - val_mae: 1.2305\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1917 - mae: 1.1917 - val_loss: 1.2019 - val_mae: 1.2019\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1498 - mae: 1.1498 - val_loss: 1.1725 - val_mae: 1.1725\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1365 - mae: 1.1365 - val_loss: 1.1438 - val_mae: 1.1438\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0880 - mae: 1.0880 - val_loss: 1.1110 - val_mae: 1.1110\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0798 - mae: 1.0798 - val_loss: 1.0812 - val_mae: 1.0812\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0246 - mae: 1.0246 - val_loss: 1.0591 - val_mae: 1.0591\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9936 - mae: 0.9936 - val_loss: 1.0333 - val_mae: 1.0333\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9737 - mae: 0.9737 - val_loss: 1.0041 - val_mae: 1.0041\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9330 - mae: 0.9330 - val_loss: 0.9703 - val_mae: 0.9703\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8832 - mae: 0.8832 - val_loss: 0.9333 - val_mae: 0.9333\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8721 - mae: 0.8721 - val_loss: 0.8972 - val_mae: 0.8972\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8542 - mae: 0.8542 - val_loss: 0.8554 - val_mae: 0.8554\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8235 - mae: 0.8235 - val_loss: 0.8149 - val_mae: 0.8149\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7739 - mae: 0.7739 - val_loss: 0.7761 - val_mae: 0.7761\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7232 - mae: 0.7232 - val_loss: 0.7356 - val_mae: 0.7356\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6919 - mae: 0.6919 - val_loss: 0.6985 - val_mae: 0.6985\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6644 - mae: 0.6644 - val_loss: 0.6680 - val_mae: 0.6680\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5936 - mae: 0.5936 - val_loss: 0.6418 - val_mae: 0.6418\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5801 - mae: 0.5801 - val_loss: 0.6160 - val_mae: 0.6160\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5531 - mae: 0.5531 - val_loss: 0.5826 - val_mae: 0.5826\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5189 - mae: 0.5189 - val_loss: 0.5467 - val_mae: 0.5467\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4943 - mae: 0.4943 - val_loss: 0.5140 - val_mae: 0.5140\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5105 - mae: 0.5105 - val_loss: 0.4847 - val_mae: 0.4847\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4850 - mae: 0.4850 - val_loss: 0.4425 - val_mae: 0.4425\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4430 - mae: 0.4430 - val_loss: 0.3968 - val_mae: 0.3968\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3970 - mae: 0.3970 - val_loss: 0.3692 - val_mae: 0.3692\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3801 - mae: 0.3801 - val_loss: 0.3614 - val_mae: 0.3614\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3962 - mae: 0.3962 - val_loss: 0.3424 - val_mae: 0.3424\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3232 - mae: 0.3232 - val_loss: 0.3215 - val_mae: 0.3215\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 0.3096 - mae: 0.3096 - val_loss: 0.2939 - val_mae: 0.2939\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3264 - mae: 0.3264 - val_loss: 0.2843 - val_mae: 0.2843\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3063 - mae: 0.3063 - val_loss: 0.2758 - val_mae: 0.2758\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3048 - mae: 0.3048 - val_loss: 0.2733 - val_mae: 0.2733\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2824 - mae: 0.2824 - val_loss: 0.2595 - val_mae: 0.2595\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2682 - mae: 0.2682 - val_loss: 0.2285 - val_mae: 0.2285\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2988 - mae: 0.2988 - val_loss: 0.2095 - val_mae: 0.2095\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2904 - mae: 0.2904 - val_loss: 0.2171 - val_mae: 0.2171\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2700 - mae: 0.2700 - val_loss: 0.2294 - val_mae: 0.2294\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2294 - mae: 0.2294\n",
      "Val score is 0.22938665747642517\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 1.8748 - mae: 1.8748 - val_loss: 1.9031 - val_mae: 1.9031\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 181us/sample - loss: 1.8260 - mae: 1.8260 - val_loss: 1.8426 - val_mae: 1.8426\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7914 - mae: 1.7914 - val_loss: 1.7910 - val_mae: 1.7910\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7706 - mae: 1.7706 - val_loss: 1.7443 - val_mae: 1.7443\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7409 - mae: 1.7409 - val_loss: 1.6924 - val_mae: 1.6924\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7127 - mae: 1.7127 - val_loss: 1.6421 - val_mae: 1.6421\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6861 - mae: 1.6861 - val_loss: 1.5953 - val_mae: 1.5953\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6658 - mae: 1.6658 - val_loss: 1.5557 - val_mae: 1.5557\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6488 - mae: 1.6488 - val_loss: 1.5242 - val_mae: 1.5242\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 1.6130 - mae: 1.6130 - val_loss: 1.4969 - val_mae: 1.4969\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5771 - mae: 1.5771 - val_loss: 1.4686 - val_mae: 1.4686\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5505 - mae: 1.5505 - val_loss: 1.4336 - val_mae: 1.4336\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5200 - mae: 1.5200 - val_loss: 1.4000 - val_mae: 1.4000\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4900 - mae: 1.4900 - val_loss: 1.3670 - val_mae: 1.3670\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4606 - mae: 1.4606 - val_loss: 1.3332 - val_mae: 1.3332\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4415 - mae: 1.4415 - val_loss: 1.2966 - val_mae: 1.2966\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3998 - mae: 1.3998 - val_loss: 1.2596 - val_mae: 1.2596\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3641 - mae: 1.3641 - val_loss: 1.2212 - val_mae: 1.2212\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3309 - mae: 1.3309 - val_loss: 1.1821 - val_mae: 1.1821\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2988 - mae: 1.2988 - val_loss: 1.1443 - val_mae: 1.1443\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2622 - mae: 1.2622 - val_loss: 1.1071 - val_mae: 1.1071\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2438 - mae: 1.2438 - val_loss: 1.0731 - val_mae: 1.0731\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1873 - mae: 1.1873 - val_loss: 1.0369 - val_mae: 1.0369\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1657 - mae: 1.1657 - val_loss: 0.9987 - val_mae: 0.9987\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1197 - mae: 1.1197 - val_loss: 0.9523 - val_mae: 0.9523\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0748 - mae: 1.0748 - val_loss: 0.8983 - val_mae: 0.8983\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0331 - mae: 1.0331 - val_loss: 0.8383 - val_mae: 0.8383\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 248us/sample - loss: 0.9905 - mae: 0.9905 - val_loss: 0.7833 - val_mae: 0.7833\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9660 - mae: 0.9660 - val_loss: 0.7315 - val_mae: 0.7315\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9089 - mae: 0.9089 - val_loss: 0.6858 - val_mae: 0.6858\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8918 - mae: 0.8918 - val_loss: 0.6408 - val_mae: 0.6408\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8294 - mae: 0.8294 - val_loss: 0.5949 - val_mae: 0.5949\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7835 - mae: 0.7835 - val_loss: 0.5511 - val_mae: 0.5511\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7633 - mae: 0.7633 - val_loss: 0.5250 - val_mae: 0.5250\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7010 - mae: 0.7010 - val_loss: 0.4986 - val_mae: 0.4986\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6743 - mae: 0.6743 - val_loss: 0.4779 - val_mae: 0.4779\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6310 - mae: 0.6310 - val_loss: 0.4533 - val_mae: 0.4533\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5484 - mae: 0.5484 - val_loss: 0.4334 - val_mae: 0.4334\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5365 - mae: 0.5365 - val_loss: 0.3958 - val_mae: 0.3958\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5269 - mae: 0.5269 - val_loss: 0.3536 - val_mae: 0.3536\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4774 - mae: 0.4774 - val_loss: 0.3120 - val_mae: 0.3120\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4136 - mae: 0.4136 - val_loss: 0.2827 - val_mae: 0.2827\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4220 - mae: 0.4220 - val_loss: 0.2594 - val_mae: 0.2594\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3576 - mae: 0.3576 - val_loss: 0.2318 - val_mae: 0.2318\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2932 - mae: 0.2932 - val_loss: 0.2073 - val_mae: 0.2073\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3615 - mae: 0.3615 - val_loss: 0.1967 - val_mae: 0.1967\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3672 - mae: 0.3672 - val_loss: 0.1811 - val_mae: 0.1811\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3767 - mae: 0.3767 - val_loss: 0.1859 - val_mae: 0.1859\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3096 - mae: 0.3096 - val_loss: 0.1941 - val_mae: 0.1941\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.1941 - mae: 0.1941\n",
      "Val score is 0.19411267340183258\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 14ms/sample - loss: 2.0217 - mae: 2.0217 - val_loss: 2.9176 - val_mae: 2.9176\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 50us/sample - loss: 1.8830 - mae: 1.8830 - val_loss: 2.8229 - val_mae: 2.8229\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.8280 - mae: 1.8280 - val_loss: 2.7328 - val_mae: 2.7328\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7931 - mae: 1.7931 - val_loss: 2.6467 - val_mae: 2.6467\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7444 - mae: 1.7444 - val_loss: 2.5648 - val_mae: 2.5648\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7186 - mae: 1.7186 - val_loss: 2.4877 - val_mae: 2.4877\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 13us/sample - loss: 1.6830 - mae: 1.6830 - val_loss: 2.4170 - val_mae: 2.4170\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 1.6495 - mae: 1.6495 - val_loss: 2.3534 - val_mae: 2.3534\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.6174 - mae: 1.6174 - val_loss: 2.2924 - val_mae: 2.2924\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5848 - mae: 1.5848 - val_loss: 2.2348 - val_mae: 2.2348\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5601 - mae: 1.5601 - val_loss: 2.1759 - val_mae: 2.1759\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5183 - mae: 1.5183 - val_loss: 2.1183 - val_mae: 2.1183\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4843 - mae: 1.4843 - val_loss: 2.0628 - val_mae: 2.0628\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4594 - mae: 1.4594 - val_loss: 2.0047 - val_mae: 2.0047\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4293 - mae: 1.4293 - val_loss: 1.9406 - val_mae: 1.9406\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3836 - mae: 1.3836 - val_loss: 1.8712 - val_mae: 1.8712\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3499 - mae: 1.3499 - val_loss: 1.8038 - val_mae: 1.8038\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3269 - mae: 1.3269 - val_loss: 1.7403 - val_mae: 1.7403\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2803 - mae: 1.2803 - val_loss: 1.6762 - val_mae: 1.6762\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2433 - mae: 1.2433 - val_loss: 1.6136 - val_mae: 1.6136\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2107 - mae: 1.2107 - val_loss: 1.5478 - val_mae: 1.5478\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1674 - mae: 1.1674 - val_loss: 1.4838 - val_mae: 1.4838\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1430 - mae: 1.1430 - val_loss: 1.4258 - val_mae: 1.4258\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1197 - mae: 1.1197 - val_loss: 1.3719 - val_mae: 1.3719\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0822 - mae: 1.0822 - val_loss: 1.3200 - val_mae: 1.3200\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0135 - mae: 1.0135 - val_loss: 1.2683 - val_mae: 1.2683\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0523 - mae: 1.0523 - val_loss: 1.2192 - val_mae: 1.2192\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9638 - mae: 0.9638 - val_loss: 1.1760 - val_mae: 1.1760\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9663 - mae: 0.9663 - val_loss: 1.1260 - val_mae: 1.1260\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8885 - mae: 0.8885 - val_loss: 1.0686 - val_mae: 1.0686\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8456 - mae: 0.8456 - val_loss: 1.0051 - val_mae: 1.0051\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7756 - mae: 0.7756 - val_loss: 0.9372 - val_mae: 0.9372\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7553 - mae: 0.7553 - val_loss: 0.8731 - val_mae: 0.8731\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7006 - mae: 0.7006 - val_loss: 0.8073 - val_mae: 0.8073\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6660 - mae: 0.6660 - val_loss: 0.7404 - val_mae: 0.7404\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6420 - mae: 0.6420 - val_loss: 0.6797 - val_mae: 0.6797\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6068 - mae: 0.6068 - val_loss: 0.6149 - val_mae: 0.6149\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5906 - mae: 0.5906 - val_loss: 0.5603 - val_mae: 0.5603\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5755 - mae: 0.5755 - val_loss: 0.5147 - val_mae: 0.5147\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5201 - mae: 0.5201 - val_loss: 0.4768 - val_mae: 0.4768\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5084 - mae: 0.5084 - val_loss: 0.4427 - val_mae: 0.4427\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4718 - mae: 0.4718 - val_loss: 0.4184 - val_mae: 0.4184\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4162 - mae: 0.4162 - val_loss: 0.3964 - val_mae: 0.3964\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4342 - mae: 0.4342 - val_loss: 0.3577 - val_mae: 0.3577\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4278 - mae: 0.4278 - val_loss: 0.3245 - val_mae: 0.3245\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4193 - mae: 0.4193 - val_loss: 0.2888 - val_mae: 0.2888\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4033 - mae: 0.4033 - val_loss: 0.2495 - val_mae: 0.2495\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3708 - mae: 0.3708 - val_loss: 0.2188 - val_mae: 0.2188\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 163us/sample - loss: 0.3469 - mae: 0.3469 - val_loss: 0.2023 - val_mae: 0.2023\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 0.3453 - mae: 0.3453 - val_loss: 0.2054 - val_mae: 0.2054\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3284 - mae: 0.3284 - val_loss: 0.2126 - val_mae: 0.2126\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.2126 - mae: 0.2126\n",
      "Val score is 0.2125750035047531\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 1.4373 - mae: 1.4373 - val_loss: 1.1790 - val_mae: 1.1790\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 1.2592 - mae: 1.2592 - val_loss: 1.0916 - val_mae: 1.0916\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2634 - mae: 1.2634 - val_loss: 1.0258 - val_mae: 1.0258\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1889 - mae: 1.1889 - val_loss: 0.9829 - val_mae: 0.9829\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1289 - mae: 1.1289 - val_loss: 0.9456 - val_mae: 0.9456\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 360us/sample - loss: 1.1185 - mae: 1.1185 - val_loss: 0.9128 - val_mae: 0.9128\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 51us/sample - loss: 1.0742 - mae: 1.0742 - val_loss: 0.8826 - val_mae: 0.8826\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0380 - mae: 1.0380 - val_loss: 0.8556 - val_mae: 0.8556\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9887 - mae: 0.9887 - val_loss: 0.8363 - val_mae: 0.8363\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9497 - mae: 0.9497 - val_loss: 0.8114 - val_mae: 0.8114\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9534 - mae: 0.9534 - val_loss: 0.7844 - val_mae: 0.7844\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9017 - mae: 0.9017 - val_loss: 0.7533 - val_mae: 0.7533\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8626 - mae: 0.8626 - val_loss: 0.7197 - val_mae: 0.7197\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8786 - mae: 0.8786 - val_loss: 0.6835 - val_mae: 0.6835\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.8447 - mae: 0.8447 - val_loss: 0.6488 - val_mae: 0.6488\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.7845 - mae: 0.7845 - val_loss: 0.6199 - val_mae: 0.6199\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7685 - mae: 0.7685 - val_loss: 0.6027 - val_mae: 0.6027\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7369 - mae: 0.7369 - val_loss: 0.5886 - val_mae: 0.5886\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7337 - mae: 0.7337 - val_loss: 0.5693 - val_mae: 0.5693\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6573 - mae: 0.6573 - val_loss: 0.5466 - val_mae: 0.5466\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6295 - mae: 0.6295 - val_loss: 0.5189 - val_mae: 0.5189\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6262 - mae: 0.6262 - val_loss: 0.4982 - val_mae: 0.4982\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.6218 - mae: 0.6218 - val_loss: 0.4847 - val_mae: 0.4847\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5539 - mae: 0.5539 - val_loss: 0.4805 - val_mae: 0.4805\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5817 - mae: 0.5817 - val_loss: 0.4730 - val_mae: 0.4730\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5695 - mae: 0.5695 - val_loss: 0.4551 - val_mae: 0.4551\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5337 - mae: 0.5337 - val_loss: 0.4290 - val_mae: 0.4290\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4807 - mae: 0.4807 - val_loss: 0.4034 - val_mae: 0.4034\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5462 - mae: 0.5462 - val_loss: 0.3871 - val_mae: 0.3871\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5050 - mae: 0.5050 - val_loss: 0.3738 - val_mae: 0.3738\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4593 - mae: 0.4593 - val_loss: 0.3660 - val_mae: 0.3660\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4348 - mae: 0.4348 - val_loss: 0.3581 - val_mae: 0.3581\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3693 - mae: 0.3693 - val_loss: 0.3482 - val_mae: 0.3482\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4174 - mae: 0.4174 - val_loss: 0.3396 - val_mae: 0.3396\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3599 - mae: 0.3599 - val_loss: 0.3309 - val_mae: 0.3309\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3382 - mae: 0.3382 - val_loss: 0.3202 - val_mae: 0.3202\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.4051 - mae: 0.4051 - val_loss: 0.3120 - val_mae: 0.3120\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.3566 - mae: 0.3566 - val_loss: 0.3132 - val_mae: 0.3132\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.2998 - mae: 0.2998 - val_loss: 0.3218 - val_mae: 0.3218\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.3218 - mae: 0.3218\n",
      "Val score is 0.3217877447605133\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 102.9050 - mae: 102.9050 - val_loss: 103.1174 - val_mae: 103.1174\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.8765 - mae: 102.8765 - val_loss: 103.0666 - val_mae: 103.0666\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.8472 - mae: 102.8472 - val_loss: 103.0184 - val_mae: 103.0184\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.8169 - mae: 102.8169 - val_loss: 102.9685 - val_mae: 102.9685\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.7855 - mae: 102.7855 - val_loss: 102.9154 - val_mae: 102.9154\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.7530 - mae: 102.7530 - val_loss: 102.8650 - val_mae: 102.8650\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.7193 - mae: 102.7193 - val_loss: 102.8140 - val_mae: 102.8140\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.6842 - mae: 102.6842 - val_loss: 102.7592 - val_mae: 102.7592\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 319us/sample - loss: 102.6478 - mae: 102.6478 - val_loss: 102.7053 - val_mae: 102.7053\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 102.6099 - mae: 102.6099 - val_loss: 102.6491 - val_mae: 102.6491\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.5705 - mae: 102.5705 - val_loss: 102.5937 - val_mae: 102.5937\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.5295 - mae: 102.5295 - val_loss: 102.5354 - val_mae: 102.5354\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.4869 - mae: 102.4869 - val_loss: 102.4740 - val_mae: 102.4740\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.4426 - mae: 102.4426 - val_loss: 102.4124 - val_mae: 102.4124\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.3965 - mae: 102.3965 - val_loss: 102.3485 - val_mae: 102.3485\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 102.3486 - mae: 102.3486 - val_loss: 102.2855 - val_mae: 102.2855\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.2990 - mae: 102.2990 - val_loss: 102.2192 - val_mae: 102.2192\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 102.2474 - mae: 102.2474 - val_loss: 102.1501 - val_mae: 102.1501\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.1939 - mae: 102.1939 - val_loss: 102.0802 - val_mae: 102.0802\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.1385 - mae: 102.1385 - val_loss: 102.0078 - val_mae: 102.0078\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 102.0812 - mae: 102.0812 - val_loss: 101.9345 - val_mae: 101.9345\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 221us/sample - loss: 102.0218 - mae: 102.0218 - val_loss: 101.8576 - val_mae: 101.8576\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 101.9604 - mae: 101.9604 - val_loss: 101.7797 - val_mae: 101.7797\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.8970 - mae: 101.8970 - val_loss: 101.7007 - val_mae: 101.7007\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.8315 - mae: 101.8315 - val_loss: 101.6229 - val_mae: 101.6229\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.7639 - mae: 101.7639 - val_loss: 101.5430 - val_mae: 101.5430\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.6943 - mae: 101.6943 - val_loss: 101.4605 - val_mae: 101.4605\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.6225 - mae: 101.6225 - val_loss: 101.3768 - val_mae: 101.3768\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.5485 - mae: 101.5485 - val_loss: 101.2907 - val_mae: 101.2907\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.4725 - mae: 101.4725 - val_loss: 101.2015 - val_mae: 101.2015\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.3942 - mae: 101.3942 - val_loss: 101.1097 - val_mae: 101.1097\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.3138 - mae: 101.3138 - val_loss: 101.0196 - val_mae: 101.0196\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.2312 - mae: 101.2312 - val_loss: 100.9267 - val_mae: 100.9267\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.1464 - mae: 101.1464 - val_loss: 100.8275 - val_mae: 100.8275\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 101.0593 - mae: 101.0593 - val_loss: 100.7293 - val_mae: 100.7293\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.9701 - mae: 100.9701 - val_loss: 100.6319 - val_mae: 100.6319\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.8786 - mae: 100.8786 - val_loss: 100.5304 - val_mae: 100.5304\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.7849 - mae: 100.7849 - val_loss: 100.4226 - val_mae: 100.4226\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.6889 - mae: 100.6889 - val_loss: 100.3205 - val_mae: 100.3205\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.5906 - mae: 100.5907 - val_loss: 100.2139 - val_mae: 100.2139\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.4901 - mae: 100.4901 - val_loss: 100.1037 - val_mae: 100.1037\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.3874 - mae: 100.3874 - val_loss: 99.9936 - val_mae: 99.9936\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.2823 - mae: 100.2823 - val_loss: 99.8807 - val_mae: 99.8807\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.1750 - mae: 100.1749 - val_loss: 99.7630 - val_mae: 99.7630\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 100.0653 - mae: 100.0653 - val_loss: 99.6492 - val_mae: 99.6492\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 99.9534 - mae: 99.9534 - val_loss: 99.5343 - val_mae: 99.5343\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 99.8391 - mae: 99.8391 - val_loss: 99.4115 - val_mae: 99.4115\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 99.7226 - mae: 99.7226 - val_loss: 99.2860 - val_mae: 99.2860\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 99.6037 - mae: 99.6037 - val_loss: 99.1619 - val_mae: 99.1619\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 99.4825 - mae: 99.4825 - val_loss: 99.0358 - val_mae: 99.0358\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 99.3590 - mae: 99.3590 - val_loss: 98.9058 - val_mae: 98.9058\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 99.2332 - mae: 99.2332 - val_loss: 98.7725 - val_mae: 98.7725\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 99.1051 - mae: 99.1051 - val_loss: 98.6346 - val_mae: 98.6346\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.9746 - mae: 98.9746 - val_loss: 98.5000 - val_mae: 98.5000\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.8418 - mae: 98.8418 - val_loss: 98.3585 - val_mae: 98.3585\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.7066 - mae: 98.7066 - val_loss: 98.2197 - val_mae: 98.2197\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.5691 - mae: 98.5691 - val_loss: 98.0830 - val_mae: 98.0830\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.4293 - mae: 98.4293 - val_loss: 97.9412 - val_mae: 97.9412\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.2871 - mae: 98.2871 - val_loss: 97.7961 - val_mae: 97.7961\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 98.1426 - mae: 98.1426 - val_loss: 97.6490 - val_mae: 97.6490\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.9958 - mae: 97.9958 - val_loss: 97.5003 - val_mae: 97.5003\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.8465 - mae: 97.8465 - val_loss: 97.3512 - val_mae: 97.3512\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.6950 - mae: 97.6950 - val_loss: 97.2004 - val_mae: 97.2004\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.5410 - mae: 97.5411 - val_loss: 97.0471 - val_mae: 97.0471\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.3848 - mae: 97.3848 - val_loss: 96.8831 - val_mae: 96.8831\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.2262 - mae: 97.2262 - val_loss: 96.7190 - val_mae: 96.7190\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 97.0652 - mae: 97.0652 - val_loss: 96.5575 - val_mae: 96.5575\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 96.9018 - mae: 96.9018 - val_loss: 96.3953 - val_mae: 96.3953\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 96.7361 - mae: 96.7361 - val_loss: 96.2315 - val_mae: 96.2315\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 220us/sample - loss: 96.5681 - mae: 96.5681 - val_loss: 96.0721 - val_mae: 96.0721\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 182us/sample - loss: 96.3977 - mae: 96.3977 - val_loss: 95.9050 - val_mae: 95.9050\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 96.2249 - mae: 96.2249 - val_loss: 95.7313 - val_mae: 95.7313\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 96.0498 - mae: 96.0498 - val_loss: 95.5582 - val_mae: 95.5582\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 95.8723 - mae: 95.8723 - val_loss: 95.3827 - val_mae: 95.3827\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 95.6924 - mae: 95.6925 - val_loss: 95.2053 - val_mae: 95.2053\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 95.5102 - mae: 95.5102 - val_loss: 95.0300 - val_mae: 95.0300\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 95.3257 - mae: 95.3257 - val_loss: 94.8488 - val_mae: 94.8488\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 95.1388 - mae: 95.1388 - val_loss: 94.6597 - val_mae: 94.6597\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 94.9495 - mae: 94.9495 - val_loss: 94.4748 - val_mae: 94.4748\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 94.7579 - mae: 94.7579 - val_loss: 94.2793 - val_mae: 94.2793\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 94.5639 - mae: 94.5639 - val_loss: 94.0819 - val_mae: 94.0819\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 94.3675 - mae: 94.3675 - val_loss: 93.8880 - val_mae: 93.8880\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 94.1688 - mae: 94.1688 - val_loss: 93.6887 - val_mae: 93.6887\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 93.9677 - mae: 93.9677 - val_loss: 93.5012 - val_mae: 93.5012\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 93.7643 - mae: 93.7643 - val_loss: 93.2990 - val_mae: 93.2990\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 93.5585 - mae: 93.5585 - val_loss: 93.1006 - val_mae: 93.1006\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 93.3504 - mae: 93.3504 - val_loss: 92.8965 - val_mae: 92.8965\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 93.1399 - mae: 93.1399 - val_loss: 92.6865 - val_mae: 92.6865\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 92.9271 - mae: 92.9271 - val_loss: 92.4713 - val_mae: 92.4713\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 92.7119 - mae: 92.7119 - val_loss: 92.2594 - val_mae: 92.2594\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 92.4944 - mae: 92.4944 - val_loss: 92.0564 - val_mae: 92.0564\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 92.2745 - mae: 92.2745 - val_loss: 91.8512 - val_mae: 91.8512\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 92.0523 - mae: 92.0523 - val_loss: 91.6448 - val_mae: 91.6448\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 91.8277 - mae: 91.8277 - val_loss: 91.4388 - val_mae: 91.4388\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 91.6008 - mae: 91.6008 - val_loss: 91.2230 - val_mae: 91.2230\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 91.3715 - mae: 91.3715 - val_loss: 91.0013 - val_mae: 91.0013\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 91.1399 - mae: 91.1399 - val_loss: 90.7734 - val_mae: 90.7734\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 90.9059 - mae: 90.9059 - val_loss: 90.5402 - val_mae: 90.5402\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 90.6696 - mae: 90.6696 - val_loss: 90.3136 - val_mae: 90.3136\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 90.4310 - mae: 90.4310 - val_loss: 90.0735 - val_mae: 90.0735\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 90.0735 - mae: 90.0735\n",
      "Val score is 90.07347106933594\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 13ms/sample - loss: 82.0171 - mae: 82.0171 - val_loss: 82.2909 - val_mae: 82.2909\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 52us/sample - loss: 81.9871 - mae: 81.9871 - val_loss: 82.2415 - val_mae: 82.2415\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.9563 - mae: 81.9563 - val_loss: 82.1944 - val_mae: 82.1944\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.9245 - mae: 81.9245 - val_loss: 82.1440 - val_mae: 82.1440\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.8916 - mae: 81.8916 - val_loss: 82.0908 - val_mae: 82.0908\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.8576 - mae: 81.8576 - val_loss: 82.0376 - val_mae: 82.0376\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 81.8223 - mae: 81.8223 - val_loss: 81.9833 - val_mae: 81.9833\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.7858 - mae: 81.7858 - val_loss: 81.9281 - val_mae: 81.9281\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 81.7479 - mae: 81.7479 - val_loss: 81.8724 - val_mae: 81.8724\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 95us/sample - loss: 81.7085 - mae: 81.7085 - val_loss: 81.8138 - val_mae: 81.8138\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.6676 - mae: 81.6676 - val_loss: 81.7544 - val_mae: 81.7544\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.6251 - mae: 81.6251 - val_loss: 81.6928 - val_mae: 81.6928\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.5811 - mae: 81.5811 - val_loss: 81.6304 - val_mae: 81.6304\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.5353 - mae: 81.5353 - val_loss: 81.5664 - val_mae: 81.5664\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.4878 - mae: 81.4878 - val_loss: 81.5000 - val_mae: 81.5000\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.4385 - mae: 81.4385 - val_loss: 81.4313 - val_mae: 81.4313\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.3874 - mae: 81.3874 - val_loss: 81.3635 - val_mae: 81.3635\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.3345 - mae: 81.3345 - val_loss: 81.2935 - val_mae: 81.2935\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 81.2797 - mae: 81.2797 - val_loss: 81.2191 - val_mae: 81.2191\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.2229 - mae: 81.2229 - val_loss: 81.1447 - val_mae: 81.1447\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.1642 - mae: 81.1642 - val_loss: 81.0681 - val_mae: 81.0681\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.1036 - mae: 81.1036 - val_loss: 80.9909 - val_mae: 80.9909\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 81.0409 - mae: 81.0409 - val_loss: 80.9107 - val_mae: 80.9107\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.9762 - mae: 80.9762 - val_loss: 80.8273 - val_mae: 80.8273\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.9095 - mae: 80.9095 - val_loss: 80.7414 - val_mae: 80.7414\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.8408 - mae: 80.8408 - val_loss: 80.6566 - val_mae: 80.6566\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.7699 - mae: 80.7699 - val_loss: 80.5685 - val_mae: 80.5685\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.6969 - mae: 80.6969 - val_loss: 80.4794 - val_mae: 80.4794\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.6219 - mae: 80.6219 - val_loss: 80.3867 - val_mae: 80.3867\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.5447 - mae: 80.5447 - val_loss: 80.2926 - val_mae: 80.2926\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.4653 - mae: 80.4653 - val_loss: 80.1981 - val_mae: 80.1981\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.3838 - mae: 80.3838 - val_loss: 80.1018 - val_mae: 80.1018\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 80.3002 - mae: 80.3002 - val_loss: 80.0003 - val_mae: 80.0003\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.2143 - mae: 80.2143 - val_loss: 79.8995 - val_mae: 79.8995\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.1263 - mae: 80.1263 - val_loss: 79.7969 - val_mae: 79.7969\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 80.0360 - mae: 80.0360 - val_loss: 79.6908 - val_mae: 79.6908\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.9435 - mae: 79.9436 - val_loss: 79.5857 - val_mae: 79.5857\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.8489 - mae: 79.8489 - val_loss: 79.4791 - val_mae: 79.4791\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.7520 - mae: 79.7520 - val_loss: 79.3698 - val_mae: 79.3698\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.6528 - mae: 79.6528 - val_loss: 79.2547 - val_mae: 79.2547\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.5514 - mae: 79.5514 - val_loss: 79.1391 - val_mae: 79.1391\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.4477 - mae: 79.4477 - val_loss: 79.0221 - val_mae: 79.0220\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.3418 - mae: 79.3418 - val_loss: 78.9009 - val_mae: 78.9009\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 79.2336 - mae: 79.2336 - val_loss: 78.7805 - val_mae: 78.7805\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.1232 - mae: 79.1232 - val_loss: 78.6569 - val_mae: 78.6569\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 79.0104 - mae: 79.0104 - val_loss: 78.5317 - val_mae: 78.5317\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.8954 - mae: 78.8954 - val_loss: 78.4045 - val_mae: 78.4046\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.7781 - mae: 78.7781 - val_loss: 78.2767 - val_mae: 78.2767\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.6585 - mae: 78.6585 - val_loss: 78.1502 - val_mae: 78.1502\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.5365 - mae: 78.5365 - val_loss: 78.0190 - val_mae: 78.0190\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.4123 - mae: 78.4123 - val_loss: 77.8879 - val_mae: 77.8879\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.2858 - mae: 78.2858 - val_loss: 77.7514 - val_mae: 77.7514\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.1570 - mae: 78.1570 - val_loss: 77.6100 - val_mae: 77.6100\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 78.0258 - mae: 78.0258 - val_loss: 77.4655 - val_mae: 77.4655\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.8923 - mae: 77.8923 - val_loss: 77.3234 - val_mae: 77.3234\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.7565 - mae: 77.7565 - val_loss: 77.1812 - val_mae: 77.1812\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.6184 - mae: 77.6184 - val_loss: 77.0407 - val_mae: 77.0407\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.4779 - mae: 77.4779 - val_loss: 76.8932 - val_mae: 76.8932\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.3352 - mae: 77.3352 - val_loss: 76.7421 - val_mae: 76.7421\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.1900 - mae: 77.1900 - val_loss: 76.5983 - val_mae: 76.5983\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 77.0426 - mae: 77.0426 - val_loss: 76.4484 - val_mae: 76.4484\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 76.8928 - mae: 76.8928 - val_loss: 76.2972 - val_mae: 76.2972\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 76.7407 - mae: 76.7407 - val_loss: 76.1395 - val_mae: 76.1395\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 76.5862 - mae: 76.5862 - val_loss: 75.9856 - val_mae: 75.9856\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 76.4294 - mae: 76.4294 - val_loss: 75.8257 - val_mae: 75.8257\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 76.2703 - mae: 76.2702 - val_loss: 75.6587 - val_mae: 75.6587\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 76.1088 - mae: 76.1087 - val_loss: 75.4929 - val_mae: 75.4929\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 75.9449 - mae: 75.9449 - val_loss: 75.3244 - val_mae: 75.3244\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 344us/sample - loss: 75.7787 - mae: 75.7787 - val_loss: 75.1514 - val_mae: 75.1514\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 44us/sample - loss: 75.6102 - mae: 75.6102 - val_loss: 74.9758 - val_mae: 74.9758\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 75.4393 - mae: 75.4393 - val_loss: 74.7977 - val_mae: 74.7977\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 75.2661 - mae: 75.2661 - val_loss: 74.6147 - val_mae: 74.6147\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 75.0905 - mae: 75.0905 - val_loss: 74.4368 - val_mae: 74.4368\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 74.9126 - mae: 74.9126 - val_loss: 74.2574 - val_mae: 74.2574\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 74.7323 - mae: 74.7323 - val_loss: 74.0720 - val_mae: 74.0720\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 74.5496 - mae: 74.5496 - val_loss: 73.8866 - val_mae: 73.8866\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 74.3647 - mae: 74.3647 - val_loss: 73.7010 - val_mae: 73.7010\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 74.1773 - mae: 74.1773 - val_loss: 73.5150 - val_mae: 73.5150\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 73.9876 - mae: 73.9877 - val_loss: 73.3244 - val_mae: 73.3244\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 73.7956 - mae: 73.7956 - val_loss: 73.1392 - val_mae: 73.1392\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 73.6012 - mae: 73.6012 - val_loss: 72.9500 - val_mae: 72.9501\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 73.4045 - mae: 73.4045 - val_loss: 72.7564 - val_mae: 72.7563\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 73.2054 - mae: 73.2054 - val_loss: 72.5565 - val_mae: 72.5565\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 73.0040 - mae: 73.0040 - val_loss: 72.3597 - val_mae: 72.3597\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 72.8002 - mae: 72.8002 - val_loss: 72.1655 - val_mae: 72.1655\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 72.5941 - mae: 72.5941 - val_loss: 71.9601 - val_mae: 71.9601\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 72.3856 - mae: 72.3856 - val_loss: 71.7524 - val_mae: 71.7524\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 72.1748 - mae: 72.1748 - val_loss: 71.5450 - val_mae: 71.5450\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 71.9616 - mae: 71.9616 - val_loss: 71.3348 - val_mae: 71.3348\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 71.7461 - mae: 71.7461 - val_loss: 71.1267 - val_mae: 71.1267\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 71.5283 - mae: 71.5283 - val_loss: 70.9116 - val_mae: 70.9116\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 71.3081 - mae: 71.3081 - val_loss: 70.6902 - val_mae: 70.6902\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 71.0855 - mae: 71.0855 - val_loss: 70.4638 - val_mae: 70.4638\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 70.8607 - mae: 70.8607 - val_loss: 70.2343 - val_mae: 70.2343\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 70.6334 - mae: 70.6334 - val_loss: 70.0042 - val_mae: 70.0042\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 70.4039 - mae: 70.4039 - val_loss: 69.7744 - val_mae: 69.7744\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 252us/sample - loss: 70.1720 - mae: 70.1720 - val_loss: 69.5540 - val_mae: 69.5540\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 69.9377 - mae: 69.9377 - val_loss: 69.3366 - val_mae: 69.3366\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 69.7158 - mae: 69.7158 - val_loss: 69.1360 - val_mae: 69.1360\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 69.4640 - mae: 69.4640 - val_loss: 69.0096 - val_mae: 69.0096\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 69.0096 - mae: 69.0096\n",
      "Val score is 69.00961303710938\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 10ms/sample - loss: 3.9751 - mae: 3.9751 - val_loss: 3.3775 - val_mae: 3.3775\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 45us/sample - loss: 3.8432 - mae: 3.8432 - val_loss: 3.3522 - val_mae: 3.3522\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.8085 - mae: 3.8085 - val_loss: 3.3222 - val_mae: 3.3222\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 391us/sample - loss: 3.7413 - mae: 3.7413 - val_loss: 3.2969 - val_mae: 3.2969\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.7097 - mae: 3.7097 - val_loss: 3.2747 - val_mae: 3.2747\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.6778 - mae: 3.6778 - val_loss: 3.2533 - val_mae: 3.2533\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.6455 - mae: 3.6455 - val_loss: 3.2318 - val_mae: 3.2318\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.6127 - mae: 3.6127 - val_loss: 3.2119 - val_mae: 3.2119\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.5793 - mae: 3.5793 - val_loss: 3.1912 - val_mae: 3.1912\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.5488 - mae: 3.5488 - val_loss: 3.1726 - val_mae: 3.1726\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.5108 - mae: 3.5108 - val_loss: 3.1572 - val_mae: 3.1572\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.4760 - mae: 3.4760 - val_loss: 3.1404 - val_mae: 3.1404\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.4400 - mae: 3.4400 - val_loss: 3.1202 - val_mae: 3.1202\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.4029 - mae: 3.4029 - val_loss: 3.0976 - val_mae: 3.0976\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.3646 - mae: 3.3646 - val_loss: 3.0747 - val_mae: 3.0747\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.3250 - mae: 3.3250 - val_loss: 3.0475 - val_mae: 3.0475\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.2840 - mae: 3.2840 - val_loss: 3.0203 - val_mae: 3.0203\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.2415 - mae: 3.2415 - val_loss: 2.9906 - val_mae: 2.9906\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.1975 - mae: 3.1975 - val_loss: 2.9608 - val_mae: 2.9608\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.1519 - mae: 3.1519 - val_loss: 2.9274 - val_mae: 2.9274\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.1047 - mae: 3.1047 - val_loss: 2.8922 - val_mae: 2.8922\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 3.0557 - mae: 3.0557 - val_loss: 2.8543 - val_mae: 2.8543\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 159us/sample - loss: 3.0050 - mae: 3.0050 - val_loss: 2.8144 - val_mae: 2.8144\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 213us/sample - loss: 2.9525 - mae: 2.9525 - val_loss: 2.7736 - val_mae: 2.7736\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 2.8981 - mae: 2.8981 - val_loss: 2.7311 - val_mae: 2.7311\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.8418 - mae: 2.8418 - val_loss: 2.6850 - val_mae: 2.6850\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.7835 - mae: 2.7835 - val_loss: 2.6347 - val_mae: 2.6347\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.7285 - mae: 2.7285 - val_loss: 2.5715 - val_mae: 2.5715\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.6688 - mae: 2.6688 - val_loss: 2.5069 - val_mae: 2.5069\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.5995 - mae: 2.5995 - val_loss: 2.4403 - val_mae: 2.4403\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.5430 - mae: 2.5430 - val_loss: 2.3755 - val_mae: 2.3755\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 2.4889 - mae: 2.4889 - val_loss: 2.3065 - val_mae: 2.3065\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.4143 - mae: 2.4143 - val_loss: 2.2353 - val_mae: 2.2353\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.3472 - mae: 2.3472 - val_loss: 2.1748 - val_mae: 2.1748\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.2851 - mae: 2.2851 - val_loss: 2.1175 - val_mae: 2.1175\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.2122 - mae: 2.2122 - val_loss: 2.0611 - val_mae: 2.0611\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.1555 - mae: 2.1555 - val_loss: 1.9927 - val_mae: 1.9927\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 2.0441 - mae: 2.0441 - val_loss: 1.9198 - val_mae: 1.9198\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.9655 - mae: 1.9655 - val_loss: 1.8454 - val_mae: 1.8454\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.8935 - mae: 1.8935 - val_loss: 1.7581 - val_mae: 1.7581\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.8313 - mae: 1.8313 - val_loss: 1.6696 - val_mae: 1.6696\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7755 - mae: 1.7755 - val_loss: 1.5810 - val_mae: 1.5810\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.7093 - mae: 1.7093 - val_loss: 1.4812 - val_mae: 1.4812\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5649 - mae: 1.5649 - val_loss: 1.3914 - val_mae: 1.3914\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.5541 - mae: 1.5541 - val_loss: 1.3182 - val_mae: 1.3182\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.4085 - mae: 1.4085 - val_loss: 1.2441 - val_mae: 1.2441\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.3367 - mae: 1.3367 - val_loss: 1.1634 - val_mae: 1.1634\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.2818 - mae: 1.2818 - val_loss: 1.0880 - val_mae: 1.0880\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1977 - mae: 1.1977 - val_loss: 0.9968 - val_mae: 0.9968\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.1180 - mae: 1.1180 - val_loss: 0.9000 - val_mae: 0.9000\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 1.0721 - mae: 1.0721 - val_loss: 0.8228 - val_mae: 0.8228\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9678 - mae: 0.9678 - val_loss: 0.7743 - val_mae: 0.7743\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.9501 - mae: 0.9501 - val_loss: 0.7163 - val_mae: 0.7163\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.8293 - mae: 0.8293 - val_loss: 0.6237 - val_mae: 0.6237\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7209 - mae: 0.7209 - val_loss: 0.5083 - val_mae: 0.5083\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7055 - mae: 0.7055 - val_loss: 0.4186 - val_mae: 0.4186\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.7235 - mae: 0.7235 - val_loss: 0.3652 - val_mae: 0.3652\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5571 - mae: 0.5571 - val_loss: 0.3214 - val_mae: 0.3214\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 287us/sample - loss: 0.4883 - mae: 0.4883 - val_loss: 0.3170 - val_mae: 0.3170\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 138us/sample - loss: 0.5279 - mae: 0.5279 - val_loss: 0.3153 - val_mae: 0.3153\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5676 - mae: 0.5676 - val_loss: 0.3185 - val_mae: 0.3185\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 0.5905 - mae: 0.5905 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 0.3391 - mae: 0.3391\n",
      "Val score is 0.33908897638320923\n",
      "Train on 80 samples, validate on 80 samples\n",
      "Epoch 1/100\n",
      "80/80 [==============================] - 1s 11ms/sample - loss: 62.5175 - mae: 62.5175 - val_loss: 62.7467 - val_mae: 62.7467\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.4752 - mae: 62.4752 - val_loss: 62.6960 - val_mae: 62.6960\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.4456 - mae: 62.4456 - val_loss: 62.6541 - val_mae: 62.6541\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.4156 - mae: 62.4156 - val_loss: 62.6167 - val_mae: 62.6167\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.3852 - mae: 62.3852 - val_loss: 62.5815 - val_mae: 62.5815\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.3540 - mae: 62.3540 - val_loss: 62.5485 - val_mae: 62.5485\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.3220 - mae: 62.3220 - val_loss: 62.5155 - val_mae: 62.5155\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.2891 - mae: 62.2891 - val_loss: 62.4848 - val_mae: 62.4848\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.2549 - mae: 62.2549 - val_loss: 62.4536 - val_mae: 62.4536\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.2195 - mae: 62.2195 - val_loss: 62.4200 - val_mae: 62.4200\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.1827 - mae: 62.1827 - val_loss: 62.3854 - val_mae: 62.3854\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.1444 - mae: 62.1444 - val_loss: 62.3495 - val_mae: 62.3495\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 0s 103us/sample - loss: 62.1046 - mae: 62.1046 - val_loss: 62.3141 - val_mae: 62.3141\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.0632 - mae: 62.0631 - val_loss: 62.2748 - val_mae: 62.2748\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 62.0200 - mae: 62.0200 - val_loss: 62.2365 - val_mae: 62.2365\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.9751 - mae: 61.9751 - val_loss: 62.1935 - val_mae: 62.1935\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.9283 - mae: 61.9283 - val_loss: 62.1486 - val_mae: 62.1485\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.8797 - mae: 61.8797 - val_loss: 62.1011 - val_mae: 62.1011\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.8292 - mae: 61.8292 - val_loss: 62.0555 - val_mae: 62.0555\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.7768 - mae: 61.7768 - val_loss: 62.0092 - val_mae: 62.0092\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 61.7224 - mae: 61.7224 - val_loss: 61.9612 - val_mae: 61.9612\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 61.6660 - mae: 61.6660 - val_loss: 61.9095 - val_mae: 61.9095\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.6075 - mae: 61.6075 - val_loss: 61.8559 - val_mae: 61.8559\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.5470 - mae: 61.5470 - val_loss: 61.8016 - val_mae: 61.8016\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.4844 - mae: 61.4844 - val_loss: 61.7416 - val_mae: 61.7416\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.4197 - mae: 61.4197 - val_loss: 61.6759 - val_mae: 61.6759\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.3529 - mae: 61.3529 - val_loss: 61.6111 - val_mae: 61.6111\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.2839 - mae: 61.2839 - val_loss: 61.5460 - val_mae: 61.5460\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.2127 - mae: 61.2127 - val_loss: 61.4806 - val_mae: 61.4806\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.1394 - mae: 61.1394 - val_loss: 61.4106 - val_mae: 61.4106\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 61.0638 - mae: 61.0638 - val_loss: 61.3421 - val_mae: 61.3421\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.9861 - mae: 60.9861 - val_loss: 61.2686 - val_mae: 61.2686\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.9061 - mae: 60.9061 - val_loss: 61.1891 - val_mae: 61.1891\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.8239 - mae: 60.8239 - val_loss: 61.1081 - val_mae: 61.1081\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.7394 - mae: 60.7394 - val_loss: 61.0291 - val_mae: 61.0291\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.6527 - mae: 60.6527 - val_loss: 60.9462 - val_mae: 60.9462\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.5637 - mae: 60.5637 - val_loss: 60.8606 - val_mae: 60.8606\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.4725 - mae: 60.4725 - val_loss: 60.7740 - val_mae: 60.7740\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.3789 - mae: 60.3789 - val_loss: 60.6840 - val_mae: 60.6840\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 60.2831 - mae: 60.2831 - val_loss: 60.5907 - val_mae: 60.5906\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.1850 - mae: 60.1850 - val_loss: 60.4943 - val_mae: 60.4943\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 60.0845 - mae: 60.0845 - val_loss: 60.3925 - val_mae: 60.3925\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.9818 - mae: 59.9818 - val_loss: 60.2941 - val_mae: 60.2941\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.8767 - mae: 59.8767 - val_loss: 60.1898 - val_mae: 60.1898\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.7693 - mae: 59.7693 - val_loss: 60.0862 - val_mae: 60.0862\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.6596 - mae: 59.6596 - val_loss: 59.9823 - val_mae: 59.9823\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.5475 - mae: 59.5475 - val_loss: 59.8715 - val_mae: 59.8715\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.4331 - mae: 59.4331 - val_loss: 59.7589 - val_mae: 59.7589\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.3164 - mae: 59.3164 - val_loss: 59.6474 - val_mae: 59.6474\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.1973 - mae: 59.1973 - val_loss: 59.5323 - val_mae: 59.5323\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 59.0759 - mae: 59.0759 - val_loss: 59.4150 - val_mae: 59.4150\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.9555 - mae: 58.9555 - val_loss: 59.2194 - val_mae: 59.2194\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.8263 - mae: 58.8263 - val_loss: 59.0192 - val_mae: 59.0192\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.6982 - mae: 58.6982 - val_loss: 58.8432 - val_mae: 58.8432\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.5675 - mae: 58.5675 - val_loss: 58.6782 - val_mae: 58.6782\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.4345 - mae: 58.4345 - val_loss: 58.5109 - val_mae: 58.5109\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.2990 - mae: 58.2990 - val_loss: 58.3540 - val_mae: 58.3540\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.1612 - mae: 58.1612 - val_loss: 58.1951 - val_mae: 58.1951\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 58.0209 - mae: 58.0209 - val_loss: 58.0415 - val_mae: 58.0415\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 57.8783 - mae: 57.8783 - val_loss: 57.8925 - val_mae: 57.8925\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 57.7333 - mae: 57.7333 - val_loss: 57.7310 - val_mae: 57.7310\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 57.5914 - mae: 57.5914 - val_loss: 57.5190 - val_mae: 57.5190\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 57.4379 - mae: 57.4379 - val_loss: 57.2874 - val_mae: 57.2874\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 57.2872 - mae: 57.2872 - val_loss: 57.0808 - val_mae: 57.0808\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 57.1337 - mae: 57.1337 - val_loss: 56.8763 - val_mae: 56.8763\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 56.9775 - mae: 56.9775 - val_loss: 56.6909 - val_mae: 56.6909\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 56.8188 - mae: 56.8187 - val_loss: 56.5119 - val_mae: 56.5119\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 56.6575 - mae: 56.6575 - val_loss: 56.3322 - val_mae: 56.3322\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 56.4937 - mae: 56.4937 - val_loss: 56.1571 - val_mae: 56.1571\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 56.3275 - mae: 56.3275 - val_loss: 55.9839 - val_mae: 55.9839\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 56.1588 - mae: 56.1588 - val_loss: 55.8082 - val_mae: 55.8082\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 55.9877 - mae: 55.9877 - val_loss: 55.6350 - val_mae: 55.6350\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 55.8142 - mae: 55.8142 - val_loss: 55.4689 - val_mae: 55.4689\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 0s 190us/sample - loss: 55.6383 - mae: 55.6383 - val_loss: 55.2961 - val_mae: 55.2961\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 0s 307us/sample - loss: 55.4600 - mae: 55.4600 - val_loss: 55.1237 - val_mae: 55.1237\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 55.2793 - mae: 55.2793 - val_loss: 54.9430 - val_mae: 54.9430\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 55.0963 - mae: 55.0963 - val_loss: 54.7558 - val_mae: 54.7558\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 54.9108 - mae: 54.9108 - val_loss: 54.5745 - val_mae: 54.5745\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 195us/sample - loss: 54.7230 - mae: 54.7230 - val_loss: 54.3907 - val_mae: 54.3907\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 54.5328 - mae: 54.5328 - val_loss: 54.1982 - val_mae: 54.1982\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 54.3402 - mae: 54.3402 - val_loss: 54.0033 - val_mae: 54.0033\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 54.1452 - mae: 54.1452 - val_loss: 53.8109 - val_mae: 53.8110\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 0s 209us/sample - loss: 53.9479 - mae: 53.9479 - val_loss: 53.6150 - val_mae: 53.6150\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 53.7482 - mae: 53.7482 - val_loss: 53.4264 - val_mae: 53.4264\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 53.5461 - mae: 53.5461 - val_loss: 53.2301 - val_mae: 53.2301\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 53.3416 - mae: 53.3416 - val_loss: 53.0323 - val_mae: 53.0323\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 53.1348 - mae: 53.1348 - val_loss: 52.8222 - val_mae: 52.8222\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 52.9256 - mae: 52.9256 - val_loss: 52.6108 - val_mae: 52.6108\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 52.7140 - mae: 52.7140 - val_loss: 52.4054 - val_mae: 52.4054\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 52.5001 - mae: 52.5001 - val_loss: 52.1838 - val_mae: 52.1838\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 52.2838 - mae: 52.2838 - val_loss: 51.9687 - val_mae: 51.9687\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 52.0651 - mae: 52.0651 - val_loss: 51.7429 - val_mae: 51.7429\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 51.8441 - mae: 51.8441 - val_loss: 51.5283 - val_mae: 51.5283\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 51.6207 - mae: 51.6207 - val_loss: 51.2921 - val_mae: 51.2921\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 51.3949 - mae: 51.3949 - val_loss: 51.0608 - val_mae: 51.0608\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 51.1668 - mae: 51.1668 - val_loss: 50.8299 - val_mae: 50.8299\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 50.9363 - mae: 50.9363 - val_loss: 50.5929 - val_mae: 50.5929\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 50.7035 - mae: 50.7035 - val_loss: 50.3757 - val_mae: 50.3757\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 50.4683 - mae: 50.4683 - val_loss: 50.1474 - val_mae: 50.1474\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 0s 195us/sample - loss: 50.2308 - mae: 50.2308 - val_loss: 49.8997 - val_mae: 49.8997\n",
      "80/80 [==============================] - 0s 0s/sample - loss: 49.8997 - mae: 49.8997\n",
      "Val score is 49.899696350097656\n",
      "DataFrame: hepatitis imput_method :MLP model :KNN score:  0.85\n",
      "DataFrame: hepatitis imput_method :MLP model :XGB score:  0.83\n",
      "DataFrame: hepatitis imput_method :MLP model :MLP score:  0.83\n",
      "DataFrame: housevotes imput_method :LOCF model :KNN score:  0.93\n",
      "DataFrame: housevotes imput_method :LOCF model :XGB score:  0.96\n",
      "DataFrame: housevotes imput_method :LOCF model :MLP score:  0.95\n",
      "DataFrame: housevotes imput_method :mean_mode model :KNN score:  0.93\n",
      "DataFrame: housevotes imput_method :mean_mode model :XGB score:  0.96\n",
      "DataFrame: housevotes imput_method :mean_mode model :MLP score:  0.95\n",
      "DataFrame: housevotes imput_method :knn model :KNN score:  0.93\n",
      "DataFrame: housevotes imput_method :knn model :XGB score:  0.97\n",
      "DataFrame: housevotes imput_method :knn model :MLP score:  0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score is 0.6808510638297873\n",
      "Best score is 0.5891472868217054\n",
      "Best score is 0.8749042719675023\n",
      "Best score is 0.9622248859587786\n",
      "Best score is 0.9595238095238096\n",
      "Best score is 0.825408717077881\n",
      "Best score is 0.8788247213779128\n",
      "Best score is 0.9119047619047619\n",
      "Best score is 0.886138439296167\n",
      "Best score is 0.5512656357726781\n",
      "Best score is 0.7294685990338164\n",
      "Best score is 0.868767274737424\n",
      "Best score is 0.829200658365536\n",
      "Best score is 0.8515587529976019\n",
      "Best score is 0.7836782861292665\n",
      "Best score is 0.8126945126945128\n",
      "DataFrame: housevotes imput_method :trees model :KNN score:  0.94\n",
      "DataFrame: housevotes imput_method :trees model :XGB score:  0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: housevotes imput_method :trees model :MLP score:  0.95\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 2s 7ms/sample - loss: 1.0098 - accuracy: 0.5000 - val_loss: 0.7369 - val_accuracy: 0.4698\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 199us/sample - loss: 0.7038 - accuracy: 0.6164 - val_loss: 0.6657 - val_accuracy: 0.5948\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 190us/sample - loss: 0.6158 - accuracy: 0.6940 - val_loss: 0.6240 - val_accuracy: 0.6638\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.5321 - accuracy: 0.7371 - val_loss: 0.6026 - val_accuracy: 0.6897\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 194us/sample - loss: 0.5215 - accuracy: 0.7328 - val_loss: 0.5848 - val_accuracy: 0.7241\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 190us/sample - loss: 0.5025 - accuracy: 0.7543 - val_loss: 0.5675 - val_accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 194us/sample - loss: 0.4854 - accuracy: 0.7802 - val_loss: 0.5543 - val_accuracy: 0.7543\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 202us/sample - loss: 0.4999 - accuracy: 0.7629 - val_loss: 0.5427 - val_accuracy: 0.7586\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.4652 - accuracy: 0.7845 - val_loss: 0.5351 - val_accuracy: 0.7586\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.4469 - accuracy: 0.8060 - val_loss: 0.5246 - val_accuracy: 0.7500\n",
      "232/232 [==============================] - 0s 73us/sample - loss: 0.5246 - accuracy: 0.7500\n",
      "Val score is 0.75\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 6ms/sample - loss: 0.8202 - accuracy: 0.5216 - val_loss: 0.6799 - val_accuracy: 0.5647\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 192us/sample - loss: 0.6983 - accuracy: 0.5733 - val_loss: 0.6645 - val_accuracy: 0.5819\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.6389 - accuracy: 0.6422 - val_loss: 0.6512 - val_accuracy: 0.6121\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 165us/sample - loss: 0.5826 - accuracy: 0.7241 - val_loss: 0.6398 - val_accuracy: 0.6509\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.5643 - accuracy: 0.7198 - val_loss: 0.6302 - val_accuracy: 0.6466\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.5280 - accuracy: 0.7198 - val_loss: 0.6211 - val_accuracy: 0.6552\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.5214 - accuracy: 0.7198 - val_loss: 0.6119 - val_accuracy: 0.6638\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.5187 - accuracy: 0.7543 - val_loss: 0.6024 - val_accuracy: 0.6810\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4984 - accuracy: 0.7716 - val_loss: 0.5934 - val_accuracy: 0.6983\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.4778 - accuracy: 0.7845 - val_loss: 0.5844 - val_accuracy: 0.7026\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4905 - accuracy: 0.7457 - val_loss: 0.5744 - val_accuracy: 0.7198\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.4714 - accuracy: 0.7543 - val_loss: 0.5658 - val_accuracy: 0.7284\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.4479 - accuracy: 0.7845 - val_loss: 0.5567 - val_accuracy: 0.7414\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.4641 - accuracy: 0.7328 - val_loss: 0.5483 - val_accuracy: 0.7371\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.4356 - accuracy: 0.7888 - val_loss: 0.5394 - val_accuracy: 0.7414\n",
      "232/232 [==============================] - 0s 56us/sample - loss: 0.5394 - accuracy: 0.7414\n",
      "Val score is 0.7413793206214905\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.6694 - accuracy: 0.6767 - val_loss: 0.5416 - val_accuracy: 0.8233\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 203us/sample - loss: 0.3780 - accuracy: 0.8233 - val_loss: 0.4739 - val_accuracy: 0.8707\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.3238 - accuracy: 0.8405 - val_loss: 0.4322 - val_accuracy: 0.8750\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.3089 - accuracy: 0.8621 - val_loss: 0.4074 - val_accuracy: 0.8750\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.2848 - accuracy: 0.8664 - val_loss: 0.3900 - val_accuracy: 0.8793\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 176us/sample - loss: 0.2794 - accuracy: 0.8793 - val_loss: 0.3801 - val_accuracy: 0.8793\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.2735 - accuracy: 0.8750 - val_loss: 0.3684 - val_accuracy: 0.8793\n",
      "232/232 [==============================] - 0s 69us/sample - loss: 0.3684 - accuracy: 0.8793\n",
      "Val score is 0.8793103694915771\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 2s 7ms/sample - loss: 0.4681 - accuracy: 0.7888 - val_loss: 0.5193 - val_accuracy: 0.9138\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 252us/sample - loss: 0.2555 - accuracy: 0.8922 - val_loss: 0.4323 - val_accuracy: 0.9181\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.2004 - accuracy: 0.9353 - val_loss: 0.3712 - val_accuracy: 0.9353\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.1627 - accuracy: 0.9310 - val_loss: 0.3277 - val_accuracy: 0.9397\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 182us/sample - loss: 0.1532 - accuracy: 0.9483 - val_loss: 0.2951 - val_accuracy: 0.9569\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.1227 - accuracy: 0.9526 - val_loss: 0.2706 - val_accuracy: 0.9612\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.1314 - accuracy: 0.9612 - val_loss: 0.2486 - val_accuracy: 0.9698\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.0996 - accuracy: 0.9698 - val_loss: 0.2293 - val_accuracy: 0.9741\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.0820 - accuracy: 0.9698 - val_loss: 0.2094 - val_accuracy: 0.9828\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.0721 - accuracy: 0.9741 - val_loss: 0.1917 - val_accuracy: 0.9871\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 202us/sample - loss: 0.0625 - accuracy: 0.9784 - val_loss: 0.1760 - val_accuracy: 0.9914\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.0622 - accuracy: 0.9828 - val_loss: 0.1619 - val_accuracy: 0.9914\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 190us/sample - loss: 0.0646 - accuracy: 0.9828 - val_loss: 0.1480 - val_accuracy: 0.9914\n",
      "232/232 [==============================] - 0s 57us/sample - loss: 0.1480 - accuracy: 0.9914\n",
      "Val score is 0.9913793206214905\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.5174 - accuracy: 0.7586 - val_loss: 0.4875 - val_accuracy: 0.8879\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.2484 - accuracy: 0.9009 - val_loss: 0.3778 - val_accuracy: 0.9310\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.1566 - accuracy: 0.9612 - val_loss: 0.3082 - val_accuracy: 0.9397\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 177us/sample - loss: 0.1288 - accuracy: 0.9569 - val_loss: 0.2600 - val_accuracy: 0.9612\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.1212 - accuracy: 0.9612 - val_loss: 0.2262 - val_accuracy: 0.9698\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 0s 168us/sample - loss: 0.1010 - accuracy: 0.9698 - val_loss: 0.1986 - val_accuracy: 0.9698\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.0949 - accuracy: 0.9698 - val_loss: 0.1786 - val_accuracy: 0.9698\n",
      "232/232 [==============================] - 0s 56us/sample - loss: 0.1786 - accuracy: 0.9698\n",
      "Val score is 0.9698275923728943\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.6891 - accuracy: 0.6897 - val_loss: 0.5506 - val_accuracy: 0.8017\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 202us/sample - loss: 0.5424 - accuracy: 0.7716 - val_loss: 0.5027 - val_accuracy: 0.8147\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 185us/sample - loss: 0.4263 - accuracy: 0.8319 - val_loss: 0.4878 - val_accuracy: 0.8405\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 184us/sample - loss: 0.4103 - accuracy: 0.8319 - val_loss: 0.4741 - val_accuracy: 0.8448\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 172us/sample - loss: 0.3724 - accuracy: 0.8534 - val_loss: 0.4595 - val_accuracy: 0.8578\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.3549 - accuracy: 0.8578 - val_loss: 0.4411 - val_accuracy: 0.8707\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 173us/sample - loss: 0.3529 - accuracy: 0.8750 - val_loss: 0.4266 - val_accuracy: 0.8793\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 167us/sample - loss: 0.3301 - accuracy: 0.8707 - val_loss: 0.4092 - val_accuracy: 0.8966\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.3086 - accuracy: 0.9009 - val_loss: 0.4063 - val_accuracy: 0.8879\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 181us/sample - loss: 0.2931 - accuracy: 0.9138 - val_loss: 0.3952 - val_accuracy: 0.8922\n",
      "232/232 [==============================] - 0s 101us/sample - loss: 0.3952 - accuracy: 0.8922\n",
      "Val score is 0.892241358757019\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.6926 - accuracy: 0.6034 - val_loss: 0.5693 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4670 - accuracy: 0.8017 - val_loss: 0.4976 - val_accuracy: 0.8060\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.4190 - accuracy: 0.8103 - val_loss: 0.4636 - val_accuracy: 0.8405\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3800 - accuracy: 0.8405 - val_loss: 0.4515 - val_accuracy: 0.8405\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.3872 - accuracy: 0.8534 - val_loss: 0.4410 - val_accuracy: 0.8491\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3611 - accuracy: 0.8405 - val_loss: 0.4274 - val_accuracy: 0.8578\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.3262 - accuracy: 0.8836 - val_loss: 0.4134 - val_accuracy: 0.8750\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 144us/sample - loss: 0.3049 - accuracy: 0.8922 - val_loss: 0.3966 - val_accuracy: 0.8793\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.3054 - accuracy: 0.8793 - val_loss: 0.3853 - val_accuracy: 0.8879\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3020 - accuracy: 0.8879 - val_loss: 0.3756 - val_accuracy: 0.8922\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.2875 - accuracy: 0.8793 - val_loss: 0.3656 - val_accuracy: 0.8879\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.2602 - accuracy: 0.8966 - val_loss: 0.3531 - val_accuracy: 0.8922\n",
      "232/232 [==============================] - 0s 106us/sample - loss: 0.3531 - accuracy: 0.8922\n",
      "Val score is 0.892241358757019\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.5399 - accuracy: 0.7500 - val_loss: 0.4893 - val_accuracy: 0.8448\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 188us/sample - loss: 0.3158 - accuracy: 0.8750 - val_loss: 0.4137 - val_accuracy: 0.9052\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 132us/sample - loss: 0.2472 - accuracy: 0.9095 - val_loss: 0.3665 - val_accuracy: 0.9138\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2264 - accuracy: 0.9095 - val_loss: 0.3352 - val_accuracy: 0.9224\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 131us/sample - loss: 0.2007 - accuracy: 0.9224 - val_loss: 0.3104 - val_accuracy: 0.9310\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.1858 - accuracy: 0.9224 - val_loss: 0.2923 - val_accuracy: 0.9353\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 127us/sample - loss: 0.1862 - accuracy: 0.9310 - val_loss: 0.2759 - val_accuracy: 0.9310\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.1620 - accuracy: 0.9483 - val_loss: 0.2635 - val_accuracy: 0.9353\n",
      "232/232 [==============================] - 0s 97us/sample - loss: 0.2635 - accuracy: 0.9353\n",
      "Val score is 0.9353448152542114\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.5249 - accuracy: 0.7328 - val_loss: 0.4841 - val_accuracy: 0.8405\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 156us/sample - loss: 0.3238 - accuracy: 0.8448 - val_loss: 0.4076 - val_accuracy: 0.8750\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.2865 - accuracy: 0.8879 - val_loss: 0.3634 - val_accuracy: 0.8922\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2228 - accuracy: 0.9095 - val_loss: 0.3343 - val_accuracy: 0.9267\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2005 - accuracy: 0.9440 - val_loss: 0.3126 - val_accuracy: 0.9267\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.2074 - accuracy: 0.9397 - val_loss: 0.2967 - val_accuracy: 0.9397\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 139us/sample - loss: 0.1839 - accuracy: 0.9483 - val_loss: 0.2822 - val_accuracy: 0.9440\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.1667 - accuracy: 0.9526 - val_loss: 0.2676 - val_accuracy: 0.9483\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.1619 - accuracy: 0.9483 - val_loss: 0.2586 - val_accuracy: 0.9483\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1296 - accuracy: 0.9483 - val_loss: 0.2469 - val_accuracy: 0.9526\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1289 - accuracy: 0.9612 - val_loss: 0.2352 - val_accuracy: 0.9569\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1313 - accuracy: 0.9569 - val_loss: 0.2225 - val_accuracy: 0.9612\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.1249 - accuracy: 0.9655 - val_loss: 0.2081 - val_accuracy: 0.9612\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.1219 - accuracy: 0.9612 - val_loss: 0.1924 - val_accuracy: 0.9612\n",
      "232/232 [==============================] - 0s 86us/sample - loss: 0.1924 - accuracy: 0.9612\n",
      "Val score is 0.9612069129943848\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 5ms/sample - loss: 0.9112 - accuracy: 0.4784 - val_loss: 0.6880 - val_accuracy: 0.5431\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.7457 - accuracy: 0.5086 - val_loss: 0.6734 - val_accuracy: 0.5905\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.6849 - accuracy: 0.6078 - val_loss: 0.6603 - val_accuracy: 0.6034\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.6643 - accuracy: 0.6293 - val_loss: 0.6509 - val_accuracy: 0.6293\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 0s 138us/sample - loss: 0.6324 - accuracy: 0.6293 - val_loss: 0.6440 - val_accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 145us/sample - loss: 0.5885 - accuracy: 0.6767 - val_loss: 0.6389 - val_accuracy: 0.6379\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.5834 - accuracy: 0.6509 - val_loss: 0.6327 - val_accuracy: 0.6552\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.5530 - accuracy: 0.7026 - val_loss: 0.6271 - val_accuracy: 0.6897\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.5399 - accuracy: 0.6767 - val_loss: 0.6179 - val_accuracy: 0.7284\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.5379 - accuracy: 0.7284 - val_loss: 0.6083 - val_accuracy: 0.7457\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.5255 - accuracy: 0.7241 - val_loss: 0.5989 - val_accuracy: 0.7414\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 132us/sample - loss: 0.4980 - accuracy: 0.7672 - val_loss: 0.5912 - val_accuracy: 0.7457\n",
      "232/232 [==============================] - 0s 41us/sample - loss: 0.5912 - accuracy: 0.7457\n",
      "Val score is 0.7456896305084229\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.8401 - accuracy: 0.4957 - val_loss: 0.6521 - val_accuracy: 0.6509\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 166us/sample - loss: 0.7035 - accuracy: 0.5560 - val_loss: 0.6330 - val_accuracy: 0.6595\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 129us/sample - loss: 0.6426 - accuracy: 0.6293 - val_loss: 0.6219 - val_accuracy: 0.6767\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 131us/sample - loss: 0.5838 - accuracy: 0.6681 - val_loss: 0.6159 - val_accuracy: 0.6897\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.5513 - accuracy: 0.7069 - val_loss: 0.6099 - val_accuracy: 0.6940\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.5365 - accuracy: 0.7155 - val_loss: 0.5980 - val_accuracy: 0.6940\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 132us/sample - loss: 0.5210 - accuracy: 0.7241 - val_loss: 0.5832 - val_accuracy: 0.7069\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.5010 - accuracy: 0.7543 - val_loss: 0.5756 - val_accuracy: 0.7112\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 144us/sample - loss: 0.4904 - accuracy: 0.7716 - val_loss: 0.5723 - val_accuracy: 0.7198\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 141us/sample - loss: 0.4861 - accuracy: 0.7672 - val_loss: 0.5632 - val_accuracy: 0.7284\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.4656 - accuracy: 0.7802 - val_loss: 0.5526 - val_accuracy: 0.7457\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4875 - accuracy: 0.7716 - val_loss: 0.5498 - val_accuracy: 0.7457\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 146us/sample - loss: 0.4555 - accuracy: 0.8103 - val_loss: 0.5448 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.4607 - accuracy: 0.7629 - val_loss: 0.5288 - val_accuracy: 0.7672\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 143us/sample - loss: 0.4344 - accuracy: 0.7888 - val_loss: 0.5157 - val_accuracy: 0.7802\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.4457 - accuracy: 0.7845 - val_loss: 0.5085 - val_accuracy: 0.7759\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4186 - accuracy: 0.8060 - val_loss: 0.5019 - val_accuracy: 0.7931\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 0s 131us/sample - loss: 0.4117 - accuracy: 0.8017 - val_loss: 0.4926 - val_accuracy: 0.7974\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.4059 - accuracy: 0.8190 - val_loss: 0.4899 - val_accuracy: 0.8060\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.4156 - accuracy: 0.8103 - val_loss: 0.4872 - val_accuracy: 0.8103\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4114 - accuracy: 0.8103 - val_loss: 0.4701 - val_accuracy: 0.8233\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 0s 131us/sample - loss: 0.3877 - accuracy: 0.8276 - val_loss: 0.4602 - val_accuracy: 0.8233\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.3775 - accuracy: 0.8319 - val_loss: 0.4504 - val_accuracy: 0.8233\n",
      "232/232 [==============================] - 0s 47us/sample - loss: 0.4504 - accuracy: 0.8233\n",
      "Val score is 0.8232758641242981\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 1.0026 - accuracy: 0.4784 - val_loss: 0.7211 - val_accuracy: 0.4784\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 188us/sample - loss: 0.4539 - accuracy: 0.8233 - val_loss: 0.5858 - val_accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 145us/sample - loss: 0.3215 - accuracy: 0.8836 - val_loss: 0.5013 - val_accuracy: 0.8836\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 139us/sample - loss: 0.2720 - accuracy: 0.8966 - val_loss: 0.4531 - val_accuracy: 0.9095\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 139us/sample - loss: 0.2626 - accuracy: 0.8879 - val_loss: 0.4178 - val_accuracy: 0.9052\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 140us/sample - loss: 0.2420 - accuracy: 0.9095 - val_loss: 0.3922 - val_accuracy: 0.9138\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.2182 - accuracy: 0.9181 - val_loss: 0.3697 - val_accuracy: 0.9095\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 145us/sample - loss: 0.2112 - accuracy: 0.9138 - val_loss: 0.3476 - val_accuracy: 0.9095\n",
      "232/232 [==============================] - 0s 50us/sample - loss: 0.3476 - accuracy: 0.9095\n",
      "Val score is 0.9094827771186829\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.7208 - accuracy: 0.6164 - val_loss: 0.5606 - val_accuracy: 0.7716\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 192us/sample - loss: 0.5219 - accuracy: 0.7629 - val_loss: 0.5145 - val_accuracy: 0.7974\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.4325 - accuracy: 0.7931 - val_loss: 0.4889 - val_accuracy: 0.8017\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3744 - accuracy: 0.8190 - val_loss: 0.4730 - val_accuracy: 0.8147\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.3434 - accuracy: 0.8362 - val_loss: 0.4563 - val_accuracy: 0.8233\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3210 - accuracy: 0.8621 - val_loss: 0.4423 - val_accuracy: 0.8319\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3021 - accuracy: 0.8578 - val_loss: 0.4247 - val_accuracy: 0.8405\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 129us/sample - loss: 0.3010 - accuracy: 0.8750 - val_loss: 0.4081 - val_accuracy: 0.8448\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.3092 - accuracy: 0.8534 - val_loss: 0.3942 - val_accuracy: 0.8578\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2863 - accuracy: 0.8922 - val_loss: 0.3814 - val_accuracy: 0.8750\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2700 - accuracy: 0.8793 - val_loss: 0.3680 - val_accuracy: 0.8793\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2863 - accuracy: 0.8707 - val_loss: 0.3537 - val_accuracy: 0.8793\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2543 - accuracy: 0.9009 - val_loss: 0.3448 - val_accuracy: 0.8879\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232/232 [==============================] - 0s 142us/sample - loss: 0.2414 - accuracy: 0.8966 - val_loss: 0.3316 - val_accuracy: 0.8879\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.2345 - accuracy: 0.8966 - val_loss: 0.3195 - val_accuracy: 0.8836\n",
      "232/232 [==============================] - 0s 90us/sample - loss: 0.3195 - accuracy: 0.8836\n",
      "Val score is 0.8836206793785095\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.8162 - accuracy: 0.5172 - val_loss: 0.5993 - val_accuracy: 0.7328\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 197us/sample - loss: 0.4912 - accuracy: 0.7802 - val_loss: 0.5180 - val_accuracy: 0.8362\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.3958 - accuracy: 0.8319 - val_loss: 0.4745 - val_accuracy: 0.8578\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.3889 - accuracy: 0.8534 - val_loss: 0.4549 - val_accuracy: 0.8664\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 135us/sample - loss: 0.3398 - accuracy: 0.8621 - val_loss: 0.4413 - val_accuracy: 0.8707\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.3227 - accuracy: 0.8922 - val_loss: 0.4285 - val_accuracy: 0.8793\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3166 - accuracy: 0.8966 - val_loss: 0.4229 - val_accuracy: 0.8922\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.3050 - accuracy: 0.8922 - val_loss: 0.4157 - val_accuracy: 0.8966\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 140us/sample - loss: 0.2955 - accuracy: 0.8922 - val_loss: 0.3989 - val_accuracy: 0.8966\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 140us/sample - loss: 0.2833 - accuracy: 0.9009 - val_loss: 0.3852 - val_accuracy: 0.8966\n",
      "232/232 [==============================] - 0s 100us/sample - loss: 0.3852 - accuracy: 0.8966\n",
      "Val score is 0.8965517282485962\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.6490 - accuracy: 0.6767 - val_loss: 0.5836 - val_accuracy: 0.7543\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 198us/sample - loss: 0.5071 - accuracy: 0.7586 - val_loss: 0.5525 - val_accuracy: 0.7672\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 137us/sample - loss: 0.4800 - accuracy: 0.7586 - val_loss: 0.5361 - val_accuracy: 0.7802\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 136us/sample - loss: 0.4574 - accuracy: 0.7845 - val_loss: 0.5264 - val_accuracy: 0.7802\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4434 - accuracy: 0.8017 - val_loss: 0.5157 - val_accuracy: 0.7802\n",
      "232/232 [==============================] - 0s 51us/sample - loss: 0.5157 - accuracy: 0.7802\n",
      "Val score is 0.7801724076271057\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/100\n",
      "232/232 [==============================] - 1s 4ms/sample - loss: 0.8255 - accuracy: 0.5733 - val_loss: 0.7300 - val_accuracy: 0.5259\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 0s 171us/sample - loss: 0.6633 - accuracy: 0.6552 - val_loss: 0.6766 - val_accuracy: 0.5948\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.6030 - accuracy: 0.7284 - val_loss: 0.6323 - val_accuracy: 0.6897\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 0s 134us/sample - loss: 0.5655 - accuracy: 0.7328 - val_loss: 0.5926 - val_accuracy: 0.7414\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 0s 142us/sample - loss: 0.5280 - accuracy: 0.7457 - val_loss: 0.5579 - val_accuracy: 0.7974\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 0s 139us/sample - loss: 0.4962 - accuracy: 0.8017 - val_loss: 0.5305 - val_accuracy: 0.8103\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 0s 133us/sample - loss: 0.4691 - accuracy: 0.7716 - val_loss: 0.5050 - val_accuracy: 0.8103\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4551 - accuracy: 0.8103 - val_loss: 0.4819 - val_accuracy: 0.8233\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.4373 - accuracy: 0.8103 - val_loss: 0.4591 - val_accuracy: 0.8190\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 0s 138us/sample - loss: 0.3962 - accuracy: 0.8491 - val_loss: 0.4401 - val_accuracy: 0.8190\n",
      "232/232 [==============================] - 0s 91us/sample - loss: 0.4401 - accuracy: 0.8190\n",
      "Val score is 0.818965494632721\n",
      "DataFrame: housevotes imput_method :MLP model :KNN score:  0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: housevotes imput_method :MLP model :XGB score:  0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: housevotes imput_method :MLP model :MLP score:  0.95\n",
      "DataFrame: mammographic imput_method :LOCF model :KNN score:  0.82\n",
      "DataFrame: mammographic imput_method :LOCF model :XGB score:  0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: mammographic imput_method :LOCF model :MLP score:  0.8\n",
      "DataFrame: mammographic imput_method :mean_mode model :KNN score:  0.82\n",
      "DataFrame: mammographic imput_method :mean_mode model :XGB score:  0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: mammographic imput_method :mean_mode model :MLP score:  0.8\n",
      "DataFrame: mammographic imput_method :knn model :KNN score:  0.83\n",
      "DataFrame: mammographic imput_method :knn model :XGB score:  0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: mammographic imput_method :knn model :MLP score:  0.8\n",
      "Best score is -0.3784435674190023\n",
      "Best score is -10.442015800870422\n",
      "Best score is -0.6410335352343898\n",
      "Best score is -0.7143650603711794\n",
      "Best score is -0.19099122871786864\n",
      "DataFrame: mammographic imput_method :trees model :KNN score:  0.82\n",
      "DataFrame: mammographic imput_method :trees model :XGB score:  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: mammographic imput_method :trees model :MLP score:  0.8\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 2s 2ms/sample - loss: 4.2141 - mae: 4.2141 - val_loss: 3.9541 - val_mae: 3.9541\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 3.9066 - mae: 3.9065 - val_loss: 3.6835 - val_mae: 3.6835\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 3.5202 - mae: 3.5202 - val_loss: 3.2895 - val_mae: 3.2895\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 3.0106 - mae: 3.0106 - val_loss: 2.7859 - val_mae: 2.7859\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 2.3770 - mae: 2.3770 - val_loss: 2.0463 - val_mae: 2.0463\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 1.6399 - mae: 1.6399 - val_loss: 1.2001 - val_mae: 1.2001\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 0.9345 - mae: 0.9345 - val_loss: 0.5592 - val_mae: 0.5592\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 0.6332 - mae: 0.6332 - val_loss: 0.5561 - val_mae: 0.5561\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.5795 - mae: 0.5795 - val_loss: 0.5191 - val_mae: 0.5191\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.4933 - mae: 0.4933 - val_loss: 0.4796 - val_mae: 0.4796\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.4672 - mae: 0.4672 - val_loss: 0.5186 - val_mae: 0.5186\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.4471 - mae: 0.4471 - val_loss: 0.4189 - val_mae: 0.4189\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.4054 - mae: 0.4054 - val_loss: 0.4203 - val_mae: 0.4203\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.4157 - mae: 0.4157 - val_loss: 0.3823 - val_mae: 0.3823\n",
      "Epoch 15/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.4075 - mae: 0.4075 - val_loss: 0.3636 - val_mae: 0.3636\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.4023 - mae: 0.4023 - val_loss: 0.3568 - val_mae: 0.3568\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.3931 - mae: 0.3931 - val_loss: 0.3698 - val_mae: 0.3698\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.3791 - mae: 0.3791 - val_loss: 0.3102 - val_mae: 0.3102\n",
      "Epoch 19/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.3823 - mae: 0.3823 - val_loss: 0.3617 - val_mae: 0.3617\n",
      "Epoch 20/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.3617 - mae: 0.3617 - val_loss: 0.3942 - val_mae: 0.3942\n",
      "830/830 [==============================] - 0s 45us/sample - loss: 0.3942 - mae: 0.3942\n",
      "Val score is 0.3942260146141052\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 1s 1ms/sample - loss: 55.6559 - mae: 55.6559 - val_loss: 55.7621 - val_mae: 55.7621\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 131us/sample - loss: 55.3294 - mae: 55.3294 - val_loss: 55.2515 - val_mae: 55.2514\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 137us/sample - loss: 54.8769 - mae: 54.8769 - val_loss: 54.6030 - val_mae: 54.6030\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 54.2746 - mae: 54.2746 - val_loss: 53.8098 - val_mae: 53.8098\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 53.5101 - mae: 53.5101 - val_loss: 52.8926 - val_mae: 52.8926\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 129us/sample - loss: 52.5766 - mae: 52.5766 - val_loss: 51.7841 - val_mae: 51.7841\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 139us/sample - loss: 51.4698 - mae: 51.4698 - val_loss: 50.5606 - val_mae: 50.5606\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 50.1875 - mae: 50.1875 - val_loss: 49.1980 - val_mae: 49.1980\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 48.7285 - mae: 48.7285 - val_loss: 47.6729 - val_mae: 47.6729\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 47.0924 - mae: 47.0924 - val_loss: 45.9323 - val_mae: 45.9323\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 45.2792 - mae: 45.2792 - val_loss: 44.0611 - val_mae: 44.0611\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 131us/sample - loss: 43.2896 - mae: 43.2896 - val_loss: 42.0329 - val_mae: 42.0329\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 41.1243 - mae: 41.1243 - val_loss: 39.8836 - val_mae: 39.8836\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 38.7883 - mae: 38.7883 - val_loss: 37.9535 - val_mae: 37.9535\n",
      "Epoch 15/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 36.2992 - mae: 36.2992 - val_loss: 33.9450 - val_mae: 33.9450\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 33.6268 - mae: 33.6268 - val_loss: 30.0619 - val_mae: 30.0619\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 30.7887 - mae: 30.7887 - val_loss: 27.4369 - val_mae: 27.4369\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 27.8331 - mae: 27.8331 - val_loss: 25.9795 - val_mae: 25.9795\n",
      "Epoch 19/100\n",
      "830/830 [==============================] - 0s 127us/sample - loss: 24.8107 - mae: 24.8107 - val_loss: 23.9163 - val_mae: 23.9163\n",
      "Epoch 20/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 21.9189 - mae: 21.9189 - val_loss: 21.5958 - val_mae: 21.5958\n",
      "Epoch 21/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 19.2640 - mae: 19.2640 - val_loss: 17.4412 - val_mae: 17.4412\n",
      "Epoch 22/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 16.8486 - mae: 16.8486 - val_loss: 15.5034 - val_mae: 15.5034\n",
      "Epoch 23/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 14.7506 - mae: 14.7506 - val_loss: 13.5587 - val_mae: 13.5587\n",
      "Epoch 24/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 13.2614 - mae: 13.2614 - val_loss: 12.7397 - val_mae: 12.7397\n",
      "Epoch 25/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 12.1312 - mae: 12.1312 - val_loss: 12.1940 - val_mae: 12.1940\n",
      "Epoch 26/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 11.2582 - mae: 11.2582 - val_loss: 10.8459 - val_mae: 10.8459\n",
      "Epoch 27/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 10.8906 - mae: 10.8906 - val_loss: 10.4081 - val_mae: 10.4081\n",
      "Epoch 28/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 10.5332 - mae: 10.5332 - val_loss: 10.2682 - val_mae: 10.2682\n",
      "Epoch 29/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 10.2838 - mae: 10.2838 - val_loss: 10.1166 - val_mae: 10.1166\n",
      "Epoch 30/100\n",
      "830/830 [==============================] - 0s 127us/sample - loss: 10.1818 - mae: 10.1818 - val_loss: 10.0893 - val_mae: 10.0893\n",
      "Epoch 31/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 10.1476 - mae: 10.1476 - val_loss: 9.9683 - val_mae: 9.9683\n",
      "Epoch 32/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 10.0784 - mae: 10.0784 - val_loss: 9.8781 - val_mae: 9.8781\n",
      "Epoch 33/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 10.0494 - mae: 10.0494 - val_loss: 9.8375 - val_mae: 9.8375\n",
      "Epoch 34/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 9.9923 - mae: 9.9923 - val_loss: 9.8666 - val_mae: 9.8666\n",
      "Epoch 35/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 10.0349 - mae: 10.0349 - val_loss: 9.8440 - val_mae: 9.8440\n",
      "830/830 [==============================] - 0s 39us/sample - loss: 9.8440 - mae: 9.8440\n",
      "Val score is 9.843972206115723\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - 1s 1ms/sample - loss: 2.7289 - mae: 2.7289 - val_loss: 2.4362 - val_mae: 2.4362\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 129us/sample - loss: 2.4148 - mae: 2.4148 - val_loss: 2.3223 - val_mae: 2.3223\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 2.1089 - mae: 2.1089 - val_loss: 2.1427 - val_mae: 2.1427\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 1.7834 - mae: 1.7834 - val_loss: 1.8693 - val_mae: 1.8693\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 1.4187 - mae: 1.4187 - val_loss: 1.5485 - val_mae: 1.5485\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 1.0404 - mae: 1.0404 - val_loss: 1.2541 - val_mae: 1.2541\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.7910 - mae: 0.7910 - val_loss: 1.0143 - val_mae: 1.0143\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.6951 - mae: 0.6951 - val_loss: 0.8464 - val_mae: 0.8464\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.6825 - mae: 0.6825 - val_loss: 0.7716 - val_mae: 0.7716\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 118us/sample - loss: 0.6570 - mae: 0.6570 - val_loss: 0.7411 - val_mae: 0.7411\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.6471 - mae: 0.6471 - val_loss: 0.7204 - val_mae: 0.7204\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.6135 - mae: 0.6135 - val_loss: 0.6098 - val_mae: 0.6098\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 0.6164 - mae: 0.6164 - val_loss: 0.6315 - val_mae: 0.6315\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.6203 - mae: 0.6203 - val_loss: 0.5982 - val_mae: 0.5982\n",
      "Epoch 15/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.6111 - mae: 0.6111 - val_loss: 0.5453 - val_mae: 0.5453\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.6028 - mae: 0.6028 - val_loss: 0.5662 - val_mae: 0.5662\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.6036 - mae: 0.6036 - val_loss: 0.5307 - val_mae: 0.5307\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.6139 - mae: 0.6139 - val_loss: 0.5174 - val_mae: 0.5174\n",
      "Epoch 19/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.5835 - mae: 0.5835 - val_loss: 0.5515 - val_mae: 0.5515\n",
      "Epoch 20/100\n",
      "830/830 [==============================] - 0s 131us/sample - loss: 0.5936 - mae: 0.5936 - val_loss: 0.5144 - val_mae: 0.5144\n",
      "Epoch 21/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.5709 - mae: 0.5709 - val_loss: 0.5313 - val_mae: 0.5313\n",
      "Epoch 22/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.5720 - mae: 0.5720 - val_loss: 0.5142 - val_mae: 0.5142\n",
      "Epoch 23/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.5845 - mae: 0.5845 - val_loss: 0.5642 - val_mae: 0.5642\n",
      "Epoch 24/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.5957 - mae: 0.5957 - val_loss: 0.5624 - val_mae: 0.5624\n",
      "830/830 [==============================] - 0s 44us/sample - loss: 0.5624 - mae: 0.5624\n",
      "Val score is 0.5624348521232605\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 1s 2ms/sample - loss: 2.7489 - mae: 2.7489 - val_loss: 2.4499 - val_mae: 2.4499\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 142us/sample - loss: 2.4145 - mae: 2.4145 - val_loss: 2.2048 - val_mae: 2.2048\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 2.0944 - mae: 2.0944 - val_loss: 1.9185 - val_mae: 1.9185\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 1.7087 - mae: 1.7087 - val_loss: 1.6195 - val_mae: 1.6195\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 1.3217 - mae: 1.3217 - val_loss: 1.3301 - val_mae: 1.3301\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 1.0200 - mae: 1.0200 - val_loss: 1.2468 - val_mae: 1.2468\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.8678 - mae: 0.8678 - val_loss: 1.2024 - val_mae: 1.2024\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 122us/sample - loss: 0.7769 - mae: 0.7769 - val_loss: 1.0387 - val_mae: 1.0387\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 124us/sample - loss: 0.7441 - mae: 0.7441 - val_loss: 0.9172 - val_mae: 0.9172\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.7605 - mae: 0.7605 - val_loss: 0.8557 - val_mae: 0.8557\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.7800 - mae: 0.7800 - val_loss: 0.9497 - val_mae: 0.9497\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 139us/sample - loss: 0.7376 - mae: 0.7376 - val_loss: 0.7989 - val_mae: 0.7989\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 133us/sample - loss: 0.6926 - mae: 0.6926 - val_loss: 0.7413 - val_mae: 0.7413\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 134us/sample - loss: 0.6829 - mae: 0.6829 - val_loss: 0.7580 - val_mae: 0.7580\n",
      "Epoch 15/100\n",
      "830/830 [==============================] - 0s 135us/sample - loss: 0.7105 - mae: 0.7105 - val_loss: 0.7404 - val_mae: 0.7404\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 130us/sample - loss: 0.7058 - mae: 0.7058 - val_loss: 0.6606 - val_mae: 0.6606\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 135us/sample - loss: 0.7780 - mae: 0.7780 - val_loss: 0.6156 - val_mae: 0.6156\n",
      "Epoch 18/100\n",
      "830/830 [==============================] - 0s 132us/sample - loss: 0.6854 - mae: 0.6854 - val_loss: 0.6542 - val_mae: 0.6542\n",
      "Epoch 19/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 0.6721 - mae: 0.6721 - val_loss: 0.6256 - val_mae: 0.6256\n",
      "830/830 [==============================] - 0s 41us/sample - loss: 0.6256 - mae: 0.6256\n",
      "Val score is 0.625557541847229\n",
      "Train on 830 samples, validate on 830 samples\n",
      "Epoch 1/100\n",
      "830/830 [==============================] - 1s 1ms/sample - loss: 2.8097 - mae: 2.8097 - val_loss: 2.4492 - val_mae: 2.4492\n",
      "Epoch 2/100\n",
      "830/830 [==============================] - 0s 128us/sample - loss: 2.5388 - mae: 2.5388 - val_loss: 2.3075 - val_mae: 2.3075\n",
      "Epoch 3/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 2.2010 - mae: 2.2010 - val_loss: 2.0445 - val_mae: 2.0445\n",
      "Epoch 4/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 1.7499 - mae: 1.7499 - val_loss: 1.5918 - val_mae: 1.5918\n",
      "Epoch 5/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 1.2024 - mae: 1.2024 - val_loss: 1.1164 - val_mae: 1.1164\n",
      "Epoch 6/100\n",
      "830/830 [==============================] - 0s 119us/sample - loss: 0.6919 - mae: 0.6919 - val_loss: 0.5892 - val_mae: 0.5892\n",
      "Epoch 7/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.4680 - mae: 0.4680 - val_loss: 0.4353 - val_mae: 0.4353\n",
      "Epoch 8/100\n",
      "830/830 [==============================] - 0s 123us/sample - loss: 0.4112 - mae: 0.4112 - val_loss: 0.4384 - val_mae: 0.4384\n",
      "Epoch 9/100\n",
      "830/830 [==============================] - 0s 146us/sample - loss: 0.3742 - mae: 0.3742 - val_loss: 0.3557 - val_mae: 0.3557\n",
      "Epoch 10/100\n",
      "830/830 [==============================] - 0s 142us/sample - loss: 0.3273 - mae: 0.3273 - val_loss: 0.3461 - val_mae: 0.3461\n",
      "Epoch 11/100\n",
      "830/830 [==============================] - 0s 125us/sample - loss: 0.2912 - mae: 0.2912 - val_loss: 0.3015 - val_mae: 0.3015\n",
      "Epoch 12/100\n",
      "830/830 [==============================] - 0s 121us/sample - loss: 0.2763 - mae: 0.2763 - val_loss: 0.2631 - val_mae: 0.2631\n",
      "Epoch 13/100\n",
      "830/830 [==============================] - 0s 120us/sample - loss: 0.2804 - mae: 0.2804 - val_loss: 0.2546 - val_mae: 0.2546\n",
      "Epoch 14/100\n",
      "830/830 [==============================] - 0s 126us/sample - loss: 0.2670 - mae: 0.2670 - val_loss: 0.2352 - val_mae: 0.2352\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/830 [==============================] - 0s 145us/sample - loss: 0.2575 - mae: 0.2575 - val_loss: 0.1870 - val_mae: 0.1870\n",
      "Epoch 16/100\n",
      "830/830 [==============================] - 0s 132us/sample - loss: 0.2359 - mae: 0.2359 - val_loss: 0.1975 - val_mae: 0.1975\n",
      "Epoch 17/100\n",
      "830/830 [==============================] - 0s 127us/sample - loss: 0.2570 - mae: 0.2570 - val_loss: 0.2036 - val_mae: 0.2036\n",
      "830/830 [==============================] - 0s 39us/sample - loss: 0.2036 - mae: 0.2036\n",
      "Val score is 0.20356497168540955\n",
      "DataFrame: mammographic imput_method :MLP model :KNN score:  0.82\n",
      "DataFrame: mammographic imput_method :MLP model :XGB score:  0.83\n",
      "DataFrame: mammographic imput_method :MLP model :MLP score:  0.81\n",
      "DataFrame: wisconsin imput_method :LOCF model :KNN score:  0.95\n",
      "DataFrame: wisconsin imput_method :LOCF model :XGB score:  0.96\n",
      "DataFrame: wisconsin imput_method :LOCF model :MLP score:  0.95\n",
      "DataFrame: wisconsin imput_method :mean_mode model :KNN score:  0.95\n",
      "DataFrame: wisconsin imput_method :mean_mode model :XGB score:  0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: wisconsin imput_method :mean_mode model :MLP score:  0.96\n",
      "DataFrame: wisconsin imput_method :knn model :KNN score:  0.95\n",
      "DataFrame: wisconsin imput_method :knn model :XGB score:  0.95\n",
      "DataFrame: wisconsin imput_method :knn model :MLP score:  0.95\n",
      "DataFrame: wisconsin imput_method :trees model :KNN score:  0.94\n",
      "DataFrame: wisconsin imput_method :trees model :XGB score:  0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seema\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: wisconsin imput_method :trees model :MLP score:  0.96\n",
      "DataFrame: wisconsin imput_method :MLP model :KNN score:  0.95\n",
      "DataFrame: wisconsin imput_method :MLP model :XGB score:  0.96\n",
      "DataFrame: wisconsin imput_method :MLP model :MLP score:  0.95\n",
      "Tiempo total:  0:36:42.789563\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "#importa los conjuntos de datos\n",
    "names=['automobile','bands','breast','cleveland','crx','dermatology','hepatitis','housevotes','mammographic','wisconsin']\n",
    "methods=['LOCF','mean_mode','knn','trees','MLP']\n",
    "models=['KNN','XGB','MLP']\n",
    "results_LOCF=[]\n",
    "results_mean_mode=[]\n",
    "results_knn=[]\n",
    "results_trees=[]\n",
    "results_MLP=[]\n",
    "for name in names: #abre los conjuntos de datos\n",
    "    DataFrame = pd.read_csv(name+'.csv', header=None, na_values=[' ','  ','?',None])\n",
    "    for method in methods: # imputa datos faltantes con cada metodo\n",
    "        df_complete_i = MissingValueImputation(method = method).fit_transform(DataFrame)\n",
    "        for model in models: #ajusta modelos de Machine Learning\n",
    "            result=Data_Prer_Mod_train(df_complete_i,model)\n",
    "            #imprime y guarda resultados\n",
    "            print('DataFrame: '+name+' imput_method :'+method+' model :'+model+' score: ',result)\n",
    "            if method=='LOCF':\n",
    "                results_LOCF.append(result)\n",
    "            elif method=='mean_mode':\n",
    "                results_mean_mode.append(result)\n",
    "            elif method=='knn':\n",
    "                results_knn.append(result)\n",
    "            elif method=='trees':\n",
    "                results_trees.append(result)\n",
    "            elif method=='MLP':\n",
    "                results_MLP.append(result)\n",
    "        df_clean(df_complete_i) #libera memoria\n",
    "    df_clean(DataFrame) #libera memoria\n",
    "print('Tiempo total: ',datetime.now() - startTime) #tiempo total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guarda los resultados\n",
    "results_dict={'LOCF':results_LOCF,\n",
    "              'mean_mode':results_mean_mode,\n",
    "              'knn':results_knn,\n",
    "              'trees': results_trees,\n",
    "              'MLP': results_MLP\n",
    "             }\n",
    "\n",
    "df_results=pd.DataFrame(results_dict)\n",
    "df_results.to_csv('Results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Análisis estadístico de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('Results.csv',  index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOCF</th>\n",
       "      <th>mean_mode</th>\n",
       "      <th>knn</th>\n",
       "      <th>trees</th>\n",
       "      <th>MLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.790333</td>\n",
       "      <td>0.790667</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.796667</td>\n",
       "      <td>0.790333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.143875</td>\n",
       "      <td>0.141834</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.136416</td>\n",
       "      <td>0.141214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.685000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LOCF  mean_mode        knn      trees        MLP\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000\n",
       "mean    0.790333   0.790667   0.794000   0.796667   0.790333\n",
       "std     0.143875   0.141834   0.141509   0.136416   0.141214\n",
       "min     0.490000   0.500000   0.530000   0.540000   0.530000\n",
       "25%     0.685000   0.670000   0.670000   0.700000   0.662500\n",
       "50%     0.820000   0.820000   0.830000   0.820000   0.820000\n",
       "75%     0.945000   0.945000   0.945000   0.940000   0.942500\n",
       "max     0.970000   0.960000   0.970000   0.960000   0.960000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29f5b1dae10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD4CAYAAADmWv3KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAZfUlEQVR4nO3de3hV9b3n8fc3MTTcij0SUYw0qXDAkIRwCcJQrlaigjgocxRkLFKK1pGBOlBvx8vzjPK0pWOPg1MFK1XkYKmoaKNHgXO4VMRyDRKlXNSoqY4gPqBQU7l854/ETJBAVpK99kpWPq/nyfPsvfZa6/f9JfJx7d/+7d8yd0dEROInJeoCREQkHAp4EZGYUsCLiMSUAl5EJKYU8CIiMXVG1AXU1LFjR8/Kyoq6DBGRZmPz5s2funtGba81qYDPyspi06ZNUZchItJsmNn7p3pNQzQiIjGlgBcRiSkFvIhITDWpMfjaHDlyhPLycioqKqIupUVKT08nMzOTtLS0qEsRkXpq8gFfXl5O+/btycrKwsyiLqdFcXf2799PeXk52dnZUZcjIvXU5IdoKioqOOussxTuETAzzjrrLL17EmmmmnzAAwr3COl3L9J8NYuAFxGR+mvyY/DfdMXc1xJ6vj9O+36d+7Rr145Dhw6dsO3gwYNMmzaNdevWATBo0CDmzp1Lhw4dANi1axczZsxg165dpKWlkZeXx9y5c9mxYwdXXnll9Zh2x44dWblyZUL7JCICzTDgm4of/ehH5ObmsnDhQgDuvfdepkyZwjPPPENFRQWjRo3iwQcf5IorrgBg1apV7Nu3D4DBgwdTXFwcWe0iUrsVQ3MiafeSNW+Hcl4FfAPs2bOHzZs3s2TJkupt99xzD127duWdd95hzZo1DBw4sDrcAYYPHw7A6tWrk12uiLRQGoNvgLfffpuCggJSU1Ort6WmplJQUMBbb71FaWkpffv2PeXxf/rTnygoKKCgoIAHHnggGSWLSAukK/gGcPdaZ5ecavs3aYhGRJJBAd8APXv2ZOvWrRw/fpyUlMo3QcePH2fbtm1ceOGF7N27lzVr1kRcpYi0dBqiaYCuXbvSu3dv7r///upt999/P3369KFr165MmDCB119/nZdeeqn69VdeeYXt27dHUa6ItFDN7go+yLTGRPvb3/5GZmZm9fNbb72Vxx9/nGnTptG1a1fcnYEDB/L4448D0Lp1a4qLi5kxYwYzZswgLS2N/Px8HnroIfbv35/0+kWkZWp2AR+F48eP17p90aJFpzymR48evPLKKydt79SpE8OGDUtUaSIip6QhGhGRmFLAi4jElAJeRCSmFPAiIjGlgBcRiSkFvIhITDW/aZLzhib2fDfqG6ciEk+6gm9hJk2axNKlS6MuQ0SSQAEvIhJTCvg6lJWV0aNHD6ZMmUJubi7XXXcdK1euZNCgQXTr1o0NGzZw+PBhJk+eTGFhIb179+aFF16oPnbw4MH06dOHPn368PrrrwOVa8IPGzaMcePG0aNHD6677jrc/ZQ1ZGVlceeddzJw4ED69evHli1bKCoq4oILLuDRRx8FKleynDVrFrm5ueTl5VWvVe/u3HLLLeTk5DBq1Cj27t1bfd7NmzczdOhQ+vbtS1FRER9//HFYv0YRiUDzG4OPwJ49e3jmmWeYP38+hYWFLF68mNdee40XX3yR2bNnk5OTw4gRI1iwYAEHDhygf//+/OAHP+Dss89mxYoVpKens3v3bsaPH8+mTZsA2Lp1K2+99RadO3dm0KBBrFu3ju9//9Tr7Jx//vmsX7+en/70p0yaNIl169ZRUVFBz549uemmm3juuecoKSlh27ZtfPrppxQWFjJkyBDWr1/Pzp072b59O5988gk5OTlMnjyZI0eOMG3aNF544QUyMjJYsmQJd911FwsWLEjWr1VEQqaADyA7O5u8vDygcqngiy++GDMjLy+PsrIyysvLefHFF/nVr34FQEVFBR988AGdO3fmlltuoaSkhNTUVHbt2lV9zv79+1cvYFZQUEBZWdlpA37MmDEA5OXlcejQIdq3b0/79u1JT0/nwIEDvPbaa4wfP57U1FQ6derE0KFD2bhxI2vXrq3e3rlzZ0aMGAHAzp07KS0t5ZJLLgHg2LFjnHvuuYn/5YlIZEINeDP7KTAFcGA7cIO7V4TZZhi+9a1vVT9OSUmpfp6SksLRo0dJTU3l2WefpXv37iccd99999GpUye2bdvG8ePHSU9Pr/WcqampHD16NFANNduvWcPphnhOdXOSnj17sn79+tO2KyLNV2gBb2bnAf8dyHH3L83sD8C1wBONOnETnNZYVFTE3LlzmTt3LmbG1q1b6d27NwcPHiQzM5OUlBSefPJJjh07FloNQ4YMYd68efzwhz/ks88+Y+3atcyZM4ejR48yb948rr/+evbu3cuqVauYMGEC3bt3Z9++faxfv56BAwdy5MgRdu3aRc+ePUOrUUSSK+wPWc8AWpvZGUAb4KOQ24vE3XffzZEjR8jPzyc3N5e7774bgJtvvpknn3ySAQMGsGvXLtq2bRtaDWPHjiU/P59evXoxYsQIfvnLX3LOOecwduxYunXrRl5eHj/5yU8YOrTyewStWrVi6dKl3HbbbfTq1YuCgoLqD4FFJB7sdG/tG31ys+nAA8CXwHJ3v66WfaYCUwG6dOnS9/333z/h9R07dnDhhReGVqPUTX8DaSl++8MnIml3ypOTGnysmW129361vRbaFbyZfQe4EsgGOgNtzWziN/dz9/nu3s/d+2VkZIRVjohIixPmh6w/AN5z930AZvYc8J+AU98GqYUbO3Ys77333gnbfvGLX1BUVBRRRSLSnIUZ8B8AA8ysDZVDNBcDm0Jsr9l7/vnnoy5BRGIktCEad/8zsBTYQuUUyRRgfljtiYjIiUKdB+/u9wL3htmGiIjUTmvRiIjEVLNbquCa4msSer4lo5ec9vWysjJGjx5NaWlpQtsVEQmbruBFRGJKAV8P7777Lr1792bOnDlcddVVXHrppXTr1o2f/exn1fu0a9eOu+66i169ejFgwAA++eSTCCsWkZZMAR/Qzp07ufrqq/nd735HRkYGJSUlLFmyhO3bt7NkyRI+/PBDAA4fPsyAAQPYtm0bQ4YM4bHHHou4chFpqRTwAezbt48rr7ySRYsWUVBQAMDFF19Mhw4dSE9PJycnh6+XWGjVqhWjR48GoG/fvpSVlUVVtoi0cAr4ADp06MD555/PunXrqredarnftLS06uV5gywDLCISlmY3iyYKrVq1YtmyZRQVFdGuXbuoyxERCaTZBXxd0xrD0rZtW4qLi7nkkkuYOPGkNdNERJqcZhfwyZaVlVU9B/7MM89k48aNJ+1TXFxc/fjQoUPVj8eNG8e4cePCL1JEpBYagxcRiSkFvIhITCngRURiSgEvIhJTCngRkZhSwIuIxFSzmyb53tWJnXaY/ezS075+4MABFi9ezM0335zQdkVEwtbsAj7ZDhw4wG9+85uTAv7YsWOkpqZGVJVI+BJ974WgovoyYxxpiKYOt99+O++88w4FBQUUFhYyfPhwJkyYQF5eHseOHWPWrFkUFhaSn5/PvHnzqo+bM2dO9fZ77628a+Hhw4cZNWoUvXr1Ijc3lyVL9B+yiIRHV/B1+PnPf05paSklJSWsXr2aUaNGUVpaSnZ2NvPnz6dDhw5s3LiRv//97wwaNIiRI0eye/dudu/ezYYNG3B3xowZw9q1a9m3bx+dO3fmpZdeAuDgwYMR905E4kwBX0/9+/cnOzsbgOXLl/Pmm2+ydGnlOP7BgwfZvXs3y5cvZ/ny5fTu3RuoXL5g9+7dDB48mJkzZ3LbbbcxevRoBg8eHFk/RCT+FPD11LZt2+rH7s7cuXMpKio6YZ9XX32VO+64gxtvvPGk4zdv3szLL7/MHXfcwciRI7nnnntCr1lEWiaNwdehffv2fPHFF7W+VlRUxCOPPMKRI0cA2LVrF4cPH6aoqIgFCxZULzz217/+lb179/LRRx/Rpk0bJk6cyMyZM9myZUvS+iEiLU+zu4Kva1pjop111lkMGjSI3NxcWrduTadOnapfmzJlCmVlZfTp0wd3JyMjg2XLljFy5Eh27NjBwIEDgcr7tC5atIg9e/Ywa9YsUlJSSEtL45FHHklqX0SkZWl2AR+FxYsX17o9JSWF2bNnM3v27JNemz59OtOnTz9h2wUXXHDScI6ISFg0RCMiElMKeBGRmFLAi4jElAJeRCSmFPAiIjGlgBcRialmN03yD7M3JvR8/3RnYZ37mBkTJ07kqaeeAuDo0aOce+65XHTRRRQXF/PEE0+wadMmHn744ROOy8rKon379qSkpNCpUycWLlzIOeeck9D6RURORVfwAbRt25bS0lK+/PJLAFasWMF5550X6NhVq1axbds2+vXrV+t8eRGRsCjgA7rsssuqV4F8+umnGT9+fL2OHzJkCHv27AmjNBGRWingA7r22mv5/e9/T0VFBW+++SYXXXRRvY4vLi4mLy8vpOpERE7W7Mbgo5Kfn09ZWRlPP/00l19+eeDjhg8fTmpqKvn5+dx///0hVigiciIFfD2MGTOGmTNnsnr1avbv3x/omFWrVtGxY8eQKxMROVmoAW9mZwK/BXIBBya7+/ow2wzT5MmT6dChA3l5eaxevTrqckRETivsK/iHgFfcfZyZtQLaNPaEQaY1hiUzM/OkFSK/9sQTT7Bs2bLq52+88UayyhIRqVVoAW9m3waGAJMA3P0r4Kuw2gvT1zfuqGnYsGEMGzYMgEmTJjFp0qST9ikrKwu3MBGR0wjzCv57wD7gd2bWC9gMTHf3wzV3MrOpwFSALl26hFiOSDM0b2h0bZ+nL+U1d2FOkzwD6AM84u69gcPA7d/cyd3nu3s/d++XkZERYjkiIi1LmAFfDpS7+5+rni+lMvDrzd0TVpTUj373Is1XaAHv7v8X+NDMuldtuhh4u77nSU9PZ//+/QqaCLg7+/fvJz09PepSRKQBwp5FMw3416oZNO8CN9T3BJmZmZSXl7Nv376EFyd1S09PJzMzM+oyRKQBQg14dy8B+jXmHGlpaWRnZyeoIhGRliPQEI2Z5YZdiIiIJFbQMfhHzWyDmd1c9e1UERFp4gIFvLt/H7gOOB/YZGaLzeySUCsTEZFGCTyLxt13A/8M3AYMBf63mf3FzK4KqzgREWm4oGPw+Wb2a2AHMAK4wt0vrHr86xDrExGRBgo6i+Zh4DHgTnf/8uuN7v6Rmf1zKJWJiEijBA34y4Ev3f0YgJmlAOnu/jd3fyq06kREpMGCjsGvBFrXeN6mapuIiDRRQQM+3d2r18ytetzotd1FRCQ8QQP+sJlVLxRmZn2BL0+zv4iIRCzoGPwM4Bkz+6jq+bnANeGUJCIiiRAo4N19o5n1ALoDBvzF3Y+EWpmIiDRKfRYbKwSyqo7pbWa4+8JQqhIRkUYLFPBm9hRwAVACHKva7IACXkSkiQp6Bd8PyHHddUNEpNkIOoumFNAdeEVEmpGgV/AdgbfNbAPw9683uvuYUKoSEZFGCxrw94VZhIjUbvfeQ3XvFJKR6y+PpuHR0TQbR0GnSa4xs+8C3dx9pZm1AVLDLU1ERBoj6HLBPwaWAvOqNp0HLAurKBERabygH7L+N2AQ8DlU3/zj7LCKEhGRxgsa8H9396++fmJmZ1A5D15ERJqooAG/xszuBFpX3Yv1GeCP4ZUlIiKNFTTgbwf2AduBG4GXqbw/q4iINFFBZ9Ecp/KWfY+FW46IiCRK0LVo3qOWMXd3/17CKxIRkYSoz1o0X0sH/gvwD4kvR0REEiXQGLy776/x81d3/xdgRMi1iYhIIwQdoulT42kKlVf07UOpSEREEiLoEM3/qvH4KFAG/FPCqxERkYQJOotmeNiFiIhIYgUdorn1dK+7+4OJKUdERBKlPrNoCoEXq55fAawFPgyjKBERabz63PCjj7t/AWBm9wHPuPuUsAoTEZHGCbpUQRfgqxrPvwKyEl6NiIgkTNAr+KeADWb2PJXfaB0LLAytKhERabSgs2geMLN/AwZXbbrB3beGV5aIiDRW0CEagDbA5+7+EFBuZtlBDjKzVDPbambFDapQREQaJOgt++4FbgPuqNqUBiwK2MZ0YEf9SxMRkcYIegU/FhgDHAZw948IsFSBmWUCo4DfNrRAERFpmKAfsn7l7m5mDmBmbQMe9y/AzzjN/wzMbCowFaBLly4BT9t0vHf1uMjazn52aSTt/uf5+ZG0C/DrV/8xknaj+l2v3z0tknYBjltkTUuCBL2C/4OZzQPONLMfAyup4+YfZjYa2Ovum0+3n7vPd/d+7t4vIyMjYDkiIlKXOq/gzcyAJUAP4HOgO3CPu6+o49BBwBgzu5zKNeS/bWaL3H1iI2sWEZEA6gz4qqGZZe7eF6gr1GsedwdVH8qa2TBgpsJdRCR5gg7RvGFmhaFWIiIiCRX0Q9bhwE1mVkblTBqj8uI+0Kdt7r4aWN2A+kREpIFOG/Bm1sXdPwAuS1I9IiKSIHVdwS+jchXJ983sWXe/OhlFiYhI49U1Bl9zJuz3wixEREQSq66A91M8FhGRJq6uIZpeZvY5lVfyrasew///kPXboVYnIiINdtqAd/fUZBUiIiKJVZ/lgkVEpBlRwIuIxJQCXkQkphTwIiIxpYAXEYkpBbyISEwp4EVEYkoBLyISUwp4EZGYUsCLiMSUAl5EJKaC3tFJTqHi4x1Rl5B0F/3l1ghbL46m2XlDo2mXGyJqF0wLyDZ7uoIXEYkpBbyISEwp4EVEYkoBLyISUwp4EZGYUsCLiMSUAl5EJKYU8CIiMaWAFxGJKQW8iEhMKeBFRGJKAS8iElMKeBGRmFLAi4jElAJeRCSmFPAiIjGlgBcRiSkFvIhITCngRURiKrSAN7PzzWyVme0ws7fMbHpYbYmIyMnCvOn2UeB/uPsWM2sPbDazFe7+dohtiohIldCu4N39Y3ffUvX4C2AHcF5Y7YmIyInCvIKvZmZZQG/gz7W8NhWYCtClS5cGt3FN8TUNPrYxRnWdFUm7ANtnb4yk3Yz9kTQLwMp2V0TSrr3ukbSLRdOsxEPoH7KaWTvgWWCGu3/+zdfdfb6793P3fhkZGWGXIyLSYoQa8GaWRmW4/6u7PxdmWyIicqIwZ9EY8Diww90fDKsdERGpXZhX8IOA/wqMMLOSqp/LQ2xPRERqCO1DVnd/DX1EJCISGX2TVUQkphTwIiIxpYAXEYkpBbyISEwp4EVEYkoBLyISUwp4EZGYUsCLiMSUAl5EJKYU8CIiMaWAFxGJKQW8iEhMKeBFRGJKAS8iElMKeBGRmFLAi4jElAJeRCSmQrujU7IV/MdlkbT7VYT3rPro4LuRtNsxklYrGR5h65IMf5i9MeoSYkNX8CIiMaWAFxGJKQW8iEhMKeBFRGJKAS8iElMKeBGRmFLAi4jElAJeRCSmFPAiIjGlgBcRiSkFvIhITCngRURiSgEvIhJTCngRkZhSwIuIxJQCXkQkphTwIiIxpYAXEYkpBbyISEyFGvBmdqmZ7TSzPWZ2e5htiYjIiUILeDNLBf4PcBmQA4w3s5yw2hMRkROFeQXfH9jj7u+6+1fA74ErQ2xPRERqOCPEc58HfFjjeTlw0Td3MrOpwNSqp4fMbGeC6+gIfJrgczYnLbn/6nvL1az6/+OFNzTm8O+e6oUwA95q2eYnbXCfD8wPrQizTe7eL6zzN3Utuf/qe8vsO6j/XwtziKYcOL/G80zgoxDbExGRGsIM+I1ANzPLNrNWwLXAiyG2JyIiNYQ2ROPuR83sFuBVIBVY4O5vhdXeaYQ2/NNMtOT+q+8tV0vvPwDmftKwuIiIxIC+ySoiElMKeBGRmIpNwNe1LIKZDTOzg2ZWUvVzTxR1hiHIkhBV/S8xs7fMbE2yawxTgL/9rBp/91IzO2Zm/xBFrYkWoO8dzOyPZrat6m/fqAnXTUmAvn/HzJ43szfNbIOZ5UZRZ6Tcvdn/UPkh7jvA94BWwDYg5xv7DAOKo641or6fCbwNdKl6fnbUdSez/9/Y/wrgP6KuO4l/+zuBX1Q9zgA+A1pFXXuS+j4HuLfqcQ/g36OuO9k/cbmCb8nLIgTp+wTgOXf/AMDd9ya5xjDV928/Hng6KZWFL0jfHWhvZga0ozLgjya3zFAE6XsO8O8A7v4XIMvMOiW3zGjFJeBrWxbhvFr2G1j1VvXfzKxnckoLXZC+/yPwHTNbbWabzez6pFUXvqB/e8ysDXAp8GwS6kqGIH1/GLiQyi8Zbgemu/vx5JQXqiB93wZcBWBm/an8Sn9mUqprIsJcqiCZgiyLsAX4rrsfMrPLgWVAt9ArC1+Qvp8B9AUuBloD683sDXffFXZxSRBoSYwqVwDr3P2zEOtJpiB9LwJKgBHABcAKM/uTu38ednEhC9L3nwMPmVkJlf9z20o83r0EFpcr+DqXRXD3z939UNXjl4E0M+uYvBJDE2RJiHLgFXc/7O6fAmuBXkmqL2z1WRLjWuIzPAPB+n4DlcNz7u57gPeoHI9u7oL+m7/B3QuA66n8DOK95JUYvbgEfJ3LIpjZOVXjkF+/XUsB9ie90sQLsiTEC8BgMzujapjiImBHkusMS6AlMcysAzCUyt9FXATp+wdUvnOjavy5O/BuUqsMR5B/82dWvQYwBVgbg3cu9RKLIRo/xbIIZnZT1euPAuOAn5jZUeBL4Fqv+ni9OQvSd3ffYWavAG8Cx4HfuntpdFUnTsC/PcBYYLm7H46o1IQL2Pf/CTxhZtupHNa4repdXLMWsO8XAgvN7BiVs8h+FFnBEdFSBSIiMRWXIRoREfkGBbyISEwp4EVEYkoBLyISUwp4EZGYUsCLiMSUAl5EJKb+HxTnPzCZME6NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results.plot.hist(alpha=0.8) #hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29f66966c88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAGbCAYAAACI1+plAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAYBElEQVR4nO3dfXBld33f8c93bYjXbOoFbOPUqtnEsglgwISFDiQkps0DE5yQpLQQhqY8tG7oBHXaKU1KmKYTkiYEZjrc0JJ6gPHQptAMDQ0xBJMABpeBwhqvHzGRjHkQD/baro3Xu8bg/fUP3S3qsou1tn733Hv1es3c2atzH/TVkXTeOudqdaq1FgBg820begAAmFciCwCdiCwAdCKyANCJyAJAJycOPcCsOfXUU9uuXbuGHgOAKXHllVfe1lo77Wi3iexx2rVrV/bs2TP0GABMiar64rFuc7gYADoRWQDoRGQBoBORBYBORBYAOvHbxTBjRqNRVlZWhh6DJKurq0mShYWFgSfZWhYXF7O0tDT0GBsisjBjVlZWctW1N+TQyY8aepQtb9uBu5Ikt3zTpnRSth24Y+gRjouvDJhBh05+VO59woVDj7HlnXTDpUniczFBh9f5rPCaLAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2I7ISNRqOMRqOhxwDY0ia1LXYWnglzHlCA4U1qW2xPFgA6EVkA6ERkAaATkQWATkQWADoRWQDoRGQBoBORBYBORBYAOhFZAOhEZAGgk5mLbFXtP8qyU6rqHVV10/jyjqo6Zd3t51bV+6tqpao+W1V/UlWPqaoLququqto7vvzVZD8aAObZzEX2GN6W5POttbNba2cnuTnJW5Okqk5K8r4kb2mtLbbWHp/kLUlOGz/2itba+ePLTw4xPADzaebPwlNVi0meluSF6xb/dpKVqjo7yU8k+URr7c8P39ha+8j4sRdMcNQkyerqag4ePJilpaVJv2vmxPLycuq+NvQYMIi69xtZXr77IW9Dl5eXs3379k2a6tjmYU/2CUn2ttbuP7xgfH1vkicmOS/Jld/j8c9ed7j4N492h6q6qKr2VNWeffv2bebsAMyxmd+TTVJJjvZj/bGWH+mK1tqF3+sOrbWLk1ycJLt3735IuxALCwtJ4sTtPGhLS0u58qavDz0GDKKd9DdyztlnPORt6KSOJs5DZK9P8tSq2tZaO5QkVbUtyVOSfDbJ6Vk7ZAwAEzXzh4tbaytJrkry2nWLX5vkM+Pb/luSZ1XV8w7fWFXPraonTXZSALaaWYzsyVW1uu7yL5O8Ism54/+ic1OSc8fL0lo7mOTCJK+qquWquiHJS5PcOtD8AGwRM3e4uLV2rB8MXvI9HnNjkuce5aZbkly+CWMBwHeZxT1ZAJgJIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ3M3J9VnHWLi4tDjwCw5U1qWyyyEzapcxgCcGyT2hY7XAwAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2ILAB0IrIA0InIAkAnThAAM2jbgTty0g2XDj3GlrftwO1J4nMxQdsO3JHkjKHH2DCRhRnjdInTY3X120mShYXZ2ejPvjNm6ntAZGHGOF0izA6vyQJAJyILAJ2ILAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJEwTAwEajUVZWVoYegwdhdXU1SbKwsDDwJFvL4uLizJwoQ2RhYCsrK/nr6z6Ts3bcP/QoHKd77j4hSXLvt7828CRbx5f2nzD0CMdFZGEKnLXj/rx29/6hx+A4/c6eHUniczdBh9f5rPCaLAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2I7JQZjUYZjUZDjwEw86Zhe+osPFPGeUUBNsc0bE/tyQJAJyILAJ2ILAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANDJlotsVV1SVS8Yeg4A5p+z8EyZ1dXVHDx4MEtLS0OPwoQsLy/n4d/acj/vwoNyy4FtuW95eUPbyOXl5Wzfvn0CUx3bA35nV9Wuqrqxqt5aVddV1R9X1U9W1cerarmqnlFVj6iqt1fVp6vqqqp6/rrHXlFVnxlfnjVefkFVXV5V7x4/9x9XVX2PGb5QVf++qj5RVXuq6keq6rKquqmqfnV8n6qqN4xnvLaqXrhu+Zur6oaqel+S09c979Oq6qNVdeX4+X7gGO//ovH73bNv377jWsEAbF0b3ZNdTPL3k1yU5NNJXpzkx5L8fJLXJLkhyYdbay+vqp1JPlVVf5Xk1iQ/1Vq7t6rOSfLOJLvHz/nUJE9M8tUkH0/yo0n+1/eY4cuttWdW1X9Icsn4/icluT7JHyX5pSTnJ3lKklOTfLqqPpbkmUkel+RJSR4znvXtVfWwJH+Y5PmttX3jKP9ukpcf+Y5baxcnuThJdu/e3Ta4zh6UhYWFJBn8RMNMztLSUu79wqeHHgNmwmNOPpSTdp2zoW3kNBwR3Ghkb26tXZskVXV9kg+11lpVXZtkV5KFJD9fVf9qfP+TkpyVtYC+uarOT3J/knPXPeenWmur4+fcO36e7xXZ947/vTbJjtba3Unurqp7x2H/sSTvbK3dn+SWqvpokqcn+fF1y79aVR8eP8/jkpyX5C/HO9EnJPnaBtcHADygjUb2m+uuH1r39qHxc9yf5O+11j63/kFV9e+S3JK1vcttSe49xnPev4FZ1r/PI+c5MckxDzcnOdreZyW5vrX2zAd4vwDwoGzWb1tcluRVh19XraqnjpefkuRrrbVDSf5h1vYWe/lYkhdW1QlVdVrW9mA/NV7+ovHyH0jynPH9P5fktKp65njmh1XVEzvOB8AWs1mRfV2ShyW5pqquG7+dJP8pyT+qqk9m7VDxPZv0/o7mPUmuSXJ1kg8n+detta+Ply9n7TDzW5J8NElaa/cleUGS11fV1Un2JnlWx/kA2GIe8HBxa+0LWXvt8vDbLz3Gbf/0KI9dTvLkdYv+zXj55UkuX3e/X3uAGXatu35J1n7x6btuS/Lq8WX9Y1uSoz5/a21v1vZ4AWDT+c95ANDJVP0xiqp6T5IfPGLxr7fWLhtiHgB4KKYqsq21Xxx6BgDYLA4XA0AnIgsAnYgsAHQisgDQicgCQCciCwCdTNV/4SFZXFwcegSAuTAN21ORnTLTcP5DgHkwDdtTh4sBoBORBYBORBYAOhFZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkQWATkQWADoRWQDoxAkCYAp8af8J+Z09O4Yeg+P0xbtPSBKfuwn60v4Tcu7QQxwHkYWBTcPpuHhwHrG6miQ5aWFh4Em2jnMzW98zIgsDm4bTcQF9eE0WADoRWQDoRGQBoBORBYBORBYAOhFZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkQWATpwgAAY2Go2ysrIy9Bh0sDo+S8+Cs/RsqsXFxZk5sYbIwsBWVlZy1fVXJTuHnoRNd9faP/tq37BzzJM7hx7g+IgsTIOdyaELDg09BZts2+Vrr8j53G6ew+t0VszWtAAwQ0QWADoRWQDoRGQBoBORBYBORBYAOhFZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkd0iRqNRRqPR0GMATIVJbROdhWeLcL5SgO+Y1DbRniwAdCKyANCJyAJAJyILAJ2ILAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCdzFdmq2lVV1w09BwAkcxZZAJgmcxvZqvqhqrqqql5dVX9aVR+oquWq+oN199lfVb9bVVdX1Ser6jFDzgzAfJnLU91V1eOSvCvJy5KcP748Nck3k3yuqv6wtfblJI9I8snW2m+O4/tPkvzOQGN3tbq6moMHD2ZpaWnoUTjC8vJycmjoKWBG7F/7nnmo27Ll5eVs3759k4Y6tnnckz0tyZ8leUlrbe942Ydaa3e11u5NckOSx46X35fk0vH1K5PsOtoTVtVFVbWnqvbs27ev3+QAzJV53JO9K8mXk/xokuvHy7657vb7852P+1uttXaU5f+f1trFSS5Okt27d7ej3WfaLSwsJElGo9HAk3CkpaWlXPWVq4YeA2bDjuScM895yNuySR3Vm8fI3pfkF5JcVlX7hx4GgK1rHg8Xp7V2T5ILk/yLJKcMPA4AW9Rc7cm21r6Q5Lzx9TuTPP0o97lw3fUd666/O8m7+08JwFYxl3uyADANRBYAOhFZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkQWATkQWADqZqz+ryLEtLi4OPQLA1JjUNlFktwgnawf4jkltEx0uBoBORBYAOhFZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkQWATkQWADoRWQDoRGQBoBMnCIBpcGey7XI/886dO9f+8bndRHcmOXPoITZOZGFgTkM4v1bbapJk4cyFgSeZI2fO1veMyMLAnIYQ5pdjGADQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2ILAB0IrIA0InIAkAnIgsAnYgsAHTiBAEwx0ajUVZWVoYeY26tro7PsrPgLDs9LS4uzuyJNEQW5tjKykpu3Ls3Zww9yJy6e/zvnbfdNugc8+zrQw/wEIkszLkzkrwiNfQYc+ltaUms354Or+NZ5TVZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkQWATkQWADoRWQDoRGQBoBORBYBORBYAOhFZMhqNMhqNhh4DYFNNw7bNWXhwvlFgLk3Dts2eLAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ3MXGSramdV/bOh5wCABzJzkU2yM8l3RbaqThhgFgA4plk81d3vJzm7qvYm+VaS/Um+luT8qnrS+PYLknxfkv/YWvvPSVJVr07yD8bL39Na+62qekSSP0mykOSEJK9rrf33CX88g1tdXc3BgweztLQ09ChssuXl5Zn8SRoOuz3JvuXlB7V9Wl5ezvbt2zd/qOMwi5H9jSTntdbOr6oLkrxv/PbNVXVRkrtaa0+vqu9L8vGq+mCSc8aXZySpJO+tqh9PclqSr7bWnpckVXXK0d7h+HkvSpKzzjqr70cHwNyYxcge6VOttZvH1386yZOr6gXjt0/JWlx/eny5arx8x3j5FUneWFWvT3Jpa+2Ko72D1trFSS5Okt27d7cuH8WAFhYWkiSj0WjgSdhsS0tLuXPv3qHHgAft0Ul2nnPOg9o+TcPRuXmI7D3rrleSV7XWLlt/h6r6mSS/d/jQ8RG3PS3Jzyb5var6YGvtt7tOC8CWMYsv19yd5PuPcdtlSV5ZVQ9Lkqo6d/y662VJXl5VO8bLz6yq06vqbyY50Fr7r0nemORH+o8PwFYxc3uyrbXbq+rjVXVdkoNJbll381uT7ErymaqqJPuS/EJr7YNV9fgkn1hbnP1JXpJkMckbqupQ1n6J6pWT+0gAmHczF9kkaa29+BjLDyV5zfhy5G1vSvKmIxbflLW9XADYdLN4uBgAZoLIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANDJTP7FJzbX4uLi0CMAbLpp2LaJLFNxOiiAzTYN2zaHiwGgE5EFgE5EFgA6EVkA6ERkAaATkQWATkQWADoRWQDoRGQBoBORBYBORBYAOhFZAOjECQJgzn09ydvShh5jLn1t/K/128/Xk+wceoiHQGRhjk3Dqb7m2f7V1STJzoWFgSeZXzsz21/HIgtzbBpO9QVbmddkAaATkQWATkQWADoRWQDoRGQBoBORBYBORBYAOhFZAOhEZAGgE5EFgE5EFgA6EVkA6MQJAmCCRqNRVlZWhh6DTbI6PgvPgrPwTMTi4uLMnfRCZGGCVlZWcv21n83Ok08fehQ2wV0H7k6S1DdvH3iS+XfngVuHHuFBEVmYsJ0nn57n/PCLhh6DTfCRG9+VJD6fE3B4Xc8ar8kCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2ILAB0IrIA0InIAkAnIgsAnYgsAHQisgDQicgCQCciu0WMRqOMRqOhxwCYmGnY7jnV3RbhROHAVjMN2z17sgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2ILAB0IrIA0InIAkAnIgsAncxFZKuqVdV/Wff2iVW1r6ouHb/90qp681Ee94Wquraqrq6qD1bVGZOcG4D5NheRTXJPkvOqavv47Z9K8pUNPvY5rbWnJNmT5DU9hgNga5qns/D8RZLnJXl3kl9O8s4kzz6Ox38syVKHuabC6upqDh48mKWluf0QZ8Ly8nIO3VdDjwEzZ/+9/yfLy3cc1zZseXk527dvf+A7djQve7JJ8q4kL6qqk5I8Ocn/Ps7HX5jk2qPdUFUXVdWeqtqzb9++hzgmAFvF3OzJttauqapdWduLff9xPPQjVXV/kmuSvPYYz31xkouTZPfu3e2hTTqMhYWFJBn8BMZb3dLSUr5y0+1DjwEzZ8dJj8yZZz/6uLZh03Dkbm4iO/beJG9MckGSR2/wMc9prd3WbSIAtqx5i+zbk9zVWru2qi4YehgAtrZ5ek02rbXV1tqbjnHzS6tqdd1lYaLDAbDlzMWebGttx1GWXZ7k8vH1S5JccpSH7uo3FQBb3VztyQLANBFZAOhEZAGgE5EFgE5EFgA6EVkA6ERkAaATkQWATkQWADqZi7/4xANbXFwcegSAiZqG7Z7IbhHTcMongEmahu2ew8UA0InIAkAnIgsAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANCJyAJAJyILAJ2ILAB04gQBMGF3Hrg1H7nxXUOPwSa488CtSeLzOQF3Hrg1Z+bRQ49x3EQWJmgaTr3F5mmrB5MkZy7M3sZ/1pyZR8/k94/IwgRNw6m3gMnxmiwAdCKyANCJyAJAJyILAJ2ILAB0IrIA0Em11oaeYaZU1b4kX+z8bk5Nclvn9zEPrKeNsZ42xnraGOvpuz22tXba0W4Q2SlUVXtaa7uHnmPaWU8bYz1tjPW0MdbT8XG4GAA6EVkA6ERkp9PFQw8wI6ynjbGeNsZ62hjr6Th4TRYAOrEnCwCdiCwAdCKyA6mq51bV56pqpap+4yi3X1BVd1XV3vHl3w4x59AeaD2N73PBeB1dX1UfnfSM02ADX0+vXve1dF1V3V9Vjxpi1iFtYD2dUlV/XlVXj7+eXjbEnEPbwHp6ZFW9p6quqapPVdV5Q8w5E1prLhO+JDkhyU1JfijJw5NcneQJR9zngiSXDj3rDKynnUluSHLW+O3Th557GtfTEff/uSQfHnruaVxPSV6T5PXj66cluSPJw4eefQrX0xuS/Nb4+g8n+dDQc0/rxZ7sMJ6RZKW19vnW2n1J3pXk+QPPNI02sp5enORPW2tfSpLW2q0TnnEaHO/X0y8needEJpsuG1lPLcn3V1Ul2ZG1yH57smMObiPr6QlJPpQkrbUbk+yqqsdMdszZILLDODPJl9e9vTpedqRnjg9b/UVVPXEyo02Vjaync5M8sqour6orq+pXJjbd9Njo11Oq6uQkz03yPyYw17TZyHp6c5LHJ/lqkmuT/PPW2qHJjDc1NrKerk7yS0lSVc9I8tgkCxOZbsacOPQAW1QdZdmR/5fqM1n7e5j7q+pnk/zPJOd0n2y6bGQ9nZjkaUn+bpLtST5RVZ9srf117+GmyEbW02E/l+TjrbU7Os4zrTaynn4myd4kfyfJ2Un+sqquaK19o/dwU2Qj6+n3k7ypqvZm7YeRq7L19vg3xJ7sMFaT/K11by9k7Sfn/6e19o3W2v7x9fcneVhVnTq5EafCA66n8X0+0Fq7p7V2W5KPJXnKhOabFhtZT4e9KFvzUHGysfX0sqy9/NBaaytJbs7aa45byUa3Ty9rrZ2f5Fey9vr1zZMbcXaI7DA+neScqvrBqnp41jZ8711/h6o6Y/y60OHDMduS3D7xSYf1gOspyZ8leXZVnTg+FPq3k3x2wnMObSPrKVV1SpKfyNo624o2sp6+lLWjIhm/xvi4JJ+f6JTD28j2aef4tiT5x0k+tsX29jfM4eIBtNa+XVW/luSyrP0m39tba9dX1a+Ob/+jJC9I8sqq+naSg0le1Ma/yrdVbGQ9tdY+W1UfSHJNkkNJ3tpau264qSdvg19PSfKLST7YWrtnoFEHtcH19Lokl1TVtVk7bPrr4yMkW8YG19Pjk7yjqu7P2m/3v2KwgaecP6sIAJ04XAwAnYgsAHQisgDQicgCQCciCwCdiCwAdCKyANDJ/wU0K60PCslO5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "sns.boxplot(data=df_results,orient=\"h\") #boxplot horizontal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que todos los métodos tienen una variación en un rango de 0.5 y 0.97, con media y mediana muy similar, pero aparentemente el método de imputación basado en arboles de decisión fue un poco mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalidad univariada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro test:\n",
      "Method: LOCF W: 0.924 p_v: 0.034\n",
      "Method: mean_mode W: 0.915 p_v: 0.02\n",
      "Method: knn W: 0.914 p_v: 0.018\n",
      "Method: trees W: 0.914 p_v: 0.019\n",
      "Method: MLP W: 0.91 p_v: 0.015\n"
     ]
    }
   ],
   "source": [
    "#checa normalidad univariada\n",
    "methods=df_results.columns\n",
    "print('Shapiro test:')\n",
    "for method in methods:\n",
    "    W,p_v=stats.shapiro(df_results[method].values) #W>0.9 or p_v>0.05  Normalidad univariada \n",
    "    print('Method: '+method+' W: '+str(np.round(W,3))+' p_v: '+str(np.round(p_v,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se rechaza la normalidad y la aproximamos por la transformación Box-Cox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aproximar a normal\n",
    "pt1 = PowerTransformer(method='box-cox') #‘yeo-johnson’ boxcox solo positivos\n",
    "pt1.fit(df_results.values) #estima los lambdas optimos para la transformacion\n",
    "X_t=pt1.transform(df_results.values) #aplica la transformacion a los datos\n",
    "df_results_norm=pd.DataFrame(X_t)\n",
    "df_results_norm.columns=methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro test:\n",
      "Method: LOCF W: 0.925 p_v: 0.037\n",
      "Method: mean_mode W: 0.914 p_v: 0.019\n",
      "Method: knn W: 0.917 p_v: 0.022\n",
      "Method: trees W: 0.915 p_v: 0.02\n",
      "Method: MLP W: 0.912 p_v: 0.017\n"
     ]
    }
   ],
   "source": [
    "#checa normalidad univariada\n",
    "methods=df_results.columns\n",
    "print('Shapiro test:')\n",
    "for method in methods:\n",
    "    W,p_v=stats.shapiro(df_results_norm[method].values) #W>0.9 or p_v>0.05  Normalidad univariada \n",
    "    print('Method: '+method+' W: '+str(np.round(W,3))+' p_v: '+str(np.round(p_v,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aceptamos el supuesto de normalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-test para diferencia en medias al 5%: \n",
    "\n",
    "${H_0}:{\\mu _1} - {\\mu _2} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test para diferencia en medias al 5%:\n",
      "LOCF-mean_mode acepta H0 :True\n",
      "LOCF-knn acepta H0 :True\n",
      "LOCF-trees acepta H0 :True\n",
      "LOCF-MLP acepta H0 :True\n",
      "mean_mode-knn acepta H0 :True\n",
      "mean_mode-trees acepta H0 :True\n",
      "mean_mode-MLP acepta H0 :True\n",
      "knn-trees acepta H0 :True\n",
      "knn-MLP acepta H0 :True\n",
      "trees-MLP acepta H0 :True\n"
     ]
    }
   ],
   "source": [
    "print('t-test para diferencia en medias al 5%:')\n",
    "methods=list(df_results.columns)\n",
    "methods2=list(df_results.columns)\n",
    "for method1 in methods:\n",
    "    for method2 in methods2:\n",
    "        if method1 != method2:\n",
    "            _,pvalue=stats.ttest_ind(df_results_norm[method1].values,df_results_norm[method2].values)\n",
    "            print(method1+'-'+method2+' acepta H0 :'+str(np.round(pvalue,3)>0.05))\n",
    "    methods2.remove(method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este experimento, ningún método resultó significativamente diferente a otro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación Bayesiana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: LOCF: 0.1872 - mean_mode: 0.8128\n",
      "Probabilities: LOCF: 0.10844 - knn: 0.89156\n",
      "Probabilities: LOCF: 0.0678 - trees: 0.9322\n",
      "Probabilities: LOCF: 0.5897 - MLP: 0.4103\n",
      "Probabilities: mean_mode: 0.12784 - knn: 0.87216\n",
      "Probabilities: mean_mode: 0.03838 - trees: 0.96162\n",
      "Probabilities: mean_mode: 0.6185 - MLP: 0.3815\n",
      "Probabilities: knn: 0.36664 - trees: 0.63336\n",
      "Probabilities: knn: 0.93878 - MLP: 0.06122\n",
      "Probabilities: trees: 0.94352 - MLP: 0.05648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "methods=list(df_results.columns)\n",
    "methods2=list(df_results.columns)\n",
    "for method1 in methods:\n",
    "    for method2 in methods2:\n",
    "        if method1 != method2:\n",
    "            probs, fig=two_on_multiple(df_results[method1].values,df_results[method2].values,plot=True, names=(method1,method2))\n",
    "            print('Probabilities: '+method1+': '+str(probs[0])+' - '+method2+': '+str(probs[2]))\n",
    "    methods2.remove(method1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la comparación bayesiana se puede observar que el método basado en arboles fue mejor que todos los demás, seguido de los métodos de vecinos más cercanos, media o moda, LOCF y finalmente el método basado en redes neuronales.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:\n",
    "\n",
    "- El método basado en arboles fue el mejor en este experimento, algunas de las posibles causas son que para este método se realizo un ajuste por búsqueda aleatoria y se empleó regularización, además de las ventajas mismas de la implementación XGBoost la cual no necesita que el conjunto de datos sea estandarizado, el método de construcción de los arboles aprende con los datos que decisión tomar cuando hay valores ausentes y también como otros métodos basado en árboles,  estos logran captar cuales son las características más importantes en la construcción de los árboles.\n",
    "\n",
    "- Por otro lado, el método basado en redes neuronales fue el peor, pero hay que considerar que en este caso no se empleó regularización, se utilizó un mismo modelo, es decir por tiempo computacional no se hizo búsqueda aleatoria sobre los parámetros para ajustar el mejor, esto pudo causar un ajuste pobre, además que los métodos de redes neuronales son especialmente buenos cuando se tiene una gran cantidad de datos y en este caso el tamaño máximo fue de 1000 aproximadamente lo cual es muy poco.\n",
    "\n",
    "- Otro factor a considerar es que no se tomo en cuenta si había clases desbalanceadas, lo cual también afecta la clasificación.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
